<?xml version="1.0" encoding="UTF-8"?>
<testsuites tests="3883" failures="5" skipped="31">
	<testsuite name="github.com/cilium/cilium/api/v1/client" tests="0" failures="0" errors="0" id="0" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/client/bgp" tests="0" failures="0" errors="0" id="1" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/client/daemon" tests="0" failures="0" errors="0" id="2" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/client/endpoint" tests="0" failures="0" errors="0" id="3" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/client/ipam" tests="0" failures="0" errors="0" id="4" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/client/metrics" tests="0" failures="0" errors="0" id="5" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/client/policy" tests="0" failures="0" errors="0" id="6" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/client/prefilter" tests="0" failures="0" errors="0" id="7" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/client/recorder" tests="0" failures="0" errors="0" id="8" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/client/service" tests="0" failures="0" errors="0" id="9" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/flow" tests="0" failures="0" errors="0" id="10" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/health/client" tests="0" failures="0" errors="0" id="11" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/health/client/connectivity" tests="0" failures="0" errors="0" id="12" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/health/client/restapi" tests="0" failures="0" errors="0" id="13" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/health/models" tests="0" failures="0" errors="0" id="14" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/health/server" tests="0" failures="0" errors="0" id="15" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/health/server/restapi" tests="0" failures="0" errors="0" id="16" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/health/server/restapi/connectivity" tests="0" failures="0" errors="0" id="17" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/models" tests="0" failures="0" errors="0" id="18" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/observer" tests="0" failures="0" errors="0" id="19" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/operator/client/metrics" tests="0" failures="0" errors="0" id="20" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/operator/client/operator" tests="0" failures="0" errors="0" id="21" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/operator/client" tests="0" failures="0" errors="0" id="22" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/operator/models" tests="0" failures="0" errors="0" id="23" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/operator/server" tests="0" failures="0" errors="0" id="24" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/operator/server/restapi" tests="0" failures="0" errors="0" id="25" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/operator/server/restapi/metrics" tests="0" failures="0" errors="0" id="26" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/operator/server/restapi/operator" tests="0" failures="0" errors="0" id="27" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/peer" tests="0" failures="0" errors="0" id="28" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/recorder" tests="0" failures="0" errors="0" id="29" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/relay" tests="0" failures="0" errors="0" id="30" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/server" tests="0" failures="0" errors="0" id="31" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/server/restapi" tests="0" failures="0" errors="0" id="32" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/server/restapi/bgp" tests="0" failures="0" errors="0" id="33" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/server/restapi/daemon" tests="0" failures="0" errors="0" id="34" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/server/restapi/endpoint" tests="0" failures="0" errors="0" id="35" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/server/restapi/ipam" tests="0" failures="0" errors="0" id="36" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/server/restapi/metrics" tests="0" failures="0" errors="0" id="37" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/server/restapi/policy" tests="0" failures="0" errors="0" id="38" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/server/restapi/prefilter" tests="0" failures="0" errors="0" id="39" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/server/restapi/recorder" tests="0" failures="0" errors="0" id="40" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/api/v1/server/restapi/service" tests="0" failures="0" errors="0" id="41" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/bugtool" tests="0" failures="0" errors="0" id="42" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/bugtool/cmd" tests="7" failures="0" errors="0" id="43" hostname="kind-bpf-next" time="0.035" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/bugtool/cmd" time="0.000"></testcase>
		<testcase name="Test/BugtoolSuite" classname="github.com/cilium/cilium/bugtool/cmd" time="0.000"></testcase>
		<testcase name="Test/BugtoolSuite/TestHashEncryptionKeys" classname="github.com/cilium/cilium/bugtool/cmd" time="0.000"></testcase>
		<testcase name="Test/BugtoolSuite/TestWalkPath" classname="github.com/cilium/cilium/bugtool/cmd" time="0.000"></testcase>
		<testcase name="Test_removeIfEmpty" classname="github.com/cilium/cilium/bugtool/cmd" time="0.000"></testcase>
		<testcase name="Test_removeIfEmpty/directory_is_empty" classname="github.com/cilium/cilium/bugtool/cmd" time="0.000">
			<system-out><![CDATA[Deleted empty directory /tmp/Test_removeIfEmptydirectory_is_empty1236620774/001]]></system-out>
		</testcase>
		<testcase name="Test_removeIfEmpty/directory_is_not_empty" classname="github.com/cilium/cilium/bugtool/cmd" time="0.000">
			<system-out><![CDATA[Deleted empty directory /tmp/Test_removeIfEmptydirectory_is_not_empty4261448239/001]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/bugtool/cmd	coverage: 13.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/cilium" tests="0" failures="0" errors="0" id="44" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/cilium-health" tests="0" failures="0" errors="0" id="45" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/cilium-health/cmd" tests="0" failures="0" errors="0" id="46" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/cilium/cmd" tests="20" failures="0" errors="0" id="47" hostname="kind-bpf-next" time="0.149" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/cilium/cmd" time="0.070"></testcase>
		<testcase name="Test/BPFCtListSuite" classname="github.com/cilium/cilium/cilium/cmd" time="0.010"></testcase>
		<testcase name="Test/BPFCtListSuite/TestDumpCt4" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/BPFCtListSuite/TestDumpCt6" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/BPFIPCacheGetSuite" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/BPFIPCacheGetSuite/TestGetLPMValue" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/BPFIPCacheGetSuite/TestGetPrefix" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/BPFMetricsMapSuite" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/BPFMetricsMapSuite/TestDumpMetrics" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/BPFNatListSuite" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/BPFNatListSuite/TestDumpNat4" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/BPFNatListSuite/TestDumpNat6" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/EncryptStatusSuite" classname="github.com/cilium/cilium/cilium/cmd" time="0.060"></testcase>
		<testcase name="Test/EncryptStatusSuite/TestCountUniqueIPsecKeys" classname="github.com/cilium/cilium/cilium/cmd" time="0.050"></testcase>
		<testcase name="Test/EncryptStatusSuite/TestGetXfrmStats" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/CMDHelpersSuite" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/CMDHelpersSuite/TestExpandNestedJSON" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/CMDHelpersSuite/TestParsePolicyUpdateArgsHelper" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="Test/CMDHelpersSuite/TestParseTrafficString" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<testcase name="TestLoadInvalidPolicyJSON" classname="github.com/cilium/cilium/cilium/cmd" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/cilium/cmd	coverage: 14.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/cilium-health/launch" tests="0" failures="0" errors="0" id="48" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/cilium-health/responder" tests="0" failures="0" errors="0" id="49" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/clustermesh-apiserver/metrics" tests="0" failures="0" errors="0" id="50" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/clustermesh-apiserver/option" tests="0" failures="0" errors="0" id="51" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/clustermesh-apiserver" tests="1" failures="0" errors="0" id="52" hostname="kind-bpf-next" time="0.612" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestUsersManagement" classname="github.com/cilium/cilium/clustermesh-apiserver" time="0.080">
			<system-out><![CDATA[level=info msg=Invoked duration="32.431µs" function="github.com/cilium/cilium/clustermesh-apiserver.registerUsersManager (users_mgmt.go:69)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Starting managing etcd users based on configuration" file-path=/tmp/clustermesh-config2022214326/users.yaml subsys=clustermesh-apiserver
level=info msg="Start hook executed" duration="304.107µs" function="*main.usersManager.Start" subsys=hive
level=info msg="User successfully configured" subsys=clustermesh-apiserver user=foo
level=info msg="User successfully configured" subsys=clustermesh-apiserver user=bar
level=info msg="User successfully configured" subsys=clustermesh-apiserver user=qux
level=info msg="User successfully configured" subsys=clustermesh-apiserver user=baz
level=info msg="User successfully configured" subsys=clustermesh-apiserver user=qux
level=info msg="User successfully removed" subsys=clustermesh-apiserver user=bar
level=info msg=Stopping subsys=hive
level=info msg="Stopping managing etcd users based on configuration" file-path=/tmp/clustermesh-config2022214326/users.yaml subsys=clustermesh-apiserver
level=info msg=Closing subsys=clustermesh-apiserver
level=info msg="Stop hook executed" duration="43.622µs" function="*main.usersManager.Stop" subsys=hive]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/clustermesh-apiserver	coverage: 10.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/daemon" tests="0" failures="0" errors="0" id="53" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/daemon/cmd/cni/fake" tests="0" failures="0" errors="0" id="54" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/hubble-relay" tests="0" failures="0" errors="0" id="55" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/hubble-relay/cmd" tests="0" failures="0" errors="0" id="56" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/hubble-relay/cmd/completion" tests="0" failures="0" errors="0" id="57" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/hubble-relay/cmd/serve" tests="0" failures="0" errors="0" id="58" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/hubble-relay/cmd/version" tests="0" failures="0" errors="0" id="59" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/images/builder" tests="0" failures="0" errors="0" id="60" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/daemon/cmd" tests="27" failures="0" errors="0" id="61" hostname="kind-bpf-next" skipped="3" time="0.473" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestAgentCell" classname="github.com/cilium/cilium/daemon/cmd" time="0.250">
			<system-out><![CDATA[level=debug msg=Invoking function="pprof.glob..func1 (cell.go:51)" subsys=hive
level=info msg=Invoked duration="233.464µs" function="pprof.glob..func1 (cell.go:51)" subsys=hive
level=debug msg=Invoking function="gops.registerGopsHooks (cell.go:39)" subsys=hive
level=info msg=Invoked duration="66.203µs" function="gops.registerGopsHooks (cell.go:39)" subsys=hive
level=debug msg=Invoking function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=debug msg="signalmap.newMap: &{0xc000122b00 <nil> 4}" subsys=hive
level=debug msg="getting identity cache for identity allocator manager" subsys=identity-cache
level=info msg=Invoked duration=176.317379ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=debug msg=Invoking function="auth.newManager (cell.go:76)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=debug msg="newSignalManager: &{0xc0008d7c68 [<nil> <nil> <nil>] <nil> 0xc000f8e7e0 {{{0 0}}} {{} {} 0}}" subsys=signal
level=info msg=Invoked duration="724.429µs" function="auth.newManager (cell.go:76)" subsys=hive
level=debug msg=Invoking function="gc.registerSignalHandler (cell.go:47)" subsys=hive
level=info msg=Invoked duration="18.504µs" function="gc.registerSignalHandler (cell.go:47)" subsys=hive
level=debug msg=Invoking function="utime.initUtimeSync (cell.go:30)" subsys=hive
level=info msg=Invoked duration="47.258µs" function="utime.initUtimeSync (cell.go:30)" subsys=hive]]></system-out>
		</testcase>
		<testcase name="Test_cleanStaleCEP" classname="github.com/cilium/cilium/daemon/cmd" time="0.000"></testcase>
		<testcase name="Test_cleanStaleCEP/CEPs_with_local_pods_with_endpoints_should_not_be_GCd" classname="github.com/cilium/cilium/daemon/cmd" time="0.000"></testcase>
		<testcase name="Test_cleanStaleCEP/Non_local_CEPs_should_not_be_GCd" classname="github.com/cilium/cilium/daemon/cmd" time="0.000"></testcase>
		<testcase name="Test_cleanStaleCEP/Nothing_should_be_deleted_if_fields_are_missing" classname="github.com/cilium/cilium/daemon/cmd" time="0.000"></testcase>
		<testcase name="Test_cleanStaleCEP/CES:_local_CEPs_without_endpoints_should_be_GCd" classname="github.com/cilium/cilium/daemon/cmd" time="0.000">
			<system-out><![CDATA[level=info msg="Found stale ciliumendpoint for local pod that is not being managed, deleting." ciliumEndpointName=foo k8sNamespace=x subsys=daemon]]></system-out>
		</testcase>
		<testcase name="Test_cleanStaleCEP/CES:_Test_case_where_IP_in_apiserver_changes_and_delete_should_be_skipped" classname="github.com/cilium/cilium/daemon/cmd" time="0.000">
			<system-out><![CDATA[level=debug msg="Stale CEP fetched apiserver no longer references this Node, skipping." ciliumEndpointName=foo error="<nil>" k8sNamespace=x subsys=daemon]]></system-out>
		</testcase>
		<testcase name="Test_cleanStaleCEP/CEPs_with_local_pods_without_endpoints_should_be_GCd" classname="github.com/cilium/cilium/daemon/cmd" time="0.000">
			<system-out><![CDATA[level=info msg="Found stale ciliumendpoint for local pod that is not being managed, deleting." ciliumEndpointName=foo k8sNamespace=x subsys=daemon]]></system-out>
		</testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/daemon/cmd" time="0.030"></testcase>
		<testcase name="Test/DaemonEtcdSuite" classname="github.com/cilium/cilium/daemon/cmd" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/DaemonConsulSuite" classname="github.com/cilium/cilium/daemon/cmd" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/DaemonFQDNSuite" classname="github.com/cilium/cilium/daemon/cmd" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/KubeProxyHealthzTestSuite" classname="github.com/cilium/cilium/daemon/cmd" time="0.000"></testcase>
		<testcase name="Test/KubeProxyHealthzTestSuite/TestKubeProxyHealth" classname="github.com/cilium/cilium/daemon/cmd" time="0.000"></testcase>
		<testcase name="Test/KPRSuite" classname="github.com/cilium/cilium/daemon/cmd" time="0.010"></testcase>
		<testcase name="Test/KPRSuite/TestInitKubeProxyReplacementOptions" classname="github.com/cilium/cilium/daemon/cmd" time="0.010">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg="Auto-disabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\" features and falling back to \"enable-host-legacy-routing\"" subsys=daemon
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon]]></system-out>
		</testcase>
		<testcase name="Test/NodePortSuite" classname="github.com/cilium/cilium/daemon/cmd" time="0.000"></testcase>
		<testcase name="Test/NodePortSuite/TestCheckNodePortAndEphemeralPortRanges" classname="github.com/cilium/cilium/daemon/cmd" time="0.000"></testcase>
		<testcase name="Test/GetNodesSuite" classname="github.com/cilium/cilium/daemon/cmd" time="0.010"></testcase>
		<testcase name="Test/GetNodesSuite/Test_cleanupClients" classname="github.com/cilium/cilium/daemon/cmd" time="0.010">
			<system-out><![CDATA[level=info msg="Detected MTU 1500" subsys=mtu]]></system-out>
		</testcase>
		<testcase name="Test/GetNodesSuite/Test_getNodesHandle" classname="github.com/cilium/cilium/daemon/cmd" time="0.000">
			<system-out><![CDATA[level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="Detected MTU 1500" subsys=mtu]]></system-out>
		</testcase>
		<testcase name="Test_removeOldRouterState" classname="github.com/cilium/cilium/daemon/cmd" time="0.050"></testcase>
		<testcase name="Test_removeOldRouterState/test-1" classname="github.com/cilium/cilium/daemon/cmd" time="0.030">
			<system-out><![CDATA[level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv6.conf.cilium_host.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_host.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_host.rp_filter sysParamValue=0
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_host.accept_local sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_host.send_redirects sysParamValue=0
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv6.conf.cilium_net.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_net.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_net.rp_filter sysParamValue=0
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_net.accept_local sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_net.send_redirects sysParamValue=0
level=info msg="More than one stale router IP was found on the cilium_host device after restoration, cleaning up old router IPs." subsys=daemon]]></system-out>
		</testcase>
		<testcase name="Test_removeOldRouterState/test-2" classname="github.com/cilium/cilium/daemon/cmd" time="0.020">
			<system-out><![CDATA[level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv6.conf.cilium_host.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_host.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_host.rp_filter sysParamValue=0
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_host.accept_local sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_host.send_redirects sysParamValue=0
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv6.conf.cilium_net.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_net.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_net.rp_filter sysParamValue=0
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_net.accept_local sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_net.send_redirects sysParamValue=0]]></system-out>
		</testcase>
		<testcase name="TestCoalesceCIDRs" classname="github.com/cilium/cilium/daemon/cmd" time="0.000"></testcase>
		<testcase name="Test_getMapNameEvents" classname="github.com/cilium/cilium/daemon/cmd" time="0.000"></testcase>
		<testcase name="Test_getMapNameEventsMapErrors" classname="github.com/cilium/cilium/daemon/cmd" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/daemon/cmd	coverage: 18.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/daemon/cmd/cni" tests="8" failures="0" errors="0" id="62" hostname="kind-bpf-next" time="0.037" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestInstallCNIConfFile" classname="github.com/cilium/cilium/daemon/cmd/cni" time="0.000">
			<system-out><![CDATA[level=info msg="Generating CNI configuration file with mode none"
level=info msg="Wrote CNI configuration file to /tmp/TestInstallCNIConfFile4053838697/001/05-cilium.conflist"
level=info msg="Renaming non-Cilium CNI configuration file other.conflist to other.conflist.cilium_bak"]]></system-out>
		</testcase>
		<testcase name="TestRenderCNIConfUnchained" classname="github.com/cilium/cilium/daemon/cmd/cni" time="0.000">
			<system-out><![CDATA[level=info msg="Generating CNI configuration file with mode none"
level=info msg="Generating CNI configuration file with mode flannel"
level=info msg="Generating CNI configuration file with mode portmap"]]></system-out>
		</testcase>
		<testcase name="TestRenderCNIConfChained" classname="github.com/cilium/cilium/daemon/cmd/cni" time="0.000"></testcase>
		<testcase name="TestRenderCNIConfChained/empty" classname="github.com/cilium/cilium/daemon/cmd/cni" time="0.000"></testcase>
		<testcase name="TestRenderCNIConfChained/aws" classname="github.com/cilium/cilium/daemon/cmd/cni" time="0.000">
			<system-out><![CDATA[level=info msg="Found CNI network another-network for chaining" path=/tmp/TestRenderCNIConfChainedaws811025993/001/10-aws.conflist
level=info msg="Generated chained cilium CNI configuration from another-network"]]></system-out>
		</testcase>
		<testcase name="TestRenderCNIConfChained/cni-conf" classname="github.com/cilium/cilium/daemon/cmd/cni" time="0.000">
			<system-out><![CDATA[level=info msg="Found CNI network another-network for chaining" path=/tmp/TestRenderCNIConfChainedcni-conf2203140085/001/10-another.conf
level=info msg="Generated chained cilium CNI configuration from another-network"]]></system-out>
		</testcase>
		<testcase name="TestRenderCNIConfChained/already-existing" classname="github.com/cilium/cilium/daemon/cmd/cni" time="0.000">
			<system-out><![CDATA[level=info msg="Found CNI network another-network for chaining" path=/tmp/TestRenderCNIConfChainedalready-existing442020298/001/10-existing.conflist
level=info msg="Generated chained cilium CNI configuration from another-network"]]></system-out>
		</testcase>
		<testcase name="TestCleanupOtherCNI" classname="github.com/cilium/cilium/daemon/cmd/cni" time="0.000">
			<system-out><![CDATA[level=info msg="Renaming non-Cilium CNI configuration file 01-someoneelse.conf to 01-someoneelse.conf.cilium_bak"
level=info msg="Renaming non-Cilium CNI configuration file 44-dontkeep.json to 44-dontkeep.json.cilium_bak"
level=info msg="Renaming non-Cilium CNI configuration file 99-test.conflist to 99-test.conflist.cilium_bak"]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/daemon/cmd/cni	coverage: 56.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator" tests="0" failures="0" errors="0" id="63" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/operator/auth" tests="0" failures="0" errors="0" id="64" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/operator/auth/identity" tests="0" failures="0" errors="0" id="65" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/operator/api" tests="6" failures="0" errors="0" id="66" hostname="kind-bpf-next" time="0.106" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestHealthHandlerK8sDisabled" classname="github.com/cilium/cilium/operator/api" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="44.002µs" function="api.TestHealthHandlerK8sDisabled.func1 (health_test.go:34)" subsys=hive
level=info msg=Invoked duration="57.398µs" function="api.TestHealthHandlerK8sDisabled.func5 (health_test.go:55)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg=Stopping subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestHealthHandlerK8sEnabled" classname="github.com/cilium/cilium/operator/api" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="58.539µs" function="api.TestHealthHandlerK8sEnabled.func4 (health_test.go:104)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg=Stopping subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestMetricsHandlerWithoutMetrics" classname="github.com/cilium/cilium/operator/api" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="51.696µs" function="api.TestMetricsHandlerWithoutMetrics.func2 (metrics_test.go:48)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg=Stopping subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestMetricsHandlerWithMetrics" classname="github.com/cilium/cilium/operator/api" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="17.352µs" function="api.TestMetricsHandlerWithMetrics.func2 (metrics_test.go:103)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Registering Operator metrics" subsys=metrics
level=info msg="Start hook executed" duration="779.903µs" function="api.TestMetricsHandlerWithMetrics.func2.1 (metrics_test.go:105)" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Shutting down metrics server" subsys=metrics
level=info msg="Stop hook executed" duration="17.382µs" function="api.TestMetricsHandlerWithMetrics.func2.2 (metrics_test.go:130)" subsys=hive
level=info msg="Received shutdown signal" subsys=metrics
level=info msg="Metrics server shutdown successfully" subsys=metrics]]></system-out>
		</testcase>
		<testcase name="TestAPIServerK8sDisabled" classname="github.com/cilium/cilium/operator/api" time="0.040">
			<system-out><![CDATA[level=info msg=Invoked duration="12.453µs" function="api.TestAPIServerK8sDisabled.func1 (server_test.go:33)" subsys=hive
level=info msg=Invoked duration=21.447362ms function="api.TestAPIServerK8sDisabled.func5 (server_test.go:53)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=14.794436ms function="*api.server.Start" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="88.495µs" function="*api.server.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestAPIServerK8sEnabled" classname="github.com/cilium/cilium/operator/api" time="0.030">
			<system-out><![CDATA[level=info msg=Invoked duration=11.153923ms function="api.TestAPIServerK8sEnabled.func4 (server_test.go:111)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=11.473448ms function="*api.server.Start" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="17.623µs" function="*api.server.Stop" subsys=hive]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/api	coverage: 72.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/auth/spire" tests="11" failures="0" errors="0" id="67" hostname="kind-bpf-next" time="0.013" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestClient_Upsert" classname="github.com/cilium/cilium/operator/auth/spire" time="0.000"></testcase>
		<testcase name="TestClient_Upsert/client_not_initialized" classname="github.com/cilium/cilium/operator/auth/spire" time="0.000"></testcase>
		<testcase name="TestClient_Upsert/unable_to_list_entry_due_to_unknown_error" classname="github.com/cilium/cilium/operator/auth/spire" time="0.000"></testcase>
		<testcase name="TestClient_Upsert/entry_does_not_exist_with_not_found_error" classname="github.com/cilium/cilium/operator/auth/spire" time="0.000"></testcase>
		<testcase name="TestClient_Upsert/entry_exists" classname="github.com/cilium/cilium/operator/auth/spire" time="0.000"></testcase>
		<testcase name="TestClient_Delete" classname="github.com/cilium/cilium/operator/auth/spire" time="0.000"></testcase>
		<testcase name="TestClient_Delete/client_not_initialized" classname="github.com/cilium/cilium/operator/auth/spire" time="0.000"></testcase>
		<testcase name="TestClient_Delete/unable_to_list_entries_due_to_unknown_error" classname="github.com/cilium/cilium/operator/auth/spire" time="0.000"></testcase>
		<testcase name="TestClient_Delete/unable_to_list_entries_due_to_not_found_error" classname="github.com/cilium/cilium/operator/auth/spire" time="0.000"></testcase>
		<testcase name="TestClient_Delete/entry_does_not_exist" classname="github.com/cilium/cilium/operator/auth/spire" time="0.000"></testcase>
		<testcase name="TestClient_Delete/entry_exists" classname="github.com/cilium/cilium/operator/auth/spire" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/auth/spire	coverage: 47.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/metrics" tests="0" failures="0" errors="0" id="68" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/operator/option" tests="0" failures="0" errors="0" id="69" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/operator/cmd" tests="8" failures="0" errors="0" id="70" hostname="kind-bpf-next" time="5.655" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestPodCIDRAllocatorOverlap" classname="github.com/cilium/cilium/operator/cmd" time="5.550">
			<system-out><![CDATA[Run 1/5
level=info msg="Starting to synchronize CiliumNode custom resources" subsys=cmd.test
level=info msg="CiliumNodes caches synced with Kubernetes" subsys=cmd.test
Run 2/5
level=info msg="Starting to synchronize CiliumNode custom resources" subsys=cmd.test
level=info msg="CiliumNodes caches synced with Kubernetes" subsys=cmd.test
Run 3/5
level=info msg="Starting to synchronize CiliumNode custom resources" subsys=cmd.test
level=info msg="CiliumNodes caches synced with Kubernetes" subsys=cmd.test
Run 4/5
level=info msg="Starting to synchronize CiliumNode custom resources" subsys=cmd.test
level=info msg="CiliumNodes caches synced with Kubernetes" subsys=cmd.test
Run 5/5
level=info msg="Starting to synchronize CiliumNode custom resources" subsys=cmd.test
level=info msg="CiliumNodes caches synced with Kubernetes" subsys=cmd.test]]></system-out>
		</testcase>
		<testcase name="Test_getOldestLeases" classname="github.com/cilium/cilium/operator/cmd" time="0.000"></testcase>
		<testcase name="Test_getOldestLeases/test-1" classname="github.com/cilium/cilium/operator/cmd" time="0.000"></testcase>
		<testcase name="Test_getOldestLeases/test-2" classname="github.com/cilium/cilium/operator/cmd" time="0.000"></testcase>
		<testcase name="Test_getPath" classname="github.com/cilium/cilium/operator/cmd" time="0.000"></testcase>
		<testcase name="Test_getPath/test-1" classname="github.com/cilium/cilium/operator/cmd" time="0.000"></testcase>
		<testcase name="Test_getPath/test-2" classname="github.com/cilium/cilium/operator/cmd" time="0.000"></testcase>
		<testcase name="TestOperatorHive" classname="github.com/cilium/cilium/operator/cmd" time="0.040">
			<system-out><![CDATA[level=info msg=Invoked duration="629.714µs" function="pprof.glob..func1 (cell.go:51)" subsys=hive
level=info msg=Invoked duration="68.007µs" function="gops.registerGopsHooks (cell.go:39)" subsys=hive
level=info msg=Invoked duration="137.807µs" function="cmd.registerOperatorHooks (root.go:156)" subsys=hive
level=info msg=Invoked duration=32.804063ms function="api.glob..func1 (cell.go:32)" subsys=hive
level=info msg=Invoked duration="157.093µs" function="apis.createCRDs (cell.go:63)" subsys=hive
level=info msg=Invoked duration="362.877µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Invoked duration="165.037µs" function="auth.registerIdentityWatcher (watcher.go:43)" subsys=hive
level=info msg=Invoked duration="444.177µs" function="cmd.registerLegacyOnLeader (root.go:362)" subsys=hive
level=info msg=Invoked duration="113.642µs" function="identitygc.registerGC (gc.go:79)" subsys=hive]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/cmd	coverage: 27.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/identitygc" tests="4" failures="0" errors="0" id="71" hostname="kind-bpf-next" time="0.595" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestIdentitiesGC" classname="github.com/cilium/cilium/operator/identitygc" time="0.550">
			<system-out><![CDATA[level=info msg=Invoked duration="124.131µs" function="identitygc.TestIdentitiesGC.func3 (gc_test.go:70)" subsys=hive
level=info msg=Invoked duration="72.064µs" function="identitygc.setupCiliumEndpointWatcher (gc_test.go:217)" subsys=hive
level=info msg=Invoked duration="43.481µs" function="identitygc.registerGC (gc.go:79)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="6.703µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration=100.185274ms function="identitygc.setupCiliumEndpointWatcher.func1 (gc_test.go:224)" subsys=hive
level=info msg="Starting CRD identity garbage collector" interval=50ms subsys=hive
level=info msg="Start hook executed" duration="172.133µs" function="identitygc.registerGC.func1 (gc.go:107)" subsys=hive
level=info msg="Marking identity for later deletion" identity=88888 k8sUID= subsys=identity-heartbeat
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="186.136µs" function="identitygc.registerGC.func2 (gc.go:119)" subsys=hive
level=info msg="Stop hook executed" duration="2.706µs" function="identitygc.setupCiliumEndpointWatcher.func2 (gc_test.go:280)" subsys=hive
level=info msg="Stop hook executed" duration="27.281µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/operator/identitygc" time="0.000"></testcase>
		<testcase name="Test/OperatorTestSuite" classname="github.com/cilium/cilium/operator/identitygc" time="0.000"></testcase>
		<testcase name="Test/OperatorTestSuite/TestIdentityHeartbeatStore" classname="github.com/cilium/cilium/operator/identitygc" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/identitygc	coverage: 58.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" tests="32" failures="0" errors="0" id="72" hostname="kind-bpf-next" time="0.736" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestCepToCESCounts" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCepToCESCounts/Insert_CEPs_-_1" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCepToCESCounts/Insert_CEPs_-_2" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCepToCESCounts/Insert_CEPs_-_3" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCepToCESCounts/Insert_CEPs_-_4" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCepToCESCounts/Check_same_CEP-name_with_CES_name" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCepToCESCounts/Insert_CEPs_-_1#01" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCepToCESCounts/Insert_CEPs_-_2#01" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCepToCESCounts/Insert_CEPs_-_3#01" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCepToCESCounts/Insert_CEPs_-_4#01" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCepToCESCounts/Check_same_CEP-name_with_CES_name#01" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestRemoveStaleCEPEntries" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.710"></testcase>
		<testcase name="TestRemoveStaleCEPEntries/No_stale_CEPs" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.100">
			<system-out><![CDATA[level=info msg="CES controller workqueue configuration" subsys=ces-controller workQueueBurstLimit=20 workQueueQPSLimit=10 workQueueSyncBackOff=1s
level=info msg="Remove stale and duplicated CEP entries in CES" subsys=ces-controller]]></system-out>
		</testcase>
		<testcase name="TestRemoveStaleCEPEntries/Remove_stale_CEP" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.100">
			<system-out><![CDATA[level=info msg="CES controller workqueue configuration" subsys=ces-controller workQueueBurstLimit=20 workQueueQPSLimit=10 workQueueSyncBackOff=1s
level=info msg="Remove stale and duplicated CEP entries in CES" subsys=ces-controller]]></system-out>
		</testcase>
		<testcase name="TestRemoveStaleCEPEntries/Remove_duplicated_CEP_from_single_slice" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.100">
			<system-out><![CDATA[level=info msg="CES controller workqueue configuration" subsys=ces-controller workQueueBurstLimit=20 workQueueQPSLimit=10 workQueueSyncBackOff=1s
level=info msg="Remove stale and duplicated CEP entries in CES" subsys=ces-controller]]></system-out>
		</testcase>
		<testcase name="TestRemoveStaleCEPEntries/Remove_duplicated_CEP_from_separate_slice" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.100">
			<system-out><![CDATA[level=info msg="CES controller workqueue configuration" subsys=ces-controller workQueueBurstLimit=20 workQueueQPSLimit=10 workQueueSyncBackOff=1s
level=info msg="Remove stale and duplicated CEP entries in CES" subsys=ces-controller]]></system-out>
		</testcase>
		<testcase name="TestRemoveStaleCEPEntries/Remove_old_CEP_from_first_slice" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.100">
			<system-out><![CDATA[level=info msg="CES controller workqueue configuration" subsys=ces-controller workQueueBurstLimit=20 workQueueQPSLimit=10 workQueueSyncBackOff=1s
level=info msg="Remove stale and duplicated CEP entries in CES" subsys=ces-controller]]></system-out>
		</testcase>
		<testcase name="TestRemoveStaleCEPEntries/Remove_old_CEP_from_second_slice" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.100">
			<system-out><![CDATA[level=info msg="CES controller workqueue configuration" subsys=ces-controller workQueueBurstLimit=20 workQueueQPSLimit=10 workQueueSyncBackOff=1s
level=info msg="Remove stale and duplicated CEP entries in CES" subsys=ces-controller]]></system-out>
		</testcase>
		<testcase name="TestRemoveStaleCEPEntries/Big_remove_case_with_multiple_stale_and_duplicated_CEPs_from_multiple_slices" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.100">
			<system-out><![CDATA[level=info msg="CES controller workqueue configuration" subsys=ces-controller workQueueBurstLimit=20 workQueueQPSLimit=10 workQueueSyncBackOff=1s
level=info msg="Remove stale and duplicated CEP entries in CES" subsys=ces-controller]]></system-out>
		</testcase>
		<testcase name="TestInsertAndRemoveCEPsInCache" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestInsertAndRemoveCEPsInCache/Test_Inserting_CEPs_in_cache_and_count_number_of_CEPs_and_CESs" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestInsertAndRemoveCEPsInCache/Test_Removing_CEPs_from_cache_and_check_number_of_CEPs_and_CESs" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestInsertAndRemoveCEPsInCache/Test_InsertCEPInCache_always_adds_CEPs_to_the_largest_CES" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestDeepCopyCEPs" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestDeepCopyCEPs/Test_Inserting_CEPs_in_cache_and_count_number_of_CEPs_and_CESs" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestRemovedCEPs" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestRemovedCEPs/Test_Deleting_CEPs_and_Insert" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCiliumReconcile" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCiliumReconcile/Create_CESs,_check_for_any_errors_and__compare_returned_value_from_api-server_with_local_data" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCiliumReconcile/Attempt_to_create_empty_CES_and_check_that_it_is_not_created" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCiliumReconcile/Update_CESs,_check_for_any_errors_and__compare_returned_value_from_api-server_with_local_data" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<testcase name="TestCiliumReconcile/Delete_CESs,_check_errors_from_api-server_and_match_CESs_and_CEPs_count_" classname="github.com/cilium/cilium/operator/pkg/ciliumendpointslice" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/pkg/ciliumendpointslice	coverage: 68.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/pkg/ciliumenvoyconfig" tests="9" failures="0" errors="0" id="73" hostname="kind-bpf-next" time="0.045" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test_grpcHttpConnectionManagerMutator" classname="github.com/cilium/cilium/operator/pkg/ciliumenvoyconfig" time="0.000"></testcase>
		<testcase name="Test_grpcHttpConnectionManagerMutator/mutate_upgradeConfigs" classname="github.com/cilium/cilium/operator/pkg/ciliumenvoyconfig" time="0.000"></testcase>
		<testcase name="Test_lbModeClusterMutator" classname="github.com/cilium/cilium/operator/pkg/ciliumenvoyconfig" time="0.000"></testcase>
		<testcase name="Test_lbModeClusterMutator/no_ops" classname="github.com/cilium/cilium/operator/pkg/ciliumenvoyconfig" time="0.000"></testcase>
		<testcase name="Test_lbModeClusterMutator/mutate_lb_policy_to_round_robin" classname="github.com/cilium/cilium/operator/pkg/ciliumenvoyconfig" time="0.000"></testcase>
		<testcase name="Test_lbModeClusterMutator/mutate_lb_policy_to_least_request" classname="github.com/cilium/cilium/operator/pkg/ciliumenvoyconfig" time="0.000"></testcase>
		<testcase name="Test_getClusterResources" classname="github.com/cilium/cilium/operator/pkg/ciliumenvoyconfig" time="0.000"></testcase>
		<testcase name="Test_getRouteConfigurationResource" classname="github.com/cilium/cilium/operator/pkg/ciliumenvoyconfig" time="0.000"></testcase>
		<testcase name="Test_getListenerResource" classname="github.com/cilium/cilium/operator/pkg/ciliumenvoyconfig" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/pkg/ciliumenvoyconfig	coverage: 25.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/pkg/gateway-api" tests="104" failures="0" errors="0" id="74" hostname="kind-bpf-next" skipped="1" time="0.215" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestConformance" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<skipped message="Skipped"><![CDATA[    conformance_test.go:42: Set GATEWAY_API_CONFORMANCE_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test_hasMatchingController" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_hasMatchingController/invalid_object" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_hasMatchingController/gateway_is_matched_by_controller" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_hasMatchingController/gateway_is_linked_to_non-existent_class" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=error msg="Unable to get GatewayClass" controller=gateway error="gatewayclasses.gateway.networking.k8s.io \"non-existent\" not found" resource= subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_getGatewaysForSecret" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_getGatewaysForSecret/secret_is_used_in_gateway" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_getGatewaysForSecret/secret_is_not_used_in_gateway" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_getGatewaysForNamespace" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_getGatewaysForNamespace/with_default_namespace" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_getGatewaysForNamespace/with_another_namespace" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_getGatewaysForNamespace/with_namespace-with-allowed-gateway-selector" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_getGatewaysForNamespace/with_namespace-with-disallowed-gateway-selector" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_success" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_success/success" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_fail" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_fail/fail" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/unsupported_kind" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/mismatch_kind_for_GatewayClass" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/mismatch_kind_for_Gateway" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/mismatch_kind_for_HTTPRoute" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/no_change_in_GatewayClass_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/change_in_GatewayClass_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/only_change_LastTransitionTime_in_GatewayClass_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/no_change_in_gateway_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/change_in_gateway_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/only_change_LastTransitionTime_in_gateway_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/no_change_in_HTTPRoute_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/change_in_HTTP_route_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/only_change_LastTransitionTime_in_HTTPRoute_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/no_change_in_TLSRoute_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/change_in_TLSRoute_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_onlyStatusChanged/only_change_LastTransitionTime_in_TLSRoute_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayReconciler_Reconcile" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.040"></testcase>
		<testcase name="Test_gatewayReconciler_Reconcile/non-existent_gateway" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling Gateway" controller=gateway resource=default/non-existent-gateway subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_gatewayReconciler_Reconcile/non-existent_gateway_class" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling Gateway" controller=gateway resource=default/gateway-with-non-existent-gateway-class subsys=gateway-controller
level=error msg="Unable to get GatewayClass" controller=gateway error="gatewayclasses.gateway.networking.k8s.io \"non-existent-gateway-class\" not found" gatewayClass=non-existent-gateway-class resource=default/gateway-with-non-existent-gateway-class subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_gatewayReconciler_Reconcile/valid_http_gateway" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.020">
			<system-out><![CDATA[level=info msg="Reconciling Gateway" controller=gateway resource=default/valid-gateway subsys=gateway-controller
level=error msg="Address is not ready" controller=gateway error="load balancer status is not ready" resource=default/valid-gateway subsys=gateway-controller
level=info msg="Reconciling Gateway" controller=gateway resource=default/valid-gateway subsys=gateway-controller
level=info msg="Successfully reconciled Gateway" controller=gateway resource=default/valid-gateway subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_gatewayReconciler_Reconcile/valid_tls_gateway" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.010">
			<system-out><![CDATA[level=info msg="Reconciling Gateway" controller=gateway resource=default/valid-tlsroute-gateway subsys=gateway-controller
level=error msg="Address is not ready" controller=gateway error="load balancer status is not ready" resource=default/valid-tlsroute-gateway subsys=gateway-controller
level=info msg="Reconciling Gateway" controller=gateway resource=default/valid-tlsroute-gateway subsys=gateway-controller
level=info msg="Successfully reconciled Gateway" controller=gateway resource=default/valid-tlsroute-gateway subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_isValidPemFormat" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_isValidPemFormat/valid_cert_pem" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_isValidPemFormat/value_key_pem" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_isValidPemFormat/multiple_valid_pem_blocks" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_isValidPemFormat/invalid_first_block" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_isValidPemFormat/invalid_subsequent_block" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_isValidPemFormat/invalid_pem" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayStatusScheduledCondition" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayStatusScheduledCondition/scheduled" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayStatusScheduledCondition/non-scheduled" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayStatusReadyCondition" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayStatusReadyCondition/ready" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayStatusReadyCondition/unready" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayListenerProgrammedCondition" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayListenerProgrammedCondition/ready" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayListenerProgrammedCondition/unready" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayClassReconciler_Reconcile" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayClassReconciler_Reconcile/no_gateway_class" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling GatewayClass" controller=gatewayclass resource=/non-existing-gw-class subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_gatewayClassReconciler_Reconcile/gateway_class_exists_but_being_deleted" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling GatewayClass" controller=gatewayclass resource=/deleting-gw-class subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_gatewayClassReconciler_Reconcile/gateway_class_exists_and_active" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling GatewayClass" controller=gatewayclass resource=/dummy-gw-class subsys=gateway-controller
level=info msg="Successfully reconciled GatewayClass" controller=gatewayclass resource=/dummy-gw-class subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_gatewayClassReconciler_Reconcile/non-matching_controller_name" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling GatewayClass" controller=gatewayclass resource=/non-matching-gw-class subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_gatewayClassAcceptedCondition" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayClassAcceptedCondition/accepted_gateway_class" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_gatewayClassAcceptedCondition/non-accepted_gateway_class" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_matchesControllerName" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_matchesControllerName/matches" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_matchesControllerName/does_not_match" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_matchesControllerName/not_a_GatewayClass" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_httpRouteReconciler_Reconcile" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_httpRouteReconciler_Reconcile/no_http_route" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling HTTPRoute" controller=httpRoute resource=default/non-existing-http-route subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_httpRouteReconciler_Reconcile/http_route_exists_but_being_deleted" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling HTTPRoute" controller=httpRoute resource=default/deleting-http-route subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_httpRouteReconciler_Reconcile/valid_http_route" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling HTTPRoute" controller=httpRoute resource=default/valid-http-route subsys=gateway-controller
level=info msg="Successfully reconciled HTTPRoute" controller=httpRoute resource=default/valid-http-route subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_httpRouteReconciler_Reconcile/http_route_with_nonexistent_backend" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling HTTPRoute" controller=httpRoute resource=default/http-route-with-nonexistent-backend subsys=gateway-controller
level=info msg="Successfully reconciled HTTPRoute" controller=httpRoute resource=default/http-route-with-nonexistent-backend subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_httpRouteReconciler_Reconcile/http_route_with_nonexistent_gateway" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling HTTPRoute" controller=httpRoute resource=default/http-route-with-nonexistent-gateway subsys=gateway-controller
level=info msg="Successfully reconciled HTTPRoute" controller=httpRoute resource=default/http-route-with-nonexistent-gateway subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_httpRouteReconciler_Reconcile/http_route_with_valid_but_not_allowed_gateway" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling HTTPRoute" controller=httpRoute resource=default/http-route-with-not-allowed-gateway subsys=gateway-controller
level=info msg="Successfully reconciled HTTPRoute" controller=httpRoute resource=default/http-route-with-not-allowed-gateway subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_httpRouteReconciler_Reconcile/http_route_with_non-matching_hostname" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling HTTPRoute" controller=httpRoute resource=default/http-route-with-non-matching-hostname subsys=gateway-controller
level=info msg="Successfully reconciled HTTPRoute" controller=httpRoute resource=default/http-route-with-non-matching-hostname subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_httpRouteReconciler_Reconcile/http_route_with_cross_namespace_backend" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling HTTPRoute" controller=httpRoute resource=default/http-route-with-cross-namespace-backend subsys=gateway-controller
level=info msg="Successfully reconciled HTTPRoute" controller=httpRoute resource=default/http-route-with-cross-namespace-backend subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_httpRouteReconciler_Reconcile/http_route_with_un-supported_backend" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000">
			<system-out><![CDATA[level=info msg="Reconciling HTTPRoute" controller=httpRoute resource=default/http-route-with-unsupported-backend subsys=gateway-controller
level=info msg="Successfully reconciled HTTPRoute" controller=httpRoute resource=default/http-route-with-unsupported-backend subsys=gateway-controller]]></system-out>
		</testcase>
		<testcase name="Test_httpRouteAcceptedCondition" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_httpRouteAcceptedCondition/accepted_http_route" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_httpRouteAcceptedCondition/non-accepted_http_route" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_httpBackendNotFoundRouteCondition" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_httpBackendNotFoundRouteCondition/http_backend_not_found" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_httpNoMatchingListenerHostnameRouteCondition" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_httpNoMatchingListenerHostnameRouteCondition/no_matching_listener_hostname" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_httpRefNotPermittedRouteCondition" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_httpRefNotPermittedRouteCondition/ref_not_permitted" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_httpInvalidKindRouteCondition" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_httpInvalidKindRouteCondition/invalid_kind_route" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_mergeHTTPRouteStatusConditions" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_mergeHTTPRouteStatusConditions/create_new_http_route_status" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_mergeHTTPRouteStatusConditions/Update_the_existing_http_route_status_with_new_condition" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_mergeHTTPRouteStatusConditions/Update_the_existing_http_route_status_with_existing_condition" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_merge" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_merge/status_updated" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_merge/reason_updated" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_merge/message_updated" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_merge/new_condition" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_conditionChanged" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_conditionChanged/nil_and_non-nil_current_are_equal" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_conditionChanged/empty_slices_should_be_equal" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_conditionChanged/condition_LastTransitionTime_should_be_ignored" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_conditionChanged/check_condition_reason_differs" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_conditionChanged/condition_status_differs" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<testcase name="Test_conditionChanged/observed_generation_differs" classname="github.com/cilium/cilium/operator/pkg/gateway-api" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/pkg/gateway-api	coverage: 42.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/pkg/ingress" tests="19" failures="0" errors="0" id="75" hostname="kind-bpf-next" time="0.084" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test_ingressClassHandleEvent" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_ingressClassHandleEvent/unknown_event" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_ingressClassHandleEvent/delete_ingressClass" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_ingressClassHandleEvent/add_ingressClass" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_ingressClassHandleEvent/add_ingressClass/no_ops_if_name_is_not_us" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_ingressClassHandleEvent/add_ingressClass/with_correct_name" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_ingressClassHandleEvent/update_ingressClass" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_ingressClassHandleEvent/update_ingressClass/no_ops_if_not_ours" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_ingressClassHandleEvent/update_ingressClass/with_change_in_annotations_on_correct_name" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_handleEvent" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_handleEvent/unknown_event" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_handleEvent/delete_service" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_handleEvent/add_service" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_handleEvent/add_service/no_ops" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_handleEvent/add_service/with_load_balancer_status" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000">
			<system-out><![CDATA[level=info msg="Notify ingress controller for service ingress" subsys=ingress-controller]]></system-out>
		</testcase>
		<testcase name="Test_handleEvent/update_service" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_handleEvent/update_service/no_ops" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<testcase name="Test_handleEvent/update_service/with_load_balancer_status" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000">
			<system-out><![CDATA[level=info msg="Notify ingress controller for service ingress" subsys=ingress-controller]]></system-out>
		</testcase>
		<testcase name="Test_handleEvent/update_service/with_load_balancer_status_but_being_deleted" classname="github.com/cilium/cilium/operator/pkg/ingress" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/pkg/ingress	coverage: 5.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/pkg/ingress/annotations" tests="12" failures="0" errors="0" id="76" hostname="kind-bpf-next" time="0.024" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestGetAnnotationServiceType" classname="github.com/cilium/cilium/operator/pkg/ingress/annotations" time="0.000"></testcase>
		<testcase name="TestGetAnnotationServiceType/no_service_type_annotation" classname="github.com/cilium/cilium/operator/pkg/ingress/annotations" time="0.000"></testcase>
		<testcase name="TestGetAnnotationServiceType/service_type_annotation_as_LoadBalancer" classname="github.com/cilium/cilium/operator/pkg/ingress/annotations" time="0.000"></testcase>
		<testcase name="TestGetAnnotationServiceType/service_type_annotation_as_NodePort" classname="github.com/cilium/cilium/operator/pkg/ingress/annotations" time="0.000"></testcase>
		<testcase name="TestGetAnnotationSecureNodePort" classname="github.com/cilium/cilium/operator/pkg/ingress/annotations" time="0.000"></testcase>
		<testcase name="TestGetAnnotationSecureNodePort/no_secure_node_port_annotation" classname="github.com/cilium/cilium/operator/pkg/ingress/annotations" time="0.000"></testcase>
		<testcase name="TestGetAnnotationSecureNodePort/secure_node_port_annotation_with_valid_value" classname="github.com/cilium/cilium/operator/pkg/ingress/annotations" time="0.000"></testcase>
		<testcase name="TestGetAnnotationSecureNodePort/secure_node_port_annotation_with_invalid_value" classname="github.com/cilium/cilium/operator/pkg/ingress/annotations" time="0.000"></testcase>
		<testcase name="TestGetAnnotationInsecureNodePort" classname="github.com/cilium/cilium/operator/pkg/ingress/annotations" time="0.000"></testcase>
		<testcase name="TestGetAnnotationInsecureNodePort/no_insecure_node_port_annotation" classname="github.com/cilium/cilium/operator/pkg/ingress/annotations" time="0.000"></testcase>
		<testcase name="TestGetAnnotationInsecureNodePort/insecure_node_port_annotation_with_valid_value" classname="github.com/cilium/cilium/operator/pkg/ingress/annotations" time="0.000"></testcase>
		<testcase name="TestGetAnnotationInsecureNodePort/insecure_node_port_annotation_with_invalid_value" classname="github.com/cilium/cilium/operator/pkg/ingress/annotations" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/pkg/ingress/annotations	coverage: 36.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/pkg/lbipam" tests="33" failures="0" errors="0" id="77" hostname="kind-bpf-next" time="3.947" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestConflictResolution" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.010">
			<system-out><![CDATA[level=info msg=Invoked duration="172.531µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="8.967µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="58.048µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.202µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="8.697µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=warning msg="Pool 'pool-b' conflicts since CIDR '10.0.10.0/24' overlaps CIDR '10.0.10.0/24' from IP Pool 'pool-a'" pool1-cidr=10.0.10.0/24 pool1-name=pool-b pool2-cidr=pool-a pool2-name=10.0.10.0/24 subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg="Stop hook executed" duration="448.596µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="78.997µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="25.578µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestPoolInternalConflict" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="96.62µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="7.995µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.603µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="4.689µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="4.107µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=warning msg="Pool 'pool-a' conflicts since CIDR '10.0.10.0/24' overlaps CIDR '10.0.10.64/28' from IP Pool 'pool-a'" pool1-cidr=10.0.10.0/24 pool1-name=pool-a pool2-cidr=pool-a pool2-name=10.0.10.64/28 subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg="Stop hook executed" duration="287.26µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="26.85µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="11.777µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestAllocHappyPath" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.210">
			<system-out><![CDATA[level=info msg=Invoked duration="121.246µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="6.913µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=832ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=672ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="2.144µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="30.226µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration=3.0703ms function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="19.637µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestServiceDelete" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.200">
			<system-out><![CDATA[level=info msg=Invoked duration="127.096µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="12.293µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.582µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=751ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="5.46µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="22.092µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="35.105µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.078µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestReallocOnInit" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.100">
			<system-out><![CDATA[level=info msg=Invoked duration="105.406µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="8.585µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.283µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=722ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.987µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg="Stop hook executed" duration="51.666µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="16.801µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="13.074µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestAllocOnInit" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.200">
			<system-out><![CDATA[level=info msg=Invoked duration="80.019µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="8.567µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.292µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=721ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="4.008µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="18.304µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="23.854µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="9.788µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestPoolSelectorBasic" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.100">
			<system-out><![CDATA[level=info msg=Invoked duration="86.02µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="8.135µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.212µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=561ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.988µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg="Stop hook executed" duration="30.477µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="14.317µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="9.127µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestPoolSelectorNamespace" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.100">
			<system-out><![CDATA[level=info msg=Invoked duration="82.364µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="8.997µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=952ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=582ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="4.348µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg="Stop hook executed" duration="27.111µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.259µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="7.814µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestChangeServiceType" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.200">
			<system-out><![CDATA[level=info msg=Invoked duration="77.785µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="6.402µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=962ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=511ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="5.731µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="24.525µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="23.795µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="8.195µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestRangesFull" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.200">
			<system-out><![CDATA[level=info msg=Invoked duration="86.642µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="7.684µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=971ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=531ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.407µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="24.356µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="27.071µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="9.858µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestRequestIPs" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.100">
			<system-out><![CDATA[level=info msg=Invoked duration="86.632µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="6.312µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=942ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=512ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.807µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg="Stop hook executed" duration="35.937µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.068µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="10.049µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestAddPool" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.200">
			<system-out><![CDATA[level=info msg=Invoked duration="84.858µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="8.386µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=952ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=521ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.807µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="64.25µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="25.918µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="8.957µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestAddRange" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.200">
			<system-out><![CDATA[level=info msg=Invoked duration="84.858µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="6.782µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=972ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=601ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="4.89µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="28.814µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="27.001µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="6.914µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestDisablePool" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.600">
			<system-out><![CDATA[level=info msg=Invoked duration="82.494µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="5.87µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.212µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=771ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="4.087µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="30.797µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="23.263µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="7.945µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestPoolDelete" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.210">
			<system-out><![CDATA[level=info msg=Invoked duration="139.039µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="8.506µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.504µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=851ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="7.244µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="37.16µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="25.758µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="10.79µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestRangeDelete" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.600">
			<system-out><![CDATA[level=info msg=Invoked duration="130.924µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="7.354µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="2.685µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=471ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.657µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="40.576µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="24.646µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="7.424µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestLBIPAM_serviceIPFamilyRequest" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000"></testcase>
		<testcase name="TestLBIPAM_serviceIPFamilyRequest/No_policy,_No_families,_IPv4" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000"></testcase>
		<testcase name="TestLBIPAM_serviceIPFamilyRequest/No_policy,_No_families,_IPv6" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000"></testcase>
		<testcase name="TestLBIPAM_serviceIPFamilyRequest/No_policy,_No_families,_IPv4/IPv6" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000"></testcase>
		<testcase name="TestLBIPAM_serviceIPFamilyRequest/No_policy,_IPv6_family,_IPv4" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000"></testcase>
		<testcase name="TestLBIPAM_serviceIPFamilyRequest/No_policy,_IPv4_family,_IPv6" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000"></testcase>
		<testcase name="TestLBIPAM_serviceIPFamilyRequest/No_policy,_IPv4/IPv6_family,_No_enabled" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000"></testcase>
		<testcase name="TestLBIPAM_serviceIPFamilyRequest/No_policy,_IPv4/IPv6_family,_No_enabled#01" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000"></testcase>
		<testcase name="TestLBIPAM_serviceIPFamilyRequest/Single_stack,_No_families,_IPv6/IPv4" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000"></testcase>
		<testcase name="TestLBIPAM_serviceIPFamilyRequest/PreferDual,_No_families,_IPv6/IPv4" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000"></testcase>
		<testcase name="TestLBIPAM_serviceIPFamilyRequest/PreferDual,_IPv4_family,_IPv6" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000"></testcase>
		<testcase name="TestLBIPAM_serviceIPFamilyRequest/RequireDual,_IPv4_family,_IPv6" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.000"></testcase>
		<testcase name="TestRemoveServiceLabel" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.100">
			<system-out><![CDATA[level=info msg=Invoked duration="127.868µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="7.785µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.193µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=591ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.837µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg="Stop hook executed" duration="50.735µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="16.772µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="8.455µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestRequestIPWithMismatchedLabel" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.100">
			<system-out><![CDATA[level=info msg=Invoked duration="80.71µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="8.055µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.162µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=681ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="5.31µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg="Stop hook executed" duration="40.716µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="17.442µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="11.1µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestRemoveRequestedIP" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.100">
			<system-out><![CDATA[level=info msg=Invoked duration="94.827µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="7.294µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.002µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=611ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.907µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg="Stop hook executed" duration="60.122µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="16.09µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="14.627µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestNonMatchingLBClass" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.100">
			<system-out><![CDATA[level=info msg=Invoked duration="80.43µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="8.526µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.242µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.192µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="4.778µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg="Stop hook executed" duration=1.462215ms function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="19.351µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="70.381µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestChangePoolSelector" classname="github.com/cilium/cilium/operator/pkg/lbipam" time="0.200">
			<system-out><![CDATA[level=info msg=Invoked duration="60.102µs" function="lbipam.mkTestFixture.func7 (lbipam_fixture_test.go:149)" subsys=hive
level=info msg=Invoked duration="4.689µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=682ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=381ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.876µs" function="*job.group.Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="84.003µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.01µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="68.016µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/pkg/lbipam	coverage: 83.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/pkg/model" tests="13" failures="0" errors="0" id="78" hostname="kind-bpf-next" time="0.005" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestAddSource" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<testcase name="TestComputeHosts" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<testcase name="TestComputeHosts/no_route_hostnames,_no_listener_hostname" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<testcase name="TestComputeHosts/no_route_hostnames" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<testcase name="TestComputeHosts/matching_specific_hostname_exactly" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<testcase name="TestComputeHosts/matching_specific_hostname_on_wildcard" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<testcase name="TestComputeHosts/matching_wildcard_hostname" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<testcase name="TestComputeHosts/matching_wildcard_hostname_exactly" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<testcase name="TestModel_GetListeners" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<testcase name="TestModel_GetListeners/Combine_HTTP_and_TLS_listeners" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<testcase name="TestModel_GetListeners/Only_HTTP_listeners" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<testcase name="TestModel_GetListeners/Only_TLS_listeners" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<testcase name="TestModel_GetListeners/No_listeners" classname="github.com/cilium/cilium/operator/pkg/model" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/pkg/model	coverage: 40.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/pkg/model/ingestion" tests="26" failures="0" errors="0" id="79" hostname="kind-bpf-next" time="0.012" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestHTTPGatewayAPI" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPExactPathMatching" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPRouteListenerHostnameMatching" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPRouteMatchingAcrossRoutes" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPRouteQueryParamMatching" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPRouteRequestRedirect" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPRouteResponseHeaderModifier" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPRouteSimpleSameNamespace" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPRouteCrossNamespace" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPRouteMatching" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPRouteMethodMatching" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPRouteRequestHeaderModifier" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPRouteHeaderMatching" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/Conformance/HTTPRouteHostnameIntersection" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestHTTPGatewayAPI/basic_http" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestTLSGatewayAPI" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestTLSGatewayAPI/basic_http" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestTLSGatewayAPI/Conformance/TLSRouteSimpleSameNamespace" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestIngress" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestIngress/conformance_path_rules_test" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestIngress/cilium_test_ingress" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestIngress/cilium_test_ingress_with_NodePort" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestIngress/conformance_default_backend_test" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestIngress/conformance_default_backend_(legacy_annotation)_test" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestIngress/conformance_default_backend_(legacy_+_new)_test" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<testcase name="TestIngress/conformance_host_rules_test" classname="github.com/cilium/cilium/operator/pkg/model/ingestion" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/pkg/model/ingestion	coverage: 85.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/pkg/model/translation" tests="44" failures="0" errors="0" id="80" hostname="kind-bpf-next" time="0.073" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestWithClusterLbPolicy" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestWithClusterLbPolicy/input_is_nil" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestWithClusterLbPolicy/input_is_not_nil" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestWithOutlierDetection" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestWithOutlierDetection/input_is_nil" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestWithOutlierDetection/input_is_not_nil" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestWithOutlierDetection/input_is_not_nil/enabled" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestWithOutlierDetection/input_is_not_nil/disabled" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestWithConnectionTimeout" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestWithConnectionTimeout/input_is_nil" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestWithConnectionTimeout/input_is_not_nil" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestNewHTTPCluster" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestNewTCPCluster" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestNewHTTPConnectionManager" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestNewHTTPListener" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestNewHTTPListener/without_TLS" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestNewHTTPListener/TLS" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestNewSNIListener" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestNewSNIListener/normal_SNI_listener" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestNewRouteConfiguration" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSortableRoute" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getBackendServices" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getBackendServices/default_backend_listener" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getBackendServices/host_rule_listeners" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getBackendServices/path_rule_listeners" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getBackendServices/complex_ingress" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getServices" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getServices/default_case" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getHTTPRouteListener" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getClusters" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getClusters/default_backend_listener" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getClusters/host_rule_listeners" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getClusters/path_rule_listeners" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getClusters/complex_ingress" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getEnvoyHTTPRouteConfiguration" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getEnvoyHTTPRouteConfiguration/default_backend" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getEnvoyHTTPRouteConfiguration/host_rule" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getEnvoyHTTPRouteConfiguration/path_rules" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getEnvoyHTTPRouteConfiguration/complex_ingress" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getResources" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000"></testcase>
		<testcase name="TestSharedIngressTranslator_getResources/default_backend" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000">
			<system-out><![CDATA[    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.listener.v3.Listener", "name":"listener", "filterChains":[{"filterChainMatch":{"transportProtocol":"raw_buffer"}, "filters":[{"name":"envoy.filters.network.http_connection_manager", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager", "statPrefix":"listener-insecure", "rds":{"routeConfigName":"listener-insecure"}, "httpFilters":[{"name":"envoy.filters.http.router", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}], "useRemoteAddress":true, "upgradeConfigs":[{"upgradeType":"websocket"}]}}]}], "listenerFilters":[{"name":"envoy.filters.listener.tls_inspector", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector"}}], "socketOptions":[{"description":"Enable TCP keep-alive (default to enabled)", "level":"1", "name":"9", "intValue":"1", "state":"STATE_LISTENING"}, {"description":"TCP keep-alive idle time (in seconds) (defaults to 10s)", "level":"6", "name":"4", "intValue":"10", "state":"STATE_LISTENING"}, {"description":"TCP keep-alive probe intervals (in seconds) (defaults to 5s)", "level":"6", "name":"5", "intValue":"5", "state":"STATE_LISTENING"}, {"description":"TCP keep-alive probe max failures.", "level":"6", "name":"6", "intValue":"10", "state":"STATE_LISTENING"}]}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.route.v3.RouteConfiguration", "name":"listener-insecure", "virtualHosts":[{"name":"*", "domains":["*"], "routes":[{"match":{"prefix":"/"}, "route":{"cluster":"random-namespace/default-backend:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}]}]}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.cluster.v3.Cluster", "name":"random-namespace/default-backend:8080", "type":"EDS", "connectTimeout":"5s", "typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions", "commonHttpProtocolOptions":{"idleTimeout":"0s"}, "useDownstreamProtocolConfig":{"http2ProtocolOptions":{}}}}, "outlierDetection":{"splitExternalLocalOriginErrors":true}}]]></system-out>
		</testcase>
		<testcase name="TestSharedIngressTranslator_getResources/host_rules" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000">
			<system-out><![CDATA[    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.listener.v3.Listener", "name":"listener", "filterChains":[{"filterChainMatch":{"transportProtocol":"raw_buffer"}, "filters":[{"name":"envoy.filters.network.http_connection_manager", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager", "statPrefix":"listener-insecure", "rds":{"routeConfigName":"listener-insecure"}, "httpFilters":[{"name":"envoy.filters.http.router", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}], "useRemoteAddress":true, "upgradeConfigs":[{"upgradeType":"websocket"}]}}]}, {"filterChainMatch":{"serverNames":["foo.bar.com"], "transportProtocol":"tls"}, "filters":[{"name":"envoy.filters.network.http_connection_manager", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager", "statPrefix":"listener-secure", "rds":{"routeConfigName":"listener-secure"}, "httpFilters":[{"name":"envoy.filters.http.router", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}], "useRemoteAddress":true, "upgradeConfigs":[{"upgradeType":"websocket"}]}}], "transportSocket":{"name":"envoy.transport_sockets.tls", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext", "commonTlsContext":{"tlsCertificateSdsSecretConfigs":[{"name":"/random-namespace-conformance-tls", "sdsConfig":{"apiConfigSource":{"apiType":"GRPC", "transportApiVersion":"V3", "grpcServices":[{"envoyGrpc":{"clusterName":"xds-grpc-cilium"}}]}, "resourceApiVersion":"V3"}}]}}}}], "listenerFilters":[{"name":"envoy.filters.listener.tls_inspector", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector"}}], "socketOptions":[{"description":"Enable TCP keep-alive (default to enabled)", "level":"1", "name":"9", "intValue":"1", "state":"STATE_LISTENING"}, {"description":"TCP keep-alive idle time (in seconds) (defaults to 10s)", "level":"6", "name":"4", "intValue":"10", "state":"STATE_LISTENING"}, {"description":"TCP keep-alive probe intervals (in seconds) (defaults to 5s)", "level":"6", "name":"5", "intValue":"5", "state":"STATE_LISTENING"}, {"description":"TCP keep-alive probe max failures.", "level":"6", "name":"6", "intValue":"10", "state":"STATE_LISTENING"}]}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.route.v3.RouteConfiguration", "name":"listener-insecure", "virtualHosts":[{"name":"*.foo.com", "domains":["*.foo.com", "*.foo.com:*"], "routes":[{"match":{"safeRegex":{"regex":"(/.*)?$"}, "headers":[{"name":":authority", "stringMatch":{"safeRegex":{"regex":"^[^.]+[.]foo[.]com$"}}}]}, "route":{"cluster":"random-namespace/wildcard-foo-com:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}]}, {"name":"foo.bar.com", "domains":["foo.bar.com", "foo.bar.com:*"], "routes":[{"match":{"safeRegex":{"regex":"(/.*)?$"}}, "route":{"weightedClusters":{"clusters":[{"name":"random-namespace/foo-bar-com:http", "weight":1}, {"name":"random-namespace/foo-bar-com:http", "weight":1}]}, "maxStreamDuration":{"maxStreamDuration":"0s"}}}]}]}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.route.v3.RouteConfiguration", "name":"listener-secure", "virtualHosts":[{"name":"foo.bar.com", "domains":["foo.bar.com", "foo.bar.com:*"], "routes":[{"match":{"safeRegex":{"regex":"(/.*)?$"}}, "route":{"weightedClusters":{"clusters":[{"name":"random-namespace/foo-bar-com:http", "weight":1}, {"name":"random-namespace/foo-bar-com:http", "weight":1}]}, "maxStreamDuration":{"maxStreamDuration":"0s"}}}]}]}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.cluster.v3.Cluster", "name":"random-namespace/foo-bar-com:http", "type":"EDS", "connectTimeout":"5s", "typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions", "commonHttpProtocolOptions":{"idleTimeout":"0s"}, "useDownstreamProtocolConfig":{"http2ProtocolOptions":{}}}}, "outlierDetection":{"splitExternalLocalOriginErrors":true}}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.cluster.v3.Cluster", "name":"random-namespace/wildcard-foo-com:8080", "type":"EDS", "connectTimeout":"5s", "typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions", "commonHttpProtocolOptions":{"idleTimeout":"0s"}, "useDownstreamProtocolConfig":{"http2ProtocolOptions":{}}}}, "outlierDetection":{"splitExternalLocalOriginErrors":true}}]]></system-out>
		</testcase>
		<testcase name="TestSharedIngressTranslator_getResources/path_rules" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000">
			<system-out><![CDATA[    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.listener.v3.Listener", "name":"listener", "filterChains":[{"filterChainMatch":{"transportProtocol":"raw_buffer"}, "filters":[{"name":"envoy.filters.network.http_connection_manager", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager", "statPrefix":"listener-insecure", "rds":{"routeConfigName":"listener-insecure"}, "httpFilters":[{"name":"envoy.filters.http.router", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}], "useRemoteAddress":true, "upgradeConfigs":[{"upgradeType":"websocket"}]}}]}], "listenerFilters":[{"name":"envoy.filters.listener.tls_inspector", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector"}}], "socketOptions":[{"description":"Enable TCP keep-alive (default to enabled)", "level":"1", "name":"9", "intValue":"1", "state":"STATE_LISTENING"}, {"description":"TCP keep-alive idle time (in seconds) (defaults to 10s)", "level":"6", "name":"4", "intValue":"10", "state":"STATE_LISTENING"}, {"description":"TCP keep-alive probe intervals (in seconds) (defaults to 5s)", "level":"6", "name":"5", "intValue":"5", "state":"STATE_LISTENING"}, {"description":"TCP keep-alive probe max failures.", "level":"6", "name":"6", "intValue":"10", "state":"STATE_LISTENING"}]}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.route.v3.RouteConfiguration", "name":"listener-insecure", "virtualHosts":[{"name":"exact-path-rules", "domains":["exact-path-rules", "exact-path-rules:*"], "routes":[{"match":{"path":"/foo"}, "route":{"cluster":"random-namespace/foo-exact:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}]}, {"name":"mixed-path-rules", "domains":["mixed-path-rules", "mixed-path-rules:*"], "routes":[{"match":{"path":"/foo"}, "route":{"cluster":"random-namespace/foo-exact:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}, {"match":{"safeRegex":{"regex":"/foo(/.*)?$"}}, "route":{"cluster":"random-namespace/foo-prefix:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}]}, {"name":"prefix-path-rules", "domains":["prefix-path-rules", "prefix-path-rules:*"], "routes":[{"match":{"safeRegex":{"regex":"/aaa/bbb(/.*)?$"}}, "route":{"cluster":"random-namespace/aaa-slash-bbb-prefix:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}, {"match":{"safeRegex":{"regex":"/foo(/.*)?$"}}, "route":{"cluster":"random-namespace/foo-prefix:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}, {"match":{"safeRegex":{"regex":"/aaa(/.*)?$"}}, "route":{"cluster":"random-namespace/aaa-prefix:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}]}, {"name":"trailing-slash-path-rules", "domains":["trailing-slash-path-rules", "trailing-slash-path-rules:*"], "routes":[{"match":{"path":"/foo/"}, "route":{"cluster":"random-namespace/foo-slash-exact:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}, {"match":{"safeRegex":{"regex":"/aaa/bbb(/.*)?$"}}, "route":{"cluster":"random-namespace/aaa-slash-bbb-slash-prefix:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}]}]}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.cluster.v3.Cluster", "name":"random-namespace/aaa-prefix:8080", "type":"EDS", "connectTimeout":"5s", "typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions", "commonHttpProtocolOptions":{"idleTimeout":"0s"}, "useDownstreamProtocolConfig":{"http2ProtocolOptions":{}}}}, "outlierDetection":{"splitExternalLocalOriginErrors":true}}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.cluster.v3.Cluster", "name":"random-namespace/aaa-slash-bbb-prefix:8080", "type":"EDS", "connectTimeout":"5s", "typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions", "commonHttpProtocolOptions":{"idleTimeout":"0s"}, "useDownstreamProtocolConfig":{"http2ProtocolOptions":{}}}}, "outlierDetection":{"splitExternalLocalOriginErrors":true}}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.cluster.v3.Cluster", "name":"random-namespace/aaa-slash-bbb-slash-prefix:8080", "type":"EDS", "connectTimeout":"5s", "typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions", "commonHttpProtocolOptions":{"idleTimeout":"0s"}, "useDownstreamProtocolConfig":{"http2ProtocolOptions":{}}}}, "outlierDetection":{"splitExternalLocalOriginErrors":true}}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.cluster.v3.Cluster", "name":"random-namespace/foo-exact:8080", "type":"EDS", "connectTimeout":"5s", "typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions", "commonHttpProtocolOptions":{"idleTimeout":"0s"}, "useDownstreamProtocolConfig":{"http2ProtocolOptions":{}}}}, "outlierDetection":{"splitExternalLocalOriginErrors":true}}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.cluster.v3.Cluster", "name":"random-namespace/foo-prefix:8080", "type":"EDS", "connectTimeout":"5s", "typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions", "commonHttpProtocolOptions":{"idleTimeout":"0s"}, "useDownstreamProtocolConfig":{"http2ProtocolOptions":{}}}}, "outlierDetection":{"splitExternalLocalOriginErrors":true}}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.cluster.v3.Cluster", "name":"random-namespace/foo-slash-exact:8080", "type":"EDS", "connectTimeout":"5s", "typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions", "commonHttpProtocolOptions":{"idleTimeout":"0s"}, "useDownstreamProtocolConfig":{"http2ProtocolOptions":{}}}}, "outlierDetection":{"splitExternalLocalOriginErrors":true}}]]></system-out>
		</testcase>
		<testcase name="TestSharedIngressTranslator_getResources/complex_ingress" classname="github.com/cilium/cilium/operator/pkg/model/translation" time="0.000">
			<system-out><![CDATA[    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.listener.v3.Listener", "name":"listener", "filterChains":[{"filterChainMatch":{"transportProtocol":"raw_buffer"}, "filters":[{"name":"envoy.filters.network.http_connection_manager", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager", "statPrefix":"listener-insecure", "rds":{"routeConfigName":"listener-insecure"}, "httpFilters":[{"name":"envoy.filters.http.router", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}], "useRemoteAddress":true, "upgradeConfigs":[{"upgradeType":"websocket"}]}}]}, {"filterChainMatch":{"serverNames":["another-very-secure.server.com"], "transportProtocol":"tls"}, "filters":[{"name":"envoy.filters.network.http_connection_manager", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager", "statPrefix":"listener-secure", "rds":{"routeConfigName":"listener-secure"}, "httpFilters":[{"name":"envoy.filters.http.router", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}], "useRemoteAddress":true, "upgradeConfigs":[{"upgradeType":"websocket"}]}}], "transportSocket":{"name":"envoy.transport_sockets.tls", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext", "commonTlsContext":{"tlsCertificateSdsSecretConfigs":[{"name":"/dummy-namespace-tls-another-very-secure-server-com", "sdsConfig":{"apiConfigSource":{"apiType":"GRPC", "transportApiVersion":"V3", "grpcServices":[{"envoyGrpc":{"clusterName":"xds-grpc-cilium"}}]}, "resourceApiVersion":"V3"}}]}}}}, {"filterChainMatch":{"serverNames":["very-secure.server.com"], "transportProtocol":"tls"}, "filters":[{"name":"envoy.filters.network.http_connection_manager", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager", "statPrefix":"listener-secure", "rds":{"routeConfigName":"listener-secure"}, "httpFilters":[{"name":"envoy.filters.http.router", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router"}}], "useRemoteAddress":true, "upgradeConfigs":[{"upgradeType":"websocket"}]}}], "transportSocket":{"name":"envoy.transport_sockets.tls", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext", "commonTlsContext":{"tlsCertificateSdsSecretConfigs":[{"name":"/dummy-namespace-tls-very-secure-server-com", "sdsConfig":{"apiConfigSource":{"apiType":"GRPC", "transportApiVersion":"V3", "grpcServices":[{"envoyGrpc":{"clusterName":"xds-grpc-cilium"}}]}, "resourceApiVersion":"V3"}}]}}}}], "listenerFilters":[{"name":"envoy.filters.listener.tls_inspector", "typedConfig":{"@type":"type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector"}}], "socketOptions":[{"description":"Enable TCP keep-alive (default to enabled)", "level":"1", "name":"9", "intValue":"1", "state":"STATE_LISTENING"}, {"description":"TCP keep-alive idle time (in seconds) (defaults to 10s)", "level":"6", "name":"4", "intValue":"10", "state":"STATE_LISTENING"}, {"description":"TCP keep-alive probe intervals (in seconds) (defaults to 5s)", "level":"6", "name":"5", "intValue":"5", "state":"STATE_LISTENING"}, {"description":"TCP keep-alive probe max failures.", "level":"6", "name":"6", "intValue":"10", "state":"STATE_LISTENING"}]}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.route.v3.RouteConfiguration", "name":"listener-insecure", "virtualHosts":[{"name":"*", "domains":["*"], "routes":[{"match":{"path":"/dummy-path"}, "route":{"cluster":"dummy-namespace/dummy-backend:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}, {"match":{"safeRegex":{"regex":"/another-dummy-path(/.*)?$"}}, "route":{"cluster":"dummy-namespace/another-dummy-backend:8081", "maxStreamDuration":{"maxStreamDuration":"0s"}}}, {"match":{"prefix":"/"}, "route":{"cluster":"dummy-namespace/default-backend:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}]}]}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.route.v3.RouteConfiguration", "name":"listener-secure", "virtualHosts":[{"name":"another-very-secure.server.com", "domains":["another-very-secure.server.com", "another-very-secure.server.com:*"], "routes":[{"match":{"path":"/dummy-path"}, "route":{"cluster":"dummy-namespace/dummy-backend:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}, {"match":{"safeRegex":{"regex":"/another-dummy-path(/.*)?$"}}, "route":{"cluster":"dummy-namespace/another-dummy-backend:8081", "maxStreamDuration":{"maxStreamDuration":"0s"}}}, {"match":{"prefix":"/"}, "route":{"cluster":"dummy-namespace/default-backend:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}]}, {"name":"very-secure.server.com", "domains":["very-secure.server.com", "very-secure.server.com:*"], "routes":[{"match":{"path":"/dummy-path"}, "route":{"cluster":"dummy-namespace/dummy-backend:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}, {"match":{"safeRegex":{"regex":"/another-dummy-path(/.*)?$"}}, "route":{"cluster":"dummy-namespace/another-dummy-backend:8081", "maxStreamDuration":{"maxStreamDuration":"0s"}}}, {"match":{"prefix":"/"}, "route":{"cluster":"dummy-namespace/default-backend:8080", "maxStreamDuration":{"maxStreamDuration":"0s"}}}]}]}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.cluster.v3.Cluster", "name":"dummy-namespace/another-dummy-backend:8081", "type":"EDS", "connectTimeout":"5s", "typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions", "commonHttpProtocolOptions":{"idleTimeout":"0s"}, "useDownstreamProtocolConfig":{"http2ProtocolOptions":{}}}}, "outlierDetection":{"splitExternalLocalOriginErrors":true}}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.cluster.v3.Cluster", "name":"dummy-namespace/default-backend:8080", "type":"EDS", "connectTimeout":"5s", "typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions", "commonHttpProtocolOptions":{"idleTimeout":"0s"}, "useDownstreamProtocolConfig":{"http2ProtocolOptions":{}}}}, "outlierDetection":{"splitExternalLocalOriginErrors":true}}
    translator_test.go:465: {"@type":"type.googleapis.com/envoy.config.cluster.v3.Cluster", "name":"dummy-namespace/dummy-backend:8080", "type":"EDS", "connectTimeout":"5s", "typedExtensionProtocolOptions":{"envoy.extensions.upstreams.http.v3.HttpProtocolOptions":{"@type":"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions", "commonHttpProtocolOptions":{"idleTimeout":"0s"}, "useDownstreamProtocolConfig":{"http2ProtocolOptions":{}}}}, "outlierDetection":{"splitExternalLocalOriginErrors":true}}]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/pkg/model/translation	coverage: 67.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" tests="16" failures="0" errors="0" id="81" hostname="kind-bpf-next" time="0.052" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test_translator_Translate" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.010"></testcase>
		<testcase name="Test_translator_Translate/Basic_HTTP_Listener" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Basic_TLS_SNI_Listener" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPRouteSimpleSameNamespace" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.010"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPRouteCrossNamespace" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPExactPathMatching" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPRouteHeaderMatching" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPRouteHostnameIntersection" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPRouteListenerHostnameMatching" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPRouteMatchingAcrossRoutes" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPRouteMatching" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPRouteMethodMatching" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPRouteQueryParamMatching" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPRouteRequestHeaderModifier" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPRouteRequestRedirect" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HTTPRouteResponseHeaderModifier" classname="github.com/cilium/cilium/operator/pkg/model/translation/gateway-api" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/pkg/model/translation/gateway-api	coverage: 84.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/pkg/model/translation/ingress" tests="9" failures="0" errors="0" id="82" hostname="kind-bpf-next" time="0.056" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test_getService" classname="github.com/cilium/cilium/operator/pkg/model/translation/ingress" time="0.000"></testcase>
		<testcase name="Test_getService/Default_LB_service" classname="github.com/cilium/cilium/operator/pkg/model/translation/ingress" time="0.000"></testcase>
		<testcase name="Test_getService/Invalid_LB_service_annotation,_defaults_to_LoadBalancer" classname="github.com/cilium/cilium/operator/pkg/model/translation/ingress" time="0.000">
			<system-out><![CDATA[level=warning msg="only LoadBalancer and NodePort are supported. Defaulting to LoadBalancer" subsys=ingress-controller svcType=InvalidServiceType]]></system-out>
		</testcase>
		<testcase name="Test_getService/Node_Port_service" classname="github.com/cilium/cilium/operator/pkg/model/translation/ingress" time="0.000"></testcase>
		<testcase name="Test_getEndpointForIngress" classname="github.com/cilium/cilium/operator/pkg/model/translation/ingress" time="0.000"></testcase>
		<testcase name="Test_translator_Translate" classname="github.com/cilium/cilium/operator/pkg/model/translation/ingress" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/DefaultBackend" classname="github.com/cilium/cilium/operator/pkg/model/translation/ingress" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/HostRules" classname="github.com/cilium/cilium/operator/pkg/model/translation/ingress" time="0.000"></testcase>
		<testcase name="Test_translator_Translate/Conformance/PathRules" classname="github.com/cilium/cilium/operator/pkg/model/translation/ingress" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/pkg/model/translation/ingress	coverage: 83.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/operator/watchers" tests="8" failures="0" errors="0" id="83" hostname="kind-bpf-next" time="6.192" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test_performCiliumNodeGC" classname="github.com/cilium/cilium/operator/watchers" time="0.000">
			<system-out><![CDATA[level=info msg="Add CiliumNode to garbage collector candidates" nodeName=invalid-node subsys=watchers
level=info msg="Perform GC for invalid CiliumNode" nodeName=invalid-node subsys=watchers
level=info msg="CiliumNode is garbage collected successfully" nodeName=invalid-node subsys=watchers]]></system-out>
		</testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/operator/watchers" time="6.110"></testcase>
		<testcase name="Test/NodeTaintSuite" classname="github.com/cilium/cilium/operator/watchers" time="6.110"></testcase>
		<testcase name="Test/NodeTaintSuite/TestNodeCondition" classname="github.com/cilium/cilium/operator/watchers" time="0.020">
			<system-out><![CDATA[level=info msg="Cilium pod running for node; marking accordingly" nodeName=k8s1 subsys=watchers
level=info msg="Cilium pod running for node; marking accordingly" nodeName=k8s1 subsys=watchers]]></system-out>
		</testcase>
		<testcase name="Test/NodeTaintSuite/TestNodeConditionIfCiliumAndNodeAreReady" classname="github.com/cilium/cilium/operator/watchers" time="2.030"></testcase>
		<testcase name="Test/NodeTaintSuite/TestNodeConditionIfCiliumIsNotReady" classname="github.com/cilium/cilium/operator/watchers" time="2.010"></testcase>
		<testcase name="Test/NodeTaintSuite/TestNodeTaintWithoutCondition" classname="github.com/cilium/cilium/operator/watchers" time="0.020">
			<system-out><![CDATA[level=info msg="Cilium pod running for node; marking accordingly" nodeName=k8s1 subsys=watchers
level=info msg="Cilium pod running for node; marking accordingly" nodeName=k8s1 subsys=watchers]]></system-out>
		</testcase>
		<testcase name="Test/NodeTaintSuite/TestTaintNodeCiliumDown" classname="github.com/cilium/cilium/operator/watchers" time="2.030">
			<system-out><![CDATA[level=info msg="Cilium pod scheduled but not running for node; setting taint" nodeName=k8s1 subsys=watchers
level=info msg="Cilium pod running for node; marking accordingly" nodeName=k8s1 subsys=watchers]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/operator/watchers	coverage: 25.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/alibabacloud/api" tests="0" failures="0" errors="0" id="84" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/alibabacloud/api/mock" tests="0" failures="0" errors="0" id="85" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/alibabacloud/eni/limits" tests="0" failures="0" errors="0" id="86" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/alibabacloud/eni/types" tests="0" failures="0" errors="0" id="87" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/alibabacloud/metadata" tests="0" failures="0" errors="0" id="88" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/alibabacloud/types" tests="0" failures="0" errors="0" id="89" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/alibabacloud/eni" tests="8" failures="0" errors="0" id="90" hostname="kind-bpf-next" time="0.086" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/alibabacloud/eni" time="0.020"></testcase>
		<testcase name="Test/ENISuite" classname="github.com/cilium/cilium/pkg/alibabacloud/eni" time="0.020"></testcase>
		<testcase name="Test/ENISuite/TestCandidateAndEmtpyInterfaces" classname="github.com/cilium/cilium/pkg/alibabacloud/eni" time="0.010">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=3 numSecurityGroups=1 numVPCs=1 numVSwitches=2 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node3 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-3 name=node3 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=0 emptyInterfaceSlots=1 instanceID=i-3 maxIPsToAllocate=1 name=node3 neededIPs=1 remainingInterfaces=1 selectedInterface= selectedPoolID= subsys=ipam used=0
level=info msg="No more IPs available, creating new ENI" instanceID=i-3 name=node3 securityGroupIDs="[sg-2]" subsys=ipam toAllocate=1 vSwitchID=vsw-2
level=info msg="Created new ENI" eniID=38585728-4d7f-4ffd-8cb7-4d53362936bf instanceID=i-3 name=node3 securityGroupIDs="[sg-2]" subsys=ipam toAllocate=1 vSwitchID=vsw-2
level=info msg="Attached ENI to instance" eniID=38585728-4d7f-4ffd-8cb7-4d53362936bf instanceID=i-3 name=node3 securityGroupIDs="[sg-2]" subsys=ipam toAllocate=1 vSwitchID=vsw-2
level=info msg="Synchronized ENI information" numInstances=3 numSecurityGroups=1 numVPCs=1 numVSwitches=2 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestCreateInterface" classname="github.com/cilium/cilium/pkg/alibabacloud/eni" time="0.000">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=3 numSecurityGroups=1 numVPCs=1 numVSwitches=2 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node2 subsys=ipam
level=info msg="No more IPs available, creating new ENI" securityGroupIDs="[sg-1]" subsys=eni toAllocate=10 vSwitchID=vsw-1
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-1 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=0 emptyInterfaceSlots=2 instanceID=i-1 maxIPsToAllocate=8 name=node1 neededIPs=8 remainingInterfaces=2 selectedInterface= selectedPoolID= subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-2 name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=0 emptyInterfaceSlots=2 instanceID=i-2 maxIPsToAllocate=8 name=node2 neededIPs=8 remainingInterfaces=2 selectedInterface= selectedPoolID= subsys=ipam used=0
level=info msg="No more IPs available, creating new ENI" instanceID=i-2 name=node2 securityGroupIDs="[sg-2]" subsys=ipam toAllocate=8 vSwitchID=vsw-2
level=info msg="Created new ENI" eniID=41a18ec7-7921-4f0f-88bb-48176e0c341d securityGroupIDs="[sg-1]" subsys=eni toAllocate=10 vSwitchID=vsw-1
level=info msg="Attached ENI to instance" eniID=41a18ec7-7921-4f0f-88bb-48176e0c341d securityGroupIDs="[sg-1]" subsys=eni toAllocate=10 vSwitchID=vsw-1
level=info msg="No more IPs available, creating new ENI" securityGroupIDs="[sg-1]" subsys=eni toAllocate=10 vSwitchID=vsw-1
level=info msg="Created new ENI" eniID=fb6a0a90-9f09-4498-b63c-36e7b1dbdef9 securityGroupIDs="[sg-1]" subsys=eni toAllocate=10 vSwitchID=vsw-1
level=info msg="Attached ENI to instance" eniID=fb6a0a90-9f09-4498-b63c-36e7b1dbdef9 securityGroupIDs="[sg-1]" subsys=eni toAllocate=10 vSwitchID=vsw-1]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestGetMaximumAllocatableIPv4" classname="github.com/cilium/cilium/pkg/alibabacloud/eni" time="0.000"></testcase>
		<testcase name="Test/ENISuite/TestNode_allocENIIndex" classname="github.com/cilium/cilium/pkg/alibabacloud/eni" time="0.000"></testcase>
		<testcase name="Test/ENISuite/TestPrepareIPAllocation" classname="github.com/cilium/cilium/pkg/alibabacloud/eni" time="0.010">
			<system-out><![CDATA[level=info msg="No more IPs available, creating new ENI" instanceID=i-1 name=node1 securityGroupIDs="[sg-1]" subsys=ipam toAllocate=8 vSwitchID=vsw-1
level=info msg="Created new ENI" eniID=1a9c2c68-d607-4ff5-82c0-af5b1864c527 instanceID=i-2 name=node2 securityGroupIDs="[sg-2]" subsys=ipam toAllocate=8 vSwitchID=vsw-2
level=info msg="Attached ENI to instance" eniID=1a9c2c68-d607-4ff5-82c0-af5b1864c527 instanceID=i-2 name=node2 securityGroupIDs="[sg-2]" subsys=ipam toAllocate=8 vSwitchID=vsw-2
level=info msg="Synchronized ENI information" numInstances=3 numSecurityGroups=1 numVPCs=1 numVSwitches=2 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=info msg="No more IPs available, creating new ENI" securityGroupIDs="[sg-1]" subsys=eni toAllocate=10 vSwitchID=vsw-1
level=info msg="Created new ENI" eniID=409f199b-e5e9-4e84-a8db-f8abc211aff9 securityGroupIDs="[sg-1]" subsys=eni toAllocate=10 vSwitchID=vsw-1
level=info msg="Attached ENI to instance" eniID=409f199b-e5e9-4e84-a8db-f8abc211aff9 securityGroupIDs="[sg-1]" subsys=eni toAllocate=10 vSwitchID=vsw-1
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-1 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=0 emptyInterfaceSlots=1 instanceID=i-1 maxIPsToAllocate=8 name=node1 neededIPs=8 remainingInterfaces=1 selectedInterface= selectedPoolID= subsys=ipam used=0
level=info msg="No more IPs available, creating new ENI" instanceID=i-1 name=node1 securityGroupIDs="[sg-1]" subsys=ipam toAllocate=8 vSwitchID=vsw-1
level=info msg="Created new ENI" eniID=e6f45dbf-473d-4b1c-9c47-0455d34cbdf7 instanceID=i-1 name=node1 securityGroupIDs="[sg-1]" subsys=ipam toAllocate=8 vSwitchID=vsw-1
level=info msg="Attached ENI to instance" eniID=e6f45dbf-473d-4b1c-9c47-0455d34cbdf7 instanceID=i-1 name=node1 securityGroupIDs="[sg-1]" subsys=ipam toAllocate=8 vSwitchID=vsw-1
level=info msg="Synchronized ENI information" numInstances=3 numSecurityGroups=1 numVPCs=1 numVSwitches=2 subsys=eni
level=info msg="Created new ENI" eniID=512a351d-34fd-44c6-9ac4-6d61e857de5c instanceID=i-1 name=node1 securityGroupIDs="[sg-1]" subsys=ipam toAllocate=8 vSwitchID=vsw-1
level=info msg="Attached ENI to instance" eniID=512a351d-34fd-44c6-9ac4-6d61e857de5c instanceID=i-1 name=node1 securityGroupIDs="[sg-1]" subsys=ipam toAllocate=8 vSwitchID=vsw-1
level=info msg="Synchronized ENI information" numInstances=3 numSecurityGroups=1 numVPCs=1 numVSwitches=2 subsys=eni]]></system-out>
		</testcase>
		<testcase name="TestENIIPAMCapacityAccounting" classname="github.com/cilium/cilium/pkg/alibabacloud/eni" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/alibabacloud/eni	coverage: 68.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/alibabacloud/utils" tests="5" failures="0" errors="0" id="91" hostname="kind-bpf-next" time="0.026" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestGetENIIndexFromTags" classname="github.com/cilium/cilium/pkg/alibabacloud/utils" time="0.000"></testcase>
		<testcase name="TestGetENIIndexFromTags/default_0" classname="github.com/cilium/cilium/pkg/alibabacloud/utils" time="0.000"></testcase>
		<testcase name="TestGetENIIndexFromTags/index_1" classname="github.com/cilium/cilium/pkg/alibabacloud/utils" time="0.000"></testcase>
		<testcase name="TestFillTagWithENIIndex" classname="github.com/cilium/cilium/pkg/alibabacloud/utils" time="0.000"></testcase>
		<testcase name="TestFillTagWithENIIndex/index_1" classname="github.com/cilium/cilium/pkg/alibabacloud/utils" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/alibabacloud/utils	coverage: 88.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/alignchecker" tests="1" failures="0" errors="0" id="92" hostname="kind-bpf-next" time="0.043" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestAlignChecker" classname="github.com/cilium/cilium/pkg/alignchecker" time="0.030"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/alignchecker	coverage: 82.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/api/metrics" tests="0" failures="0" errors="0" id="93" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/allocator" tests="6" failures="0" errors="0" id="94" hostname="kind-bpf-next" time="0.394" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/allocator" time="0.370"></testcase>
		<testcase name="Test/AllocatorSuite" classname="github.com/cilium/cilium/pkg/allocator" time="0.370"></testcase>
		<testcase name="Test/AllocatorSuite/TestAllocateCached" classname="github.com/cilium/cilium/pkg/allocator" time="0.370">
			<system-out><![CDATA[level=info msg="Allocated new global key" key=key0001 subsys=allocator
level=info msg="Allocated new global key" key=key0002 subsys=allocator
level=info msg="Allocated new global key" key=key0003 subsys=allocator
level=info msg="Allocated new global key" key=key0004 subsys=allocator
level=info msg="Allocated new global key" key=key0005 subsys=allocator
level=info msg="Allocated new global key" key=key0006 subsys=allocator
level=info msg="Allocated new global key" key=key0007 subsys=allocator
level=info msg="Allocated new global key" key=key0008 subsys=allocator
level=info msg="Allocated new global key" key=key0009 subsys=allocator
level=info msg="Allocated new global key" key=key0010 subsys=allocator
level=info msg="Allocated new global key" key=key0011 subsys=allocator
level=info msg="Allocated new global key" key=key0012 subsys=allocator
level=info msg="Allocated new global key" key=key0013 subsys=allocator
level=info msg="Allocated new global key" key=key0014 subsys=allocator
level=info msg="Allocated new global key" key=key0015 subsys=allocator
level=info msg="Allocated new global key" key=key0016 subsys=allocator
level=info msg="Allocated new global key" key=key0017 subsys=allocator
level=info msg="Allocated new global key" key=key0018 subsys=allocator
level=info msg="Allocated new global key" key=key0019 subsys=allocator
level=info msg="Allocated new global key" key=key0020 subsys=allocator
level=info msg="Allocated new global key" key=key0021 subsys=allocator
level=info msg="Allocated new global key" key=key0022 subsys=allocator
level=info msg="Allocated new global key" key=key0023 subsys=allocator
level=info msg="Allocated new global key" key=key0024 subsys=allocator
level=info msg="Allocated new global key" key=key0025 subsys=allocator
level=info msg="Allocated new global key" key=key0026 subsys=allocator
level=info msg="Allocated new global key" key=key0027 subsys=allocator
level=info msg="Allocated new global key" key=key0028 subsys=allocator
level=info msg="Allocated new global key" key=key0029 subsys=allocator
level=info msg="Allocated new global key" key=key0030 subsys=allocator
level=info msg="Allocated new global key" key=key0031 subsys=allocator
level=info msg="Allocated new global key" key=key0032 subsys=allocator
level=info msg="Allocated new global key" key=key0033 subsys=allocator
level=info msg="Allocated new global key" key=key0034 subsys=allocator
level=info msg="Allocated new global key" key=key0035 subsys=allocator
level=info msg="Allocated new global key" key=key0036 subsys=allocator
level=info msg="Allocated new global key" key=key0037 subsys=allocator
level=info msg="Allocated new global key" key=key0038 subsys=allocator
level=info msg="Allocated new global key" key=key0039 subsys=allocator
level=info msg="Allocated new global key" key=key0040 subsys=allocator
level=info msg="Allocated new global key" key=key0041 subsys=allocator
level=info msg="Allocated new global key" key=key0042 subsys=allocator
level=info msg="Allocated new global key" key=key0043 subsys=allocator
level=info msg="Allocated new global key" key=key0044 subsys=allocator
level=info msg="Allocated new global key" key=key0045 subsys=allocator
level=info msg="Allocated new global key" key=key0046 subsys=allocator
level=info msg="Allocated new global key" key=key0047 subsys=allocator
level=info msg="Allocated new global key" key=key0048 subsys=allocator
level=info msg="Allocated new global key" key=key0049 subsys=allocator
level=info msg="Allocated new global key" key=key0050 subsys=allocator
level=info msg="Allocated new global key" key=key0051 subsys=allocator
level=info msg="Allocated new global key" key=key0052 subsys=allocator
level=info msg="Allocated new global key" key=key0053 subsys=allocator
level=info msg="Allocated new global key" key=key0054 subsys=allocator
level=info msg="Allocated new global key" key=key0055 subsys=allocator
level=info msg="Allocated new global key" key=key0056 subsys=allocator
level=info msg="Allocated new global key" key=key0057 subsys=allocator
level=info msg="Allocated new global key" key=key0058 subsys=allocator
level=info msg="Allocated new global key" key=key0059 subsys=allocator
level=info msg="Allocated new global key" key=key0060 subsys=allocator
level=info msg="Allocated new global key" key=key0061 subsys=allocator
level=info msg="Allocated new global key" key=key0062 subsys=allocator
level=info msg="Allocated new global key" key=key0063 subsys=allocator
level=info msg="Allocated new global key" key=key0064 subsys=allocator
level=info msg="Allocated new global key" key=key0065 subsys=allocator
level=info msg="Allocated new global key" key=key0066 subsys=allocator
level=info msg="Allocated new global key" key=key0067 subsys=allocator
level=info msg="Allocated new global key" key=key0068 subsys=allocator
level=info msg="Allocated new global key" key=key0069 subsys=allocator
level=info msg="Allocated new global key" key=key0070 subsys=allocator
level=info msg="Allocated new global key" key=key0071 subsys=allocator
level=info msg="Allocated new global key" key=key0072 subsys=allocator
level=info msg="Allocated new global key" key=key0073 subsys=allocator
level=info msg="Allocated new global key" key=key0074 subsys=allocator
level=info msg="Allocated new global key" key=key0075 subsys=allocator
level=info msg="Allocated new global key" key=key0076 subsys=allocator
level=info msg="Allocated new global key" key=key0077 subsys=allocator
level=info msg="Allocated new global key" key=key0078 subsys=allocator
level=info msg="Allocated new global key" key=key0079 subsys=allocator
level=info msg="Allocated new global key" key=key0080 subsys=allocator
level=info msg="Allocated new global key" key=key0081 subsys=allocator
level=info msg="Allocated new global key" key=key0082 subsys=allocator
level=info msg="Allocated new global key" key=key0083 subsys=allocator
level=info msg="Allocated new global key" key=key0084 subsys=allocator
level=info msg="Allocated new global key" key=key0085 subsys=allocator
level=info msg="Allocated new global key" key=key0086 subsys=allocator
level=info msg="Allocated new global key" key=key0087 subsys=allocator
level=info msg="Allocated new global key" key=key0088 subsys=allocator
level=info msg="Allocated new global key" key=key0089 subsys=allocator
level=info msg="Allocated new global key" key=key0090 subsys=allocator
level=info msg="Allocated new global key" key=key0091 subsys=allocator
level=info msg="Allocated new global key" key=key0092 subsys=allocator
level=info msg="Allocated new global key" key=key0093 subsys=allocator
level=info msg="Allocated new global key" key=key0094 subsys=allocator
level=info msg="Allocated new global key" key=key0095 subsys=allocator
level=info msg="Allocated new global key" key=key0096 subsys=allocator
level=info msg="Allocated new global key" key=key0097 subsys=allocator
level=info msg="Allocated new global key" key=key0098 subsys=allocator
level=info msg="Allocated new global key" key=key0099 subsys=allocator
level=info msg="Allocated new global key" key=key0100 subsys=allocator
level=info msg="Allocated new global key" key=key0101 subsys=allocator
level=info msg="Allocated new global key" key=key0102 subsys=allocator
level=info msg="Allocated new global key" key=key0103 subsys=allocator
level=info msg="Allocated new global key" key=key0104 subsys=allocator
level=info msg="Allocated new global key" key=key0105 subsys=allocator
level=info msg="Allocated new global key" key=key0106 subsys=allocator
level=info msg="Allocated new global key" key=key0107 subsys=allocator
level=info msg="Allocated new global key" key=key0108 subsys=allocator
level=info msg="Allocated new global key" key=key0109 subsys=allocator
level=info msg="Allocated new global key" key=key0110 subsys=allocator
level=info msg="Allocated new global key" key=key0111 subsys=allocator
level=info msg="Allocated new global key" key=key0112 subsys=allocator
level=info msg="Allocated new global key" key=key0113 subsys=allocator
level=info msg="Allocated new global key" key=key0114 subsys=allocator
level=info msg="Allocated new global key" key=key0115 subsys=allocator
level=info msg="Allocated new global key" key=key0116 subsys=allocator
level=info msg="Allocated new global key" key=key0117 subsys=allocator
level=info msg="Allocated new global key" key=key0118 subsys=allocator
level=info msg="Allocated new global key" key=key0119 subsys=allocator
level=info msg="Allocated new global key" key=key0120 subsys=allocator
level=info msg="Allocated new global key" key=key0121 subsys=allocator
level=info msg="Allocated new global key" key=key0122 subsys=allocator
level=info msg="Allocated new global key" key=key0123 subsys=allocator
level=info msg="Allocated new global key" key=key0124 subsys=allocator
level=info msg="Allocated new global key" key=key0125 subsys=allocator
level=info msg="Allocated new global key" key=key0126 subsys=allocator
level=info msg="Allocated new global key" key=key0127 subsys=allocator
level=info msg="Allocated new global key" key=key0128 subsys=allocator
level=info msg="Allocated new global key" key=key0129 subsys=allocator
level=info msg="Allocated new global key" key=key0130 subsys=allocator
level=info msg="Allocated new global key" key=key0131 subsys=allocator
level=info msg="Allocated new global key" key=key0132 subsys=allocator
level=info msg="Allocated new global key" key=key0133 subsys=allocator
level=info msg="Allocated new global key" key=key0134 subsys=allocator
level=info msg="Allocated new global key" key=key0135 subsys=allocator
level=info msg="Allocated new global key" key=key0136 subsys=allocator
level=info msg="Allocated new global key" key=key0137 subsys=allocator
level=info msg="Allocated new global key" key=key0138 subsys=allocator
level=info msg="Allocated new global key" key=key0139 subsys=allocator
level=info msg="Allocated new global key" key=key0140 subsys=allocator
level=info msg="Allocated new global key" key=key0141 subsys=allocator
level=info msg="Allocated new global key" key=key0142 subsys=allocator
level=info msg="Allocated new global key" key=key0143 subsys=allocator
level=info msg="Allocated new global key" key=key0144 subsys=allocator
level=info msg="Allocated new global key" key=key0145 subsys=allocator
level=info msg="Allocated new global key" key=key0146 subsys=allocator
level=info msg="Allocated new global key" key=key0147 subsys=allocator
level=info msg="Allocated new global key" key=key0148 subsys=allocator
level=info msg="Allocated new global key" key=key0149 subsys=allocator
level=info msg="Allocated new global key" key=key0150 subsys=allocator
level=info msg="Allocated new global key" key=key0151 subsys=allocator
level=info msg="Allocated new global key" key=key0152 subsys=allocator
level=info msg="Allocated new global key" key=key0153 subsys=allocator
level=info msg="Allocated new global key" key=key0154 subsys=allocator
level=info msg="Allocated new global key" key=key0155 subsys=allocator
level=info msg="Allocated new global key" key=key0156 subsys=allocator
level=info msg="Allocated new global key" key=key0157 subsys=allocator
level=info msg="Allocated new global key" key=key0158 subsys=allocator
level=info msg="Allocated new global key" key=key0159 subsys=allocator
level=info msg="Allocated new global key" key=key0160 subsys=allocator
level=info msg="Allocated new global key" key=key0161 subsys=allocator
level=info msg="Allocated new global key" key=key0162 subsys=allocator
level=info msg="Allocated new global key" key=key0163 subsys=allocator
level=info msg="Allocated new global key" key=key0164 subsys=allocator
level=info msg="Allocated new global key" key=key0165 subsys=allocator
level=info msg="Allocated new global key" key=key0166 subsys=allocator
level=info msg="Allocated new global key" key=key0167 subsys=allocator
level=info msg="Allocated new global key" key=key0168 subsys=allocator
level=info msg="Allocated new global key" key=key0169 subsys=allocator
level=info msg="Allocated new global key" key=key0170 subsys=allocator
level=info msg="Allocated new global key" key=key0171 subsys=allocator
level=info msg="Allocated new global key" key=key0172 subsys=allocator
level=info msg="Allocated new global key" key=key0173 subsys=allocator
level=info msg="Allocated new global key" key=key0174 subsys=allocator
level=info msg="Allocated new global key" key=key0175 subsys=allocator
level=info msg="Allocated new global key" key=key0176 subsys=allocator
level=info msg="Allocated new global key" key=key0177 subsys=allocator
level=info msg="Allocated new global key" key=key0178 subsys=allocator
level=info msg="Allocated new global key" key=key0179 subsys=allocator
level=info msg="Allocated new global key" key=key0180 subsys=allocator
level=info msg="Allocated new global key" key=key0181 subsys=allocator
level=info msg="Allocated new global key" key=key0182 subsys=allocator
level=info msg="Allocated new global key" key=key0183 subsys=allocator
level=info msg="Allocated new global key" key=key0184 subsys=allocator
level=info msg="Allocated new global key" key=key0185 subsys=allocator
level=info msg="Allocated new global key" key=key0186 subsys=allocator
level=info msg="Allocated new global key" key=key0187 subsys=allocator
level=info msg="Allocated new global key" key=key0188 subsys=allocator
level=info msg="Allocated new global key" key=key0189 subsys=allocator
level=info msg="Allocated new global key" key=key0190 subsys=allocator
level=info msg="Allocated new global key" key=key0191 subsys=allocator
level=info msg="Allocated new global key" key=key0192 subsys=allocator
level=info msg="Allocated new global key" key=key0193 subsys=allocator
level=info msg="Allocated new global key" key=key0194 subsys=allocator
level=info msg="Allocated new global key" key=key0195 subsys=allocator
level=info msg="Allocated new global key" key=key0196 subsys=allocator
level=info msg="Allocated new global key" key=key0197 subsys=allocator
level=info msg="Allocated new global key" key=key0198 subsys=allocator
level=info msg="Allocated new global key" key=key0199 subsys=allocator
level=info msg="Allocated new global key" key=key0200 subsys=allocator
level=info msg="Allocated new global key" key=key0201 subsys=allocator
level=info msg="Allocated new global key" key=key0202 subsys=allocator
level=info msg="Allocated new global key" key=key0203 subsys=allocator
level=info msg="Allocated new global key" key=key0204 subsys=allocator
level=info msg="Allocated new global key" key=key0205 subsys=allocator
level=info msg="Allocated new global key" key=key0206 subsys=allocator
level=info msg="Allocated new global key" key=key0207 subsys=allocator
level=info msg="Allocated new global key" key=key0208 subsys=allocator
level=info msg="Allocated new global key" key=key0209 subsys=allocator
level=info msg="Allocated new global key" key=key0210 subsys=allocator
level=info msg="Allocated new global key" key=key0211 subsys=allocator
level=info msg="Allocated new global key" key=key0212 subsys=allocator
level=info msg="Allocated new global key" key=key0213 subsys=allocator
level=info msg="Allocated new global key" key=key0214 subsys=allocator
level=info msg="Allocated new global key" key=key0215 subsys=allocator
level=info msg="Allocated new global key" key=key0216 subsys=allocator
level=info msg="Allocated new global key" key=key0217 subsys=allocator
level=info msg="Allocated new global key" key=key0218 subsys=allocator
level=info msg="Allocated new global key" key=key0219 subsys=allocator
level=info msg="Allocated new global key" key=key0220 subsys=allocator
level=info msg="Allocated new global key" key=key0221 subsys=allocator
level=info msg="Allocated new global key" key=key0222 subsys=allocator
level=info msg="Allocated new global key" key=key0223 subsys=allocator
level=info msg="Allocated new global key" key=key0224 subsys=allocator
level=info msg="Allocated new global key" key=key0225 subsys=allocator
level=info msg="Allocated new global key" key=key0226 subsys=allocator
level=info msg="Allocated new global key" key=key0227 subsys=allocator
level=info msg="Allocated new global key" key=key0228 subsys=allocator
level=info msg="Allocated new global key" key=key0229 subsys=allocator
level=info msg="Allocated new global key" key=key0230 subsys=allocator
level=info msg="Allocated new global key" key=key0231 subsys=allocator
level=info msg="Allocated new global key" key=key0232 subsys=allocator
level=info msg="Allocated new global key" key=key0233 subsys=allocator
level=info msg="Allocated new global key" key=key0234 subsys=allocator
level=info msg="Allocated new global key" key=key0235 subsys=allocator
level=info msg="Allocated new global key" key=key0236 subsys=allocator
level=info msg="Allocated new global key" key=key0237 subsys=allocator
level=info msg="Allocated new global key" key=key0238 subsys=allocator
level=info msg="Allocated new global key" key=key0239 subsys=allocator
level=info msg="Allocated new global key" key=key0240 subsys=allocator
level=info msg="Allocated new global key" key=key0241 subsys=allocator
level=info msg="Allocated new global key" key=key0242 subsys=allocator
level=info msg="Allocated new global key" key=key0243 subsys=allocator
level=info msg="Allocated new global key" key=key0244 subsys=allocator
level=info msg="Allocated new global key" key=key0245 subsys=allocator
level=info msg="Allocated new global key" key=key0246 subsys=allocator
level=info msg="Allocated new global key" key=key0247 subsys=allocator
level=info msg="Allocated new global key" key=key0248 subsys=allocator
level=info msg="Allocated new global key" key=key0249 subsys=allocator
level=info msg="Allocated new global key" key=key0250 subsys=allocator
level=info msg="Allocated new global key" key=key0251 subsys=allocator
level=info msg="Allocated new global key" key=key0252 subsys=allocator
level=info msg="Allocated new global key" key=key0253 subsys=allocator
level=info msg="Allocated new global key" key=key0254 subsys=allocator
level=info msg="Allocated new global key" key=key0255 subsys=allocator
level=info msg="Allocated new global key" key=key0256 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=0 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=1 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=2 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=3 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=4 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=5 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=6 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=7 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=8 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=9 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=10 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=11 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=12 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=13 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=14 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=warning msg="Key allocation attempt failed" attempt=15 error="no more available IDs in configured space" key=key0257 subsys=allocator
level=info msg="Reusing existing global key" key=key0001 subsys=allocator
level=info msg="Releasing key" key=key0001 subsys=allocator
level=info msg="Reusing existing global key" key=key0002 subsys=allocator
level=info msg="Releasing key" key=key0002 subsys=allocator
level=info msg="Reusing existing global key" key=key0003 subsys=allocator
level=info msg="Releasing key" key=key0003 subsys=allocator
level=info msg="Reusing existing global key" key=key0004 subsys=allocator
level=info msg="Releasing key" key=key0004 subsys=allocator
level=info msg="Reusing existing global key" key=key0005 subsys=allocator
level=info msg="Releasing key" key=key0005 subsys=allocator
level=info msg="Reusing existing global key" key=key0006 subsys=allocator
level=info msg="Releasing key" key=key0006 subsys=allocator
level=info msg="Reusing existing global key" key=key0007 subsys=allocator
level=info msg="Releasing key" key=key0007 subsys=allocator
level=info msg="Reusing existing global key" key=key0008 subsys=allocator
level=info msg="Releasing key" key=key0008 subsys=allocator
level=info msg="Reusing existing global key" key=key0009 subsys=allocator
level=info msg="Releasing key" key=key0009 subsys=allocator
level=info msg="Reusing existing global key" key=key0010 subsys=allocator
level=info msg="Releasing key" key=key0010 subsys=allocator
level=info msg="Reusing existing global key" key=key0011 subsys=allocator
level=info msg="Releasing key" key=key0011 subsys=allocator
level=info msg="Reusing existing global key" key=key0012 subsys=allocator
level=info msg="Releasing key" key=key0012 subsys=allocator
level=info msg="Reusing existing global key" key=key0013 subsys=allocator
level=info msg="Releasing key" key=key0013 subsys=allocator
level=info msg="Reusing existing global key" key=key0014 subsys=allocator
level=info msg="Releasing key" key=key0014 subsys=allocator
level=info msg="Reusing existing global key" key=key0015 subsys=allocator
level=info msg="Releasing key" key=key0015 subsys=allocator
level=info msg="Reusing existing global key" key=key0016 subsys=allocator
level=info msg="Releasing key" key=key0016 subsys=allocator
level=info msg="Reusing existing global key" key=key0017 subsys=allocator
level=info msg="Releasing key" key=key0017 subsys=allocator
level=info msg="Reusing existing global key" key=key0018 subsys=allocator
level=info msg="Releasing key" key=key0018 subsys=allocator
level=info msg="Reusing existing global key" key=key0019 subsys=allocator
level=info msg="Releasing key" key=key0019 subsys=allocator
level=info msg="Reusing existing global key" key=key0020 subsys=allocator
level=info msg="Releasing key" key=key0020 subsys=allocator
level=info msg="Reusing existing global key" key=key0021 subsys=allocator
level=info msg="Releasing key" key=key0021 subsys=allocator
level=info msg="Reusing existing global key" key=key0022 subsys=allocator
level=info msg="Releasing key" key=key0022 subsys=allocator
level=info msg="Reusing existing global key" key=key0023 subsys=allocator
level=info msg="Releasing key" key=key0023 subsys=allocator
level=info msg="Reusing existing global key" key=key0024 subsys=allocator
level=info msg="Releasing key" key=key0024 subsys=allocator
level=info msg="Reusing existing global key" key=key0025 subsys=allocator
level=info msg="Releasing key" key=key0025 subsys=allocator
level=info msg="Reusing existing global key" key=key0026 subsys=allocator
level=info msg="Releasing key" key=key0026 subsys=allocator
level=info msg="Reusing existing global key" key=key0027 subsys=allocator
level=info msg="Releasing key" key=key0027 subsys=allocator
level=info msg="Reusing existing global key" key=key0028 subsys=allocator
level=info msg="Releasing key" key=key0028 subsys=allocator
level=info msg="Reusing existing global key" key=key0029 subsys=allocator
level=info msg="Releasing key" key=key0029 subsys=allocator
level=info msg="Reusing existing global key" key=key0030 subsys=allocator
level=info msg="Releasing key" key=key0030 subsys=allocator
level=info msg="Reusing existing global key" key=key0031 subsys=allocator
level=info msg="Releasing key" key=key0031 subsys=allocator
level=info msg="Reusing existing global key" key=key0032 subsys=allocator
level=info msg="Releasing key" key=key0032 subsys=allocator
level=info msg="Reusing existing global key" key=key0033 subsys=allocator
level=info msg="Releasing key" key=key0033 subsys=allocator
level=info msg="Reusing existing global key" key=key0034 subsys=allocator
level=info msg="Releasing key" key=key0034 subsys=allocator
level=info msg="Reusing existing global key" key=key0035 subsys=allocator
level=info msg="Releasing key" key=key0035 subsys=allocator
level=info msg="Reusing existing global key" key=key0036 subsys=allocator
level=info msg="Releasing key" key=key0036 subsys=allocator
level=info msg="Reusing existing global key" key=key0037 subsys=allocator
level=info msg="Releasing key" key=key0037 subsys=allocator
level=info msg="Reusing existing global key" key=key0038 subsys=allocator
level=info msg="Releasing key" key=key0038 subsys=allocator
level=info msg="Reusing existing global key" key=key0039 subsys=allocator
level=info msg="Releasing key" key=key0039 subsys=allocator
level=info msg="Reusing existing global key" key=key0040 subsys=allocator
level=info msg="Releasing key" key=key0040 subsys=allocator
level=info msg="Reusing existing global key" key=key0041 subsys=allocator
level=info msg="Releasing key" key=key0041 subsys=allocator
level=info msg="Reusing existing global key" key=key0042 subsys=allocator
level=info msg="Releasing key" key=key0042 subsys=allocator
level=info msg="Reusing existing global key" key=key0043 subsys=allocator
level=info msg="Releasing key" key=key0043 subsys=allocator
level=info msg="Reusing existing global key" key=key0044 subsys=allocator
level=info msg="Releasing key" key=key0044 subsys=allocator
level=info msg="Reusing existing global key" key=key0045 subsys=allocator
level=info msg="Releasing key" key=key0045 subsys=allocator
level=info msg="Reusing existing global key" key=key0046 subsys=allocator
level=info msg="Releasing key" key=key0046 subsys=allocator
level=info msg="Reusing existing global key" key=key0047 subsys=allocator
level=info msg="Releasing key" key=key0047 subsys=allocator
level=info msg="Reusing existing global key" key=key0048 subsys=allocator
level=info msg="Releasing key" key=key0048 subsys=allocator
level=info msg="Reusing existing global key" key=key0049 subsys=allocator
level=info msg="Releasing key" key=key0049 subsys=allocator
level=info msg="Reusing existing global key" key=key0050 subsys=allocator
level=info msg="Releasing key" key=key0050 subsys=allocator
level=info msg="Reusing existing global key" key=key0051 subsys=allocator
level=info msg="Releasing key" key=key0051 subsys=allocator
level=info msg="Reusing existing global key" key=key0052 subsys=allocator
level=info msg="Releasing key" key=key0052 subsys=allocator
level=info msg="Reusing existing global key" key=key0053 subsys=allocator
level=info msg="Releasing key" key=key0053 subsys=allocator
level=info msg="Reusing existing global key" key=key0054 subsys=allocator
level=info msg="Releasing key" key=key0054 subsys=allocator
level=info msg="Reusing existing global key" key=key0055 subsys=allocator
level=info msg="Releasing key" key=key0055 subsys=allocator
level=info msg="Reusing existing global key" key=key0056 subsys=allocator
level=info msg="Releasing key" key=key0056 subsys=allocator
level=info msg="Reusing existing global key" key=key0057 subsys=allocator
level=info msg="Releasing key" key=key0057 subsys=allocator
level=info msg="Reusing existing global key" key=key0058 subsys=allocator
level=info msg="Releasing key" key=key0058 subsys=allocator
level=info msg="Reusing existing global key" key=key0059 subsys=allocator
level=info msg="Releasing key" key=key0059 subsys=allocator
level=info msg="Reusing existing global key" key=key0060 subsys=allocator
level=info msg="Releasing key" key=key0060 subsys=allocator
level=info msg="Reusing existing global key" key=key0061 subsys=allocator
level=info msg="Releasing key" key=key0061 subsys=allocator
level=info msg="Reusing existing global key" key=key0062 subsys=allocator
level=info msg="Releasing key" key=key0062 subsys=allocator
level=info msg="Reusing existing global key" key=key0063 subsys=allocator
level=info msg="Releasing key" key=key0063 subsys=allocator
level=info msg="Reusing existing global key" key=key0064 subsys=allocator
level=info msg="Releasing key" key=key0064 subsys=allocator
level=info msg="Reusing existing global key" key=key0065 subsys=allocator
level=info msg="Releasing key" key=key0065 subsys=allocator
level=info msg="Reusing existing global key" key=key0066 subsys=allocator
level=info msg="Releasing key" key=key0066 subsys=allocator
level=info msg="Reusing existing global key" key=key0067 subsys=allocator
level=info msg="Releasing key" key=key0067 subsys=allocator
level=info msg="Reusing existing global key" key=key0068 subsys=allocator
level=info msg="Releasing key" key=key0068 subsys=allocator
level=info msg="Reusing existing global key" key=key0069 subsys=allocator
level=info msg="Releasing key" key=key0069 subsys=allocator
level=info msg="Reusing existing global key" key=key0070 subsys=allocator
level=info msg="Releasing key" key=key0070 subsys=allocator
level=info msg="Reusing existing global key" key=key0071 subsys=allocator
level=info msg="Releasing key" key=key0071 subsys=allocator
level=info msg="Reusing existing global key" key=key0072 subsys=allocator
level=info msg="Releasing key" key=key0072 subsys=allocator
level=info msg="Reusing existing global key" key=key0073 subsys=allocator
level=info msg="Releasing key" key=key0073 subsys=allocator
level=info msg="Reusing existing global key" key=key0074 subsys=allocator
level=info msg="Releasing key" key=key0074 subsys=allocator
level=info msg="Reusing existing global key" key=key0075 subsys=allocator
level=info msg="Releasing key" key=key0075 subsys=allocator
level=info msg="Reusing existing global key" key=key0076 subsys=allocator
level=info msg="Releasing key" key=key0076 subsys=allocator
level=info msg="Reusing existing global key" key=key0077 subsys=allocator
level=info msg="Releasing key" key=key0077 subsys=allocator
level=info msg="Reusing existing global key" key=key0078 subsys=allocator
level=info msg="Releasing key" key=key0078 subsys=allocator
level=info msg="Reusing existing global key" key=key0079 subsys=allocator
level=info msg="Releasing key" key=key0079 subsys=allocator
level=info msg="Reusing existing global key" key=key0080 subsys=allocator
level=info msg="Releasing key" key=key0080 subsys=allocator
level=info msg="Reusing existing global key" key=key0081 subsys=allocator
level=info msg="Releasing key" key=key0081 subsys=allocator
level=info msg="Reusing existing global key" key=key0082 subsys=allocator
level=info msg="Releasing key" key=key0082 subsys=allocator
level=info msg="Reusing existing global key" key=key0083 subsys=allocator
level=info msg="Releasing key" key=key0083 subsys=allocator
level=info msg="Reusing existing global key" key=key0084 subsys=allocator
level=info msg="Releasing key" key=key0084 subsys=allocator
level=info msg="Reusing existing global key" key=key0085 subsys=allocator
level=info msg="Releasing key" key=key0085 subsys=allocator
level=info msg="Reusing existing global key" key=key0086 subsys=allocator
level=info msg="Releasing key" key=key0086 subsys=allocator
level=info msg="Reusing existing global key" key=key0087 subsys=allocator
level=info msg="Releasing key" key=key0087 subsys=allocator
level=info msg="Reusing existing global key" key=key0088 subsys=allocator
level=info msg="Releasing key" key=key0088 subsys=allocator
level=info msg="Reusing existing global key" key=key0089 subsys=allocator
level=info msg="Releasing key" key=key0089 subsys=allocator
level=info msg="Reusing existing global key" key=key0090 subsys=allocator
level=info msg="Releasing key" key=key0090 subsys=allocator
level=info msg="Reusing existing global key" key=key0091 subsys=allocator
level=info msg="Releasing key" key=key0091 subsys=allocator
level=info msg="Reusing existing global key" key=key0092 subsys=allocator
level=info msg="Releasing key" key=key0092 subsys=allocator
level=info msg="Reusing existing global key" key=key0093 subsys=allocator
level=info msg="Releasing key" key=key0093 subsys=allocator
level=info msg="Reusing existing global key" key=key0094 subsys=allocator
level=info msg="Releasing key" key=key0094 subsys=allocator
level=info msg="Reusing existing global key" key=key0095 subsys=allocator
level=info msg="Releasing key" key=key0095 subsys=allocator
level=info msg="Reusing existing global key" key=key0096 subsys=allocator
level=info msg="Releasing key" key=key0096 subsys=allocator
level=info msg="Reusing existing global key" key=key0097 subsys=allocator
level=info msg="Releasing key" key=key0097 subsys=allocator
level=info msg="Reusing existing global key" key=key0098 subsys=allocator
level=info msg="Releasing key" key=key0098 subsys=allocator
level=info msg="Reusing existing global key" key=key0099 subsys=allocator
level=info msg="Releasing key" key=key0099 subsys=allocator
level=info msg="Reusing existing global key" key=key0100 subsys=allocator
level=info msg="Releasing key" key=key0100 subsys=allocator
level=info msg="Reusing existing global key" key=key0101 subsys=allocator
level=info msg="Releasing key" key=key0101 subsys=allocator
level=info msg="Reusing existing global key" key=key0102 subsys=allocator
level=info msg="Releasing key" key=key0102 subsys=allocator
level=info msg="Reusing existing global key" key=key0103 subsys=allocator
level=info msg="Releasing key" key=key0103 subsys=allocator
level=info msg="Reusing existing global key" key=key0104 subsys=allocator
level=info msg="Releasing key" key=key0104 subsys=allocator
level=info msg="Reusing existing global key" key=key0105 subsys=allocator
level=info msg="Releasing key" key=key0105 subsys=allocator
level=info msg="Reusing existing global key" key=key0106 subsys=allocator
level=info msg="Releasing key" key=key0106 subsys=allocator
level=info msg="Reusing existing global key" key=key0107 subsys=allocator
level=info msg="Releasing key" key=key0107 subsys=allocator
level=info msg="Reusing existing global key" key=key0108 subsys=allocator
level=info msg="Releasing key" key=key0108 subsys=allocator
level=info msg="Reusing existing global key" key=key0109 subsys=allocator
level=info msg="Releasing key" key=key0109 subsys=allocator
level=info msg="Reusing existing global key" key=key0110 subsys=allocator
level=info msg="Releasing key" key=key0110 subsys=allocator
level=info msg="Reusing existing global key" key=key0111 subsys=allocator
level=info msg="Releasing key" key=key0111 subsys=allocator
level=info msg="Reusing existing global key" key=key0112 subsys=allocator
level=info msg="Releasing key" key=key0112 subsys=allocator
level=info msg="Reusing existing global key" key=key0113 subsys=allocator
level=info msg="Releasing key" key=key0113 subsys=allocator
level=info msg="Reusing existing global key" key=key0114 subsys=allocator
level=info msg="Releasing key" key=key0114 subsys=allocator
level=info msg="Reusing existing global key" key=key0115 subsys=allocator
level=info msg="Releasing key" key=key0115 subsys=allocator
level=info msg="Reusing existing global key" key=key0116 subsys=allocator
level=info msg="Releasing key" key=key0116 subsys=allocator
level=info msg="Reusing existing global key" key=key0117 subsys=allocator
level=info msg="Releasing key" key=key0117 subsys=allocator
level=info msg="Reusing existing global key" key=key0118 subsys=allocator
level=info msg="Releasing key" key=key0118 subsys=allocator
level=info msg="Reusing existing global key" key=key0119 subsys=allocator
level=info msg="Releasing key" key=key0119 subsys=allocator
level=info msg="Reusing existing global key" key=key0120 subsys=allocator
level=info msg="Releasing key" key=key0120 subsys=allocator
level=info msg="Reusing existing global key" key=key0121 subsys=allocator
level=info msg="Releasing key" key=key0121 subsys=allocator
level=info msg="Reusing existing global key" key=key0122 subsys=allocator
level=info msg="Releasing key" key=key0122 subsys=allocator
level=info msg="Reusing existing global key" key=key0123 subsys=allocator
level=info msg="Releasing key" key=key0123 subsys=allocator
level=info msg="Reusing existing global key" key=key0124 subsys=allocator
level=info msg="Releasing key" key=key0124 subsys=allocator
level=info msg="Reusing existing global key" key=key0125 subsys=allocator
level=info msg="Releasing key" key=key0125 subsys=allocator
level=info msg="Reusing existing global key" key=key0126 subsys=allocator
level=info msg="Releasing key" key=key0126 subsys=allocator
level=info msg="Reusing existing global key" key=key0127 subsys=allocator
level=info msg="Releasing key" key=key0127 subsys=allocator
level=info msg="Reusing existing global key" key=key0128 subsys=allocator
level=info msg="Releasing key" key=key0128 subsys=allocator
level=info msg="Reusing existing global key" key=key0129 subsys=allocator
level=info msg="Releasing key" key=key0129 subsys=allocator
level=info msg="Reusing existing global key" key=key0130 subsys=allocator
level=info msg="Releasing key" key=key0130 subsys=allocator
level=info msg="Reusing existing global key" key=key0131 subsys=allocator
level=info msg="Releasing key" key=key0131 subsys=allocator
level=info msg="Reusing existing global key" key=key0132 subsys=allocator
level=info msg="Releasing key" key=key0132 subsys=allocator
level=info msg="Reusing existing global key" key=key0133 subsys=allocator
level=info msg="Releasing key" key=key0133 subsys=allocator
level=info msg="Reusing existing global key" key=key0134 subsys=allocator
level=info msg="Releasing key" key=key0134 subsys=allocator
level=info msg="Reusing existing global key" key=key0135 subsys=allocator
level=info msg="Releasing key" key=key0135 subsys=allocator
level=info msg="Reusing existing global key" key=key0136 subsys=allocator
level=info msg="Releasing key" key=key0136 subsys=allocator
level=info msg="Reusing existing global key" key=key0137 subsys=allocator
level=info msg="Releasing key" key=key0137 subsys=allocator
level=info msg="Reusing existing global key" key=key0138 subsys=allocator
level=info msg="Releasing key" key=key0138 subsys=allocator
level=info msg="Reusing existing global key" key=key0139 subsys=allocator
level=info msg="Releasing key" key=key0139 subsys=allocator
level=info msg="Reusing existing global key" key=key0140 subsys=allocator
level=info msg="Releasing key" key=key0140 subsys=allocator
level=info msg="Reusing existing global key" key=key0141 subsys=allocator
level=info msg="Releasing key" key=key0141 subsys=allocator
level=info msg="Reusing existing global key" key=key0142 subsys=allocator
level=info msg="Releasing key" key=key0142 subsys=allocator
level=info msg="Reusing existing global key" key=key0143 subsys=allocator
level=info msg="Releasing key" key=key0143 subsys=allocator
level=info msg="Reusing existing global key" key=key0144 subsys=allocator
level=info msg="Releasing key" key=key0144 subsys=allocator
level=info msg="Reusing existing global key" key=key0145 subsys=allocator
level=info msg="Releasing key" key=key0145 subsys=allocator
level=info msg="Reusing existing global key" key=key0146 subsys=allocator
level=info msg="Releasing key" key=key0146 subsys=allocator
level=info msg="Reusing existing global key" key=key0147 subsys=allocator
level=info msg="Releasing key" key=key0147 subsys=allocator
level=info msg="Reusing existing global key" key=key0148 subsys=allocator
level=info msg="Releasing key" key=key0148 subsys=allocator
level=info msg="Reusing existing global key" key=key0149 subsys=allocator
level=info msg="Releasing key" key=key0149 subsys=allocator
level=info msg="Reusing existing global key" key=key0150 subsys=allocator
level=info msg="Releasing key" key=key0150 subsys=allocator
level=info msg="Reusing existing global key" key=key0151 subsys=allocator
level=info msg="Releasing key" key=key0151 subsys=allocator
level=info msg="Reusing existing global key" key=key0152 subsys=allocator
level=info msg="Releasing key" key=key0152 subsys=allocator
level=info msg="Reusing existing global key" key=key0153 subsys=allocator
level=info msg="Releasing key" key=key0153 subsys=allocator
level=info msg="Reusing existing global key" key=key0154 subsys=allocator
level=info msg="Releasing key" key=key0154 subsys=allocator
level=info msg="Reusing existing global key" key=key0155 subsys=allocator
level=info msg="Releasing key" key=key0155 subsys=allocator
level=info msg="Reusing existing global key" key=key0156 subsys=allocator
level=info msg="Releasing key" key=key0156 subsys=allocator
level=info msg="Reusing existing global key" key=key0157 subsys=allocator
level=info msg="Releasing key" key=key0157 subsys=allocator
level=info msg="Reusing existing global key" key=key0158 subsys=allocator
level=info msg="Releasing key" key=key0158 subsys=allocator
level=info msg="Reusing existing global key" key=key0159 subsys=allocator
level=info msg="Releasing key" key=key0159 subsys=allocator
level=info msg="Reusing existing global key" key=key0160 subsys=allocator
level=info msg="Releasing key" key=key0160 subsys=allocator
level=info msg="Reusing existing global key" key=key0161 subsys=allocator
level=info msg="Releasing key" key=key0161 subsys=allocator
level=info msg="Reusing existing global key" key=key0162 subsys=allocator
level=info msg="Releasing key" key=key0162 subsys=allocator
level=info msg="Reusing existing global key" key=key0163 subsys=allocator
level=info msg="Releasing key" key=key0163 subsys=allocator
level=info msg="Reusing existing global key" key=key0164 subsys=allocator
level=info msg="Releasing key" key=key0164 subsys=allocator
level=info msg="Reusing existing global key" key=key0165 subsys=allocator
level=info msg="Releasing key" key=key0165 subsys=allocator
level=info msg="Reusing existing global key" key=key0166 subsys=allocator
level=info msg="Releasing key" key=key0166 subsys=allocator
level=info msg="Reusing existing global key" key=key0167 subsys=allocator
level=info msg="Releasing key" key=key0167 subsys=allocator
level=info msg="Reusing existing global key" key=key0168 subsys=allocator
level=info msg="Releasing key" key=key0168 subsys=allocator
level=info msg="Reusing existing global key" key=key0169 subsys=allocator
level=info msg="Releasing key" key=key0169 subsys=allocator
level=info msg="Reusing existing global key" key=key0170 subsys=allocator
level=info msg="Releasing key" key=key0170 subsys=allocator
level=info msg="Reusing existing global key" key=key0171 subsys=allocator
level=info msg="Releasing key" key=key0171 subsys=allocator
level=info msg="Reusing existing global key" key=key0172 subsys=allocator
level=info msg="Releasing key" key=key0172 subsys=allocator
level=info msg="Reusing existing global key" key=key0173 subsys=allocator
level=info msg="Releasing key" key=key0173 subsys=allocator
level=info msg="Reusing existing global key" key=key0174 subsys=allocator
level=info msg="Releasing key" key=key0174 subsys=allocator
level=info msg="Reusing existing global key" key=key0175 subsys=allocator
level=info msg="Releasing key" key=key0175 subsys=allocator
level=info msg="Reusing existing global key" key=key0176 subsys=allocator
level=info msg="Releasing key" key=key0176 subsys=allocator
level=info msg="Reusing existing global key" key=key0177 subsys=allocator
level=info msg="Releasing key" key=key0177 subsys=allocator
level=info msg="Reusing existing global key" key=key0178 subsys=allocator
level=info msg="Releasing key" key=key0178 subsys=allocator
level=info msg="Reusing existing global key" key=key0179 subsys=allocator
level=info msg="Releasing key" key=key0179 subsys=allocator
level=info msg="Reusing existing global key" key=key0180 subsys=allocator
level=info msg="Releasing key" key=key0180 subsys=allocator
level=info msg="Reusing existing global key" key=key0181 subsys=allocator
level=info msg="Releasing key" key=key0181 subsys=allocator
level=info msg="Reusing existing global key" key=key0182 subsys=allocator
level=info msg="Releasing key" key=key0182 subsys=allocator
level=info msg="Reusing existing global key" key=key0183 subsys=allocator
level=info msg="Releasing key" key=key0183 subsys=allocator
level=info msg="Reusing existing global key" key=key0184 subsys=allocator
level=info msg="Releasing key" key=key0184 subsys=allocator
level=info msg="Reusing existing global key" key=key0185 subsys=allocator
level=info msg="Releasing key" key=key0185 subsys=allocator
level=info msg="Reusing existing global key" key=key0186 subsys=allocator
level=info msg="Releasing key" key=key0186 subsys=allocator
level=info msg="Reusing existing global key" key=key0187 subsys=allocator
level=info msg="Releasing key" key=key0187 subsys=allocator
level=info msg="Reusing existing global key" key=key0188 subsys=allocator
level=info msg="Releasing key" key=key0188 subsys=allocator
level=info msg="Reusing existing global key" key=key0189 subsys=allocator
level=info msg="Releasing key" key=key0189 subsys=allocator
level=info msg="Reusing existing global key" key=key0190 subsys=allocator
level=info msg="Releasing key" key=key0190 subsys=allocator
level=info msg="Reusing existing global key" key=key0191 subsys=allocator
level=info msg="Releasing key" key=key0191 subsys=allocator
level=info msg="Reusing existing global key" key=key0192 subsys=allocator
level=info msg="Releasing key" key=key0192 subsys=allocator
level=info msg="Reusing existing global key" key=key0193 subsys=allocator
level=info msg="Releasing key" key=key0193 subsys=allocator
level=info msg="Reusing existing global key" key=key0194 subsys=allocator
level=info msg="Releasing key" key=key0194 subsys=allocator
level=info msg="Reusing existing global key" key=key0195 subsys=allocator
level=info msg="Releasing key" key=key0195 subsys=allocator
level=info msg="Reusing existing global key" key=key0196 subsys=allocator
level=info msg="Releasing key" key=key0196 subsys=allocator
level=info msg="Reusing existing global key" key=key0197 subsys=allocator
level=info msg="Releasing key" key=key0197 subsys=allocator
level=info msg="Reusing existing global key" key=key0198 subsys=allocator
level=info msg="Releasing key" key=key0198 subsys=allocator
level=info msg="Reusing existing global key" key=key0199 subsys=allocator
level=info msg="Releasing key" key=key0199 subsys=allocator
level=info msg="Reusing existing global key" key=key0200 subsys=allocator
level=info msg="Releasing key" key=key0200 subsys=allocator
level=info msg="Reusing existing global key" key=key0201 subsys=allocator
level=info msg="Releasing key" key=key0201 subsys=allocator
level=info msg="Reusing existing global key" key=key0202 subsys=allocator
level=info msg="Releasing key" key=key0202 subsys=allocator
level=info msg="Reusing existing global key" key=key0203 subsys=allocator
level=info msg="Releasing key" key=key0203 subsys=allocator
level=info msg="Reusing existing global key" key=key0204 subsys=allocator
level=info msg="Releasing key" key=key0204 subsys=allocator
level=info msg="Reusing existing global key" key=key0205 subsys=allocator
level=info msg="Releasing key" key=key0205 subsys=allocator
level=info msg="Reusing existing global key" key=key0206 subsys=allocator
level=info msg="Releasing key" key=key0206 subsys=allocator
level=info msg="Reusing existing global key" key=key0207 subsys=allocator
level=info msg="Releasing key" key=key0207 subsys=allocator
level=info msg="Reusing existing global key" key=key0208 subsys=allocator
level=info msg="Releasing key" key=key0208 subsys=allocator
level=info msg="Reusing existing global key" key=key0209 subsys=allocator
level=info msg="Releasing key" key=key0209 subsys=allocator
level=info msg="Reusing existing global key" key=key0210 subsys=allocator
level=info msg="Releasing key" key=key0210 subsys=allocator
level=info msg="Reusing existing global key" key=key0211 subsys=allocator
level=info msg="Releasing key" key=key0211 subsys=allocator
level=info msg="Reusing existing global key" key=key0212 subsys=allocator
level=info msg="Releasing key" key=key0212 subsys=allocator
level=info msg="Reusing existing global key" key=key0213 subsys=allocator
level=info msg="Releasing key" key=key0213 subsys=allocator
level=info msg="Reusing existing global key" key=key0214 subsys=allocator
level=info msg="Releasing key" key=key0214 subsys=allocator
level=info msg="Reusing existing global key" key=key0215 subsys=allocator
level=info msg="Releasing key" key=key0215 subsys=allocator
level=info msg="Reusing existing global key" key=key0216 subsys=allocator
level=info msg="Releasing key" key=key0216 subsys=allocator
level=info msg="Reusing existing global key" key=key0217 subsys=allocator
level=info msg="Releasing key" key=key0217 subsys=allocator
level=info msg="Reusing existing global key" key=key0218 subsys=allocator
level=info msg="Releasing key" key=key0218 subsys=allocator
level=info msg="Reusing existing global key" key=key0219 subsys=allocator
level=info msg="Releasing key" key=key0219 subsys=allocator
level=info msg="Reusing existing global key" key=key0220 subsys=allocator
level=info msg="Releasing key" key=key0220 subsys=allocator
level=info msg="Reusing existing global key" key=key0221 subsys=allocator
level=info msg="Releasing key" key=key0221 subsys=allocator
level=info msg="Reusing existing global key" key=key0222 subsys=allocator
level=info msg="Releasing key" key=key0222 subsys=allocator
level=info msg="Reusing existing global key" key=key0223 subsys=allocator
level=info msg="Releasing key" key=key0223 subsys=allocator
level=info msg="Reusing existing global key" key=key0224 subsys=allocator
level=info msg="Releasing key" key=key0224 subsys=allocator
level=info msg="Reusing existing global key" key=key0225 subsys=allocator
level=info msg="Releasing key" key=key0225 subsys=allocator
level=info msg="Reusing existing global key" key=key0226 subsys=allocator
level=info msg="Releasing key" key=key0226 subsys=allocator
level=info msg="Reusing existing global key" key=key0227 subsys=allocator
level=info msg="Releasing key" key=key0227 subsys=allocator
level=info msg="Reusing existing global key" key=key0228 subsys=allocator
level=info msg="Releasing key" key=key0228 subsys=allocator
level=info msg="Reusing existing global key" key=key0229 subsys=allocator
level=info msg="Releasing key" key=key0229 subsys=allocator
level=info msg="Reusing existing global key" key=key0230 subsys=allocator
level=info msg="Releasing key" key=key0230 subsys=allocator
level=info msg="Reusing existing global key" key=key0231 subsys=allocator
level=info msg="Releasing key" key=key0231 subsys=allocator
level=info msg="Reusing existing global key" key=key0232 subsys=allocator
level=info msg="Releasing key" key=key0232 subsys=allocator
level=info msg="Reusing existing global key" key=key0233 subsys=allocator
level=info msg="Releasing key" key=key0233 subsys=allocator
level=info msg="Reusing existing global key" key=key0234 subsys=allocator
level=info msg="Releasing key" key=key0234 subsys=allocator
level=info msg="Reusing existing global key" key=key0235 subsys=allocator
level=info msg="Releasing key" key=key0235 subsys=allocator
level=info msg="Reusing existing global key" key=key0236 subsys=allocator
level=info msg="Releasing key" key=key0236 subsys=allocator
level=info msg="Reusing existing global key" key=key0237 subsys=allocator
level=info msg="Releasing key" key=key0237 subsys=allocator
level=info msg="Reusing existing global key" key=key0238 subsys=allocator
level=info msg="Releasing key" key=key0238 subsys=allocator
level=info msg="Reusing existing global key" key=key0239 subsys=allocator
level=info msg="Releasing key" key=key0239 subsys=allocator
level=info msg="Reusing existing global key" key=key0240 subsys=allocator
level=info msg="Releasing key" key=key0240 subsys=allocator
level=info msg="Reusing existing global key" key=key0241 subsys=allocator
level=info msg="Releasing key" key=key0241 subsys=allocator
level=info msg="Reusing existing global key" key=key0242 subsys=allocator
level=info msg="Releasing key" key=key0242 subsys=allocator
level=info msg="Reusing existing global key" key=key0243 subsys=allocator
level=info msg="Releasing key" key=key0243 subsys=allocator
level=info msg="Reusing existing global key" key=key0244 subsys=allocator
level=info msg="Releasing key" key=key0244 subsys=allocator
level=info msg="Reusing existing global key" key=key0245 subsys=allocator
level=info msg="Releasing key" key=key0245 subsys=allocator
level=info msg="Reusing existing global key" key=key0246 subsys=allocator
level=info msg="Releasing key" key=key0246 subsys=allocator
level=info msg="Reusing existing global key" key=key0247 subsys=allocator
level=info msg="Releasing key" key=key0247 subsys=allocator
level=info msg="Reusing existing global key" key=key0248 subsys=allocator
level=info msg="Releasing key" key=key0248 subsys=allocator
level=info msg="Reusing existing global key" key=key0249 subsys=allocator
level=info msg="Releasing key" key=key0249 subsys=allocator
level=info msg="Reusing existing global key" key=key0250 subsys=allocator
level=info msg="Releasing key" key=key0250 subsys=allocator
level=info msg="Reusing existing global key" key=key0251 subsys=allocator
level=info msg="Releasing key" key=key0251 subsys=allocator
level=info msg="Reusing existing global key" key=key0252 subsys=allocator
level=info msg="Releasing key" key=key0252 subsys=allocator
level=info msg="Reusing existing global key" key=key0253 subsys=allocator
level=info msg="Releasing key" key=key0253 subsys=allocator
level=info msg="Reusing existing global key" key=key0254 subsys=allocator
level=info msg="Releasing key" key=key0254 subsys=allocator
level=info msg="Reusing existing global key" key=key0255 subsys=allocator
level=info msg="Releasing key" key=key0255 subsys=allocator
level=info msg="Reusing existing global key" key=key0256 subsys=allocator
level=info msg="Releasing key" key=key0256 subsys=allocator
level=info msg="Releasing key" key=key0001 subsys=allocator
level=info msg="Releasing key" key=key0002 subsys=allocator
level=info msg="Releasing key" key=key0003 subsys=allocator
level=info msg="Releasing key" key=key0004 subsys=allocator
level=info msg="Releasing key" key=key0005 subsys=allocator
level=info msg="Releasing key" key=key0006 subsys=allocator
level=info msg="Releasing key" key=key0007 subsys=allocator
level=info msg="Releasing key" key=key0008 subsys=allocator
level=info msg="Releasing key" key=key0009 subsys=allocator
level=info msg="Releasing key" key=key0010 subsys=allocator
level=info msg="Releasing key" key=key0011 subsys=allocator
level=info msg="Releasing key" key=key0012 subsys=allocator
level=info msg="Releasing key" key=key0013 subsys=allocator
level=info msg="Releasing key" key=key0014 subsys=allocator
level=info msg="Releasing key" key=key0015 subsys=allocator
level=info msg="Releasing key" key=key0016 subsys=allocator
level=info msg="Releasing key" key=key0017 subsys=allocator
level=info msg="Releasing key" key=key0018 subsys=allocator
level=info msg="Releasing key" key=key0019 subsys=allocator
level=info msg="Releasing key" key=key0020 subsys=allocator
level=info msg="Releasing key" key=key0021 subsys=allocator
level=info msg="Releasing key" key=key0022 subsys=allocator
level=info msg="Releasing key" key=key0023 subsys=allocator
level=info msg="Releasing key" key=key0024 subsys=allocator
level=info msg="Releasing key" key=key0025 subsys=allocator
level=info msg="Releasing key" key=key0026 subsys=allocator
level=info msg="Releasing key" key=key0027 subsys=allocator
level=info msg="Releasing key" key=key0028 subsys=allocator
level=info msg="Releasing key" key=key0029 subsys=allocator
level=info msg="Releasing key" key=key0030 subsys=allocator
level=info msg="Releasing key" key=key0031 subsys=allocator
level=info msg="Releasing key" key=key0032 subsys=allocator
level=info msg="Releasing key" key=key0033 subsys=allocator
level=info msg="Releasing key" key=key0034 subsys=allocator
level=info msg="Releasing key" key=key0035 subsys=allocator
level=info msg="Releasing key" key=key0036 subsys=allocator
level=info msg="Releasing key" key=key0037 subsys=allocator
level=info msg="Releasing key" key=key0038 subsys=allocator
level=info msg="Releasing key" key=key0039 subsys=allocator
level=info msg="Releasing key" key=key0040 subsys=allocator
level=info msg="Releasing key" key=key0041 subsys=allocator
level=info msg="Releasing key" key=key0042 subsys=allocator
level=info msg="Releasing key" key=key0043 subsys=allocator
level=info msg="Releasing key" key=key0044 subsys=allocator
level=info msg="Releasing key" key=key0045 subsys=allocator
level=info msg="Releasing key" key=key0046 subsys=allocator
level=info msg="Releasing key" key=key0047 subsys=allocator
level=info msg="Releasing key" key=key0048 subsys=allocator
level=info msg="Releasing key" key=key0049 subsys=allocator
level=info msg="Releasing key" key=key0050 subsys=allocator
level=info msg="Releasing key" key=key0051 subsys=allocator
level=info msg="Releasing key" key=key0052 subsys=allocator
level=info msg="Releasing key" key=key0053 subsys=allocator
level=info msg="Releasing key" key=key0054 subsys=allocator
level=info msg="Releasing key" key=key0055 subsys=allocator
level=info msg="Releasing key" key=key0056 subsys=allocator
level=info msg="Releasing key" key=key0057 subsys=allocator
level=info msg="Releasing key" key=key0058 subsys=allocator
level=info msg="Releasing key" key=key0059 subsys=allocator
level=info msg="Releasing key" key=key0060 subsys=allocator
level=info msg="Releasing key" key=key0061 subsys=allocator
level=info msg="Releasing key" key=key0062 subsys=allocator
level=info msg="Releasing key" key=key0063 subsys=allocator
level=info msg="Releasing key" key=key0064 subsys=allocator
level=info msg="Releasing key" key=key0065 subsys=allocator
level=info msg="Releasing key" key=key0066 subsys=allocator
level=info msg="Releasing key" key=key0067 subsys=allocator
level=info msg="Releasing key" key=key0068 subsys=allocator
level=info msg="Releasing key" key=key0069 subsys=allocator
level=info msg="Releasing key" key=key0070 subsys=allocator
level=info msg="Releasing key" key=key0071 subsys=allocator
level=info msg="Releasing key" key=key0072 subsys=allocator
level=info msg="Releasing key" key=key0073 subsys=allocator
level=info msg="Releasing key" key=key0074 subsys=allocator
level=info msg="Releasing key" key=key0075 subsys=allocator
level=info msg="Releasing key" key=key0076 subsys=allocator
level=info msg="Releasing key" key=key0077 subsys=allocator
level=info msg="Releasing key" key=key0078 subsys=allocator
level=info msg="Releasing key" key=key0079 subsys=allocator
level=info msg="Releasing key" key=key0080 subsys=allocator
level=info msg="Releasing key" key=key0081 subsys=allocator
level=info msg="Releasing key" key=key0082 subsys=allocator
level=info msg="Releasing key" key=key0083 subsys=allocator
level=info msg="Releasing key" key=key0084 subsys=allocator
level=info msg="Releasing key" key=key0085 subsys=allocator
level=info msg="Releasing key" key=key0086 subsys=allocator
level=info msg="Releasing key" key=key0087 subsys=allocator
level=info msg="Releasing key" key=key0088 subsys=allocator
level=info msg="Releasing key" key=key0089 subsys=allocator
level=info msg="Releasing key" key=key0090 subsys=allocator
level=info msg="Releasing key" key=key0091 subsys=allocator
level=info msg="Releasing key" key=key0092 subsys=allocator
level=info msg="Releasing key" key=key0093 subsys=allocator
level=info msg="Releasing key" key=key0094 subsys=allocator
level=info msg="Releasing key" key=key0095 subsys=allocator
level=info msg="Releasing key" key=key0096 subsys=allocator
level=info msg="Releasing key" key=key0097 subsys=allocator
level=info msg="Releasing key" key=key0098 subsys=allocator
level=info msg="Releasing key" key=key0099 subsys=allocator
level=info msg="Releasing key" key=key0100 subsys=allocator
level=info msg="Releasing key" key=key0101 subsys=allocator
level=info msg="Releasing key" key=key0102 subsys=allocator
level=info msg="Releasing key" key=key0103 subsys=allocator
level=info msg="Releasing key" key=key0104 subsys=allocator
level=info msg="Releasing key" key=key0105 subsys=allocator
level=info msg="Releasing key" key=key0106 subsys=allocator
level=info msg="Releasing key" key=key0107 subsys=allocator
level=info msg="Releasing key" key=key0108 subsys=allocator
level=info msg="Releasing key" key=key0109 subsys=allocator
level=info msg="Releasing key" key=key0110 subsys=allocator
level=info msg="Releasing key" key=key0111 subsys=allocator
level=info msg="Releasing key" key=key0112 subsys=allocator
level=info msg="Releasing key" key=key0113 subsys=allocator
level=info msg="Releasing key" key=key0114 subsys=allocator
level=info msg="Releasing key" key=key0115 subsys=allocator
level=info msg="Releasing key" key=key0116 subsys=allocator
level=info msg="Releasing key" key=key0117 subsys=allocator
level=info msg="Releasing key" key=key0118 subsys=allocator
level=info msg="Releasing key" key=key0119 subsys=allocator
level=info msg="Releasing key" key=key0120 subsys=allocator
level=info msg="Releasing key" key=key0121 subsys=allocator
level=info msg="Releasing key" key=key0122 subsys=allocator
level=info msg="Releasing key" key=key0123 subsys=allocator
level=info msg="Releasing key" key=key0124 subsys=allocator
level=info msg="Releasing key" key=key0125 subsys=allocator
level=info msg="Releasing key" key=key0126 subsys=allocator
level=info msg="Releasing key" key=key0127 subsys=allocator
level=info msg="Releasing key" key=key0128 subsys=allocator
level=info msg="Releasing key" key=key0129 subsys=allocator
level=info msg="Releasing key" key=key0130 subsys=allocator
level=info msg="Releasing key" key=key0131 subsys=allocator
level=info msg="Releasing key" key=key0132 subsys=allocator
level=info msg="Releasing key" key=key0133 subsys=allocator
level=info msg="Releasing key" key=key0134 subsys=allocator
level=info msg="Releasing key" key=key0135 subsys=allocator
level=info msg="Releasing key" key=key0136 subsys=allocator
level=info msg="Releasing key" key=key0137 subsys=allocator
level=info msg="Releasing key" key=key0138 subsys=allocator
level=info msg="Releasing key" key=key0139 subsys=allocator
level=info msg="Releasing key" key=key0140 subsys=allocator
level=info msg="Releasing key" key=key0141 subsys=allocator
level=info msg="Releasing key" key=key0142 subsys=allocator
level=info msg="Releasing key" key=key0143 subsys=allocator
level=info msg="Releasing key" key=key0144 subsys=allocator
level=info msg="Releasing key" key=key0145 subsys=allocator
level=info msg="Releasing key" key=key0146 subsys=allocator
level=info msg="Releasing key" key=key0147 subsys=allocator
level=info msg="Releasing key" key=key0148 subsys=allocator
level=info msg="Releasing key" key=key0149 subsys=allocator
level=info msg="Releasing key" key=key0150 subsys=allocator
level=info msg="Releasing key" key=key0151 subsys=allocator
level=info msg="Releasing key" key=key0152 subsys=allocator
level=info msg="Releasing key" key=key0153 subsys=allocator
level=info msg="Releasing key" key=key0154 subsys=allocator
level=info msg="Releasing key" key=key0155 subsys=allocator
level=info msg="Releasing key" key=key0156 subsys=allocator
level=info msg="Releasing key" key=key0157 subsys=allocator
level=info msg="Releasing key" key=key0158 subsys=allocator
level=info msg="Releasing key" key=key0159 subsys=allocator
level=info msg="Releasing key" key=key0160 subsys=allocator
level=info msg="Releasing key" key=key0161 subsys=allocator
level=info msg="Releasing key" key=key0162 subsys=allocator
level=info msg="Releasing key" key=key0163 subsys=allocator
level=info msg="Releasing key" key=key0164 subsys=allocator
level=info msg="Releasing key" key=key0165 subsys=allocator
level=info msg="Releasing key" key=key0166 subsys=allocator
level=info msg="Releasing key" key=key0167 subsys=allocator
level=info msg="Releasing key" key=key0168 subsys=allocator
level=info msg="Releasing key" key=key0169 subsys=allocator
level=info msg="Releasing key" key=key0170 subsys=allocator
level=info msg="Releasing key" key=key0171 subsys=allocator
level=info msg="Releasing key" key=key0172 subsys=allocator
level=info msg="Releasing key" key=key0173 subsys=allocator
level=info msg="Releasing key" key=key0174 subsys=allocator
level=info msg="Releasing key" key=key0175 subsys=allocator
level=info msg="Releasing key" key=key0176 subsys=allocator
level=info msg="Releasing key" key=key0177 subsys=allocator
level=info msg="Releasing key" key=key0178 subsys=allocator
level=info msg="Releasing key" key=key0179 subsys=allocator
level=info msg="Releasing key" key=key0180 subsys=allocator
level=info msg="Releasing key" key=key0181 subsys=allocator
level=info msg="Releasing key" key=key0182 subsys=allocator
level=info msg="Releasing key" key=key0183 subsys=allocator
level=info msg="Releasing key" key=key0184 subsys=allocator
level=info msg="Releasing key" key=key0185 subsys=allocator
level=info msg="Releasing key" key=key0186 subsys=allocator
level=info msg="Releasing key" key=key0187 subsys=allocator
level=info msg="Releasing key" key=key0188 subsys=allocator
level=info msg="Releasing key" key=key0189 subsys=allocator
level=info msg="Releasing key" key=key0190 subsys=allocator
level=info msg="Releasing key" key=key0191 subsys=allocator
level=info msg="Releasing key" key=key0192 subsys=allocator
level=info msg="Releasing key" key=key0193 subsys=allocator
level=info msg="Releasing key" key=key0194 subsys=allocator
level=info msg="Releasing key" key=key0195 subsys=allocator
level=info msg="Releasing key" key=key0196 subsys=allocator
level=info msg="Releasing key" key=key0197 subsys=allocator
level=info msg="Releasing key" key=key0198 subsys=allocator
level=info msg="Releasing key" key=key0199 subsys=allocator
level=info msg="Releasing key" key=key0200 subsys=allocator
level=info msg="Releasing key" key=key0201 subsys=allocator
level=info msg="Releasing key" key=key0202 subsys=allocator
level=info msg="Releasing key" key=key0203 subsys=allocator
level=info msg="Releasing key" key=key0204 subsys=allocator
level=info msg="Releasing key" key=key0205 subsys=allocator
level=info msg="Releasing key" key=key0206 subsys=allocator
level=info msg="Releasing key" key=key0207 subsys=allocator
level=info msg="Releasing key" key=key0208 subsys=allocator
level=info msg="Releasing key" key=key0209 subsys=allocator
level=info msg="Releasing key" key=key0210 subsys=allocator
level=info msg="Releasing key" key=key0211 subsys=allocator
level=info msg="Releasing key" key=key0212 subsys=allocator
level=info msg="Releasing key" key=key0213 subsys=allocator
level=info msg="Releasing key" key=key0214 subsys=allocator
level=info msg="Releasing key" key=key0215 subsys=allocator
level=info msg="Releasing key" key=key0216 subsys=allocator
level=info msg="Releasing key" key=key0217 subsys=allocator
level=info msg="Releasing key" key=key0218 subsys=allocator
level=info msg="Releasing key" key=key0219 subsys=allocator
level=info msg="Releasing key" key=key0220 subsys=allocator
level=info msg="Releasing key" key=key0221 subsys=allocator
level=info msg="Releasing key" key=key0222 subsys=allocator
level=info msg="Releasing key" key=key0223 subsys=allocator
level=info msg="Releasing key" key=key0224 subsys=allocator
level=info msg="Releasing key" key=key0225 subsys=allocator
level=info msg="Releasing key" key=key0226 subsys=allocator
level=info msg="Releasing key" key=key0227 subsys=allocator
level=info msg="Releasing key" key=key0228 subsys=allocator
level=info msg="Releasing key" key=key0229 subsys=allocator
level=info msg="Releasing key" key=key0230 subsys=allocator
level=info msg="Releasing key" key=key0231 subsys=allocator
level=info msg="Releasing key" key=key0232 subsys=allocator
level=info msg="Releasing key" key=key0233 subsys=allocator
level=info msg="Releasing key" key=key0234 subsys=allocator
level=info msg="Releasing key" key=key0235 subsys=allocator
level=info msg="Releasing key" key=key0236 subsys=allocator
level=info msg="Releasing key" key=key0237 subsys=allocator
level=info msg="Releasing key" key=key0238 subsys=allocator
level=info msg="Releasing key" key=key0239 subsys=allocator
level=info msg="Releasing key" key=key0240 subsys=allocator
level=info msg="Releasing key" key=key0241 subsys=allocator
level=info msg="Releasing key" key=key0242 subsys=allocator
level=info msg="Releasing key" key=key0243 subsys=allocator
level=info msg="Releasing key" key=key0244 subsys=allocator
level=info msg="Releasing key" key=key0245 subsys=allocator
level=info msg="Releasing key" key=key0246 subsys=allocator
level=info msg="Releasing key" key=key0247 subsys=allocator
level=info msg="Releasing key" key=key0248 subsys=allocator
level=info msg="Releasing key" key=key0249 subsys=allocator
level=info msg="Releasing key" key=key0250 subsys=allocator
level=info msg="Releasing key" key=key0251 subsys=allocator
level=info msg="Releasing key" key=key0252 subsys=allocator
level=info msg="Releasing key" key=key0253 subsys=allocator
level=info msg="Releasing key" key=key0254 subsys=allocator
level=info msg="Releasing key" key=key0255 subsys=allocator
level=info msg="Releasing key" key=key0256 subsys=allocator
level=info msg="Releasing key" key=key0001 subsys=allocator
level=info msg="Releasing key" key=key0002 subsys=allocator
level=info msg="Releasing key" key=key0003 subsys=allocator
level=info msg="Releasing key" key=key0004 subsys=allocator
level=info msg="Releasing key" key=key0005 subsys=allocator
level=info msg="Releasing key" key=key0006 subsys=allocator
level=info msg="Releasing key" key=key0007 subsys=allocator
level=info msg="Releasing key" key=key0008 subsys=allocator
level=info msg="Releasing key" key=key0009 subsys=allocator
level=info msg="Releasing key" key=key0010 subsys=allocator
level=info msg="Releasing key" key=key0011 subsys=allocator
level=info msg="Releasing key" key=key0012 subsys=allocator
level=info msg="Releasing key" key=key0013 subsys=allocator
level=info msg="Releasing key" key=key0014 subsys=allocator
level=info msg="Releasing key" key=key0015 subsys=allocator
level=info msg="Releasing key" key=key0016 subsys=allocator
level=info msg="Releasing key" key=key0017 subsys=allocator
level=info msg="Releasing key" key=key0018 subsys=allocator
level=info msg="Releasing key" key=key0019 subsys=allocator
level=info msg="Releasing key" key=key0020 subsys=allocator
level=info msg="Releasing key" key=key0021 subsys=allocator
level=info msg="Releasing key" key=key0022 subsys=allocator
level=info msg="Releasing key" key=key0023 subsys=allocator
level=info msg="Releasing key" key=key0024 subsys=allocator
level=info msg="Releasing key" key=key0025 subsys=allocator
level=info msg="Releasing key" key=key0026 subsys=allocator
level=info msg="Releasing key" key=key0027 subsys=allocator
level=info msg="Releasing key" key=key0028 subsys=allocator
level=info msg="Releasing key" key=key0029 subsys=allocator
level=info msg="Releasing key" key=key0030 subsys=allocator
level=info msg="Releasing key" key=key0031 subsys=allocator
level=info msg="Releasing key" key=key0032 subsys=allocator
level=info msg="Releasing key" key=key0033 subsys=allocator
level=info msg="Releasing key" key=key0034 subsys=allocator
level=info msg="Releasing key" key=key0035 subsys=allocator
level=info msg="Releasing key" key=key0036 subsys=allocator
level=info msg="Releasing key" key=key0037 subsys=allocator
level=info msg="Releasing key" key=key0038 subsys=allocator
level=info msg="Releasing key" key=key0039 subsys=allocator
level=info msg="Releasing key" key=key0040 subsys=allocator
level=info msg="Releasing key" key=key0041 subsys=allocator
level=info msg="Releasing key" key=key0042 subsys=allocator
level=info msg="Releasing key" key=key0043 subsys=allocator
level=info msg="Releasing key" key=key0044 subsys=allocator
level=info msg="Releasing key" key=key0045 subsys=allocator
level=info msg="Releasing key" key=key0046 subsys=allocator
level=info msg="Releasing key" key=key0047 subsys=allocator
level=info msg="Releasing key" key=key0048 subsys=allocator
level=info msg="Releasing key" key=key0049 subsys=allocator
level=info msg="Releasing key" key=key0050 subsys=allocator
level=info msg="Releasing key" key=key0051 subsys=allocator
level=info msg="Releasing key" key=key0052 subsys=allocator
level=info msg="Releasing key" key=key0053 subsys=allocator
level=info msg="Releasing key" key=key0054 subsys=allocator
level=info msg="Releasing key" key=key0055 subsys=allocator
level=info msg="Releasing key" key=key0056 subsys=allocator
level=info msg="Releasing key" key=key0057 subsys=allocator
level=info msg="Releasing key" key=key0058 subsys=allocator
level=info msg="Releasing key" key=key0059 subsys=allocator
level=info msg="Releasing key" key=key0060 subsys=allocator
level=info msg="Releasing key" key=key0061 subsys=allocator
level=info msg="Releasing key" key=key0062 subsys=allocator
level=info msg="Releasing key" key=key0063 subsys=allocator
level=info msg="Releasing key" key=key0064 subsys=allocator
level=info msg="Releasing key" key=key0065 subsys=allocator
level=info msg="Releasing key" key=key0066 subsys=allocator
level=info msg="Releasing key" key=key0067 subsys=allocator
level=info msg="Releasing key" key=key0068 subsys=allocator
level=info msg="Releasing key" key=key0069 subsys=allocator
level=info msg="Releasing key" key=key0070 subsys=allocator
level=info msg="Releasing key" key=key0071 subsys=allocator
level=info msg="Releasing key" key=key0072 subsys=allocator
level=info msg="Releasing key" key=key0073 subsys=allocator
level=info msg="Releasing key" key=key0074 subsys=allocator
level=info msg="Releasing key" key=key0075 subsys=allocator
level=info msg="Releasing key" key=key0076 subsys=allocator
level=info msg="Releasing key" key=key0077 subsys=allocator
level=info msg="Releasing key" key=key0078 subsys=allocator
level=info msg="Releasing key" key=key0079 subsys=allocator
level=info msg="Releasing key" key=key0080 subsys=allocator
level=info msg="Releasing key" key=key0081 subsys=allocator
level=info msg="Releasing key" key=key0082 subsys=allocator
level=info msg="Releasing key" key=key0083 subsys=allocator
level=info msg="Releasing key" key=key0084 subsys=allocator
level=info msg="Releasing key" key=key0085 subsys=allocator
level=info msg="Releasing key" key=key0086 subsys=allocator
level=info msg="Releasing key" key=key0087 subsys=allocator
level=info msg="Releasing key" key=key0088 subsys=allocator
level=info msg="Releasing key" key=key0089 subsys=allocator
level=info msg="Releasing key" key=key0090 subsys=allocator
level=info msg="Releasing key" key=key0091 subsys=allocator
level=info msg="Releasing key" key=key0092 subsys=allocator
level=info msg="Releasing key" key=key0093 subsys=allocator
level=info msg="Releasing key" key=key0094 subsys=allocator
level=info msg="Releasing key" key=key0095 subsys=allocator
level=info msg="Releasing key" key=key0096 subsys=allocator
level=info msg="Releasing key" key=key0097 subsys=allocator
level=info msg="Releasing key" key=key0098 subsys=allocator
level=info msg="Releasing key" key=key0099 subsys=allocator
level=info msg="Releasing key" key=key0100 subsys=allocator
level=info msg="Releasing key" key=key0101 subsys=allocator
level=info msg="Releasing key" key=key0102 subsys=allocator
level=info msg="Releasing key" key=key0103 subsys=allocator
level=info msg="Releasing key" key=key0104 subsys=allocator
level=info msg="Releasing key" key=key0105 subsys=allocator
level=info msg="Releasing key" key=key0106 subsys=allocator
level=info msg="Releasing key" key=key0107 subsys=allocator
level=info msg="Releasing key" key=key0108 subsys=allocator
level=info msg="Releasing key" key=key0109 subsys=allocator
level=info msg="Releasing key" key=key0110 subsys=allocator
level=info msg="Releasing key" key=key0111 subsys=allocator
level=info msg="Releasing key" key=key0112 subsys=allocator
level=info msg="Releasing key" key=key0113 subsys=allocator
level=info msg="Releasing key" key=key0114 subsys=allocator
level=info msg="Releasing key" key=key0115 subsys=allocator
level=info msg="Releasing key" key=key0116 subsys=allocator
level=info msg="Releasing key" key=key0117 subsys=allocator
level=info msg="Releasing key" key=key0118 subsys=allocator
level=info msg="Releasing key" key=key0119 subsys=allocator
level=info msg="Releasing key" key=key0120 subsys=allocator
level=info msg="Releasing key" key=key0121 subsys=allocator
level=info msg="Releasing key" key=key0122 subsys=allocator
level=info msg="Releasing key" key=key0123 subsys=allocator
level=info msg="Releasing key" key=key0124 subsys=allocator
level=info msg="Releasing key" key=key0125 subsys=allocator
level=info msg="Releasing key" key=key0126 subsys=allocator
level=info msg="Releasing key" key=key0127 subsys=allocator
level=info msg="Releasing key" key=key0128 subsys=allocator
level=info msg="Releasing key" key=key0129 subsys=allocator
level=info msg="Releasing key" key=key0130 subsys=allocator
level=info msg="Releasing key" key=key0131 subsys=allocator
level=info msg="Releasing key" key=key0132 subsys=allocator
level=info msg="Releasing key" key=key0133 subsys=allocator
level=info msg="Releasing key" key=key0134 subsys=allocator
level=info msg="Releasing key" key=key0135 subsys=allocator
level=info msg="Releasing key" key=key0136 subsys=allocator
level=info msg="Releasing key" key=key0137 subsys=allocator
level=info msg="Releasing key" key=key0138 subsys=allocator
level=info msg="Releasing key" key=key0139 subsys=allocator
level=info msg="Releasing key" key=key0140 subsys=allocator
level=info msg="Releasing key" key=key0141 subsys=allocator
level=info msg="Releasing key" key=key0142 subsys=allocator
level=info msg="Releasing key" key=key0143 subsys=allocator
level=info msg="Releasing key" key=key0144 subsys=allocator
level=info msg="Releasing key" key=key0145 subsys=allocator
level=info msg="Releasing key" key=key0146 subsys=allocator
level=info msg="Releasing key" key=key0147 subsys=allocator
level=info msg="Releasing key" key=key0148 subsys=allocator
level=info msg="Releasing key" key=key0149 subsys=allocator
level=info msg="Releasing key" key=key0150 subsys=allocator
level=info msg="Releasing key" key=key0151 subsys=allocator
level=info msg="Releasing key" key=key0152 subsys=allocator
level=info msg="Releasing key" key=key0153 subsys=allocator
level=info msg="Releasing key" key=key0154 subsys=allocator
level=info msg="Releasing key" key=key0155 subsys=allocator
level=info msg="Releasing key" key=key0156 subsys=allocator
level=info msg="Releasing key" key=key0157 subsys=allocator
level=info msg="Releasing key" key=key0158 subsys=allocator
level=info msg="Releasing key" key=key0159 subsys=allocator
level=info msg="Releasing key" key=key0160 subsys=allocator
level=info msg="Releasing key" key=key0161 subsys=allocator
level=info msg="Releasing key" key=key0162 subsys=allocator
level=info msg="Releasing key" key=key0163 subsys=allocator
level=info msg="Releasing key" key=key0164 subsys=allocator
level=info msg="Releasing key" key=key0165 subsys=allocator
level=info msg="Releasing key" key=key0166 subsys=allocator
level=info msg="Releasing key" key=key0167 subsys=allocator
level=info msg="Releasing key" key=key0168 subsys=allocator
level=info msg="Releasing key" key=key0169 subsys=allocator
level=info msg="Releasing key" key=key0170 subsys=allocator
level=info msg="Releasing key" key=key0171 subsys=allocator
level=info msg="Releasing key" key=key0172 subsys=allocator
level=info msg="Releasing key" key=key0173 subsys=allocator
level=info msg="Releasing key" key=key0174 subsys=allocator
level=info msg="Releasing key" key=key0175 subsys=allocator
level=info msg="Releasing key" key=key0176 subsys=allocator
level=info msg="Releasing key" key=key0177 subsys=allocator
level=info msg="Releasing key" key=key0178 subsys=allocator
level=info msg="Releasing key" key=key0179 subsys=allocator
level=info msg="Releasing key" key=key0180 subsys=allocator
level=info msg="Releasing key" key=key0181 subsys=allocator
level=info msg="Releasing key" key=key0182 subsys=allocator
level=info msg="Releasing key" key=key0183 subsys=allocator
level=info msg="Releasing key" key=key0184 subsys=allocator
level=info msg="Releasing key" key=key0185 subsys=allocator
level=info msg="Releasing key" key=key0186 subsys=allocator
level=info msg="Releasing key" key=key0187 subsys=allocator
level=info msg="Releasing key" key=key0188 subsys=allocator
level=info msg="Releasing key" key=key0189 subsys=allocator
level=info msg="Releasing key" key=key0190 subsys=allocator
level=info msg="Releasing key" key=key0191 subsys=allocator
level=info msg="Releasing key" key=key0192 subsys=allocator
level=info msg="Releasing key" key=key0193 subsys=allocator
level=info msg="Releasing key" key=key0194 subsys=allocator
level=info msg="Releasing key" key=key0195 subsys=allocator
level=info msg="Releasing key" key=key0196 subsys=allocator
level=info msg="Releasing key" key=key0197 subsys=allocator
level=info msg="Releasing key" key=key0198 subsys=allocator
level=info msg="Releasing key" key=key0199 subsys=allocator
level=info msg="Releasing key" key=key0200 subsys=allocator
level=info msg="Releasing key" key=key0201 subsys=allocator
level=info msg="Releasing key" key=key0202 subsys=allocator
level=info msg="Releasing key" key=key0203 subsys=allocator
level=info msg="Releasing key" key=key0204 subsys=allocator
level=info msg="Releasing key" key=key0205 subsys=allocator
level=info msg="Releasing key" key=key0206 subsys=allocator
level=info msg="Releasing key" key=key0207 subsys=allocator
level=info msg="Releasing key" key=key0208 subsys=allocator
level=info msg="Releasing key" key=key0209 subsys=allocator
level=info msg="Releasing key" key=key0210 subsys=allocator
level=info msg="Releasing key" key=key0211 subsys=allocator
level=info msg="Releasing key" key=key0212 subsys=allocator
level=info msg="Releasing key" key=key0213 subsys=allocator
level=info msg="Releasing key" key=key0214 subsys=allocator
level=info msg="Releasing key" key=key0215 subsys=allocator
level=info msg="Releasing key" key=key0216 subsys=allocator
level=info msg="Releasing key" key=key0217 subsys=allocator
level=info msg="Releasing key" key=key0218 subsys=allocator
level=info msg="Releasing key" key=key0219 subsys=allocator
level=info msg="Releasing key" key=key0220 subsys=allocator
level=info msg="Releasing key" key=key0221 subsys=allocator
level=info msg="Releasing key" key=key0222 subsys=allocator
level=info msg="Releasing key" key=key0223 subsys=allocator
level=info msg="Releasing key" key=key0224 subsys=allocator
level=info msg="Releasing key" key=key0225 subsys=allocator
level=info msg="Releasing key" key=key0226 subsys=allocator
level=info msg="Releasing key" key=key0227 subsys=allocator
level=info msg="Releasing key" key=key0228 subsys=allocator
level=info msg="Releasing key" key=key0229 subsys=allocator
level=info msg="Releasing key" key=key0230 subsys=allocator
level=info msg="Releasing key" key=key0231 subsys=allocator
level=info msg="Releasing key" key=key0232 subsys=allocator
level=info msg="Releasing key" key=key0233 subsys=allocator
level=info msg="Releasing key" key=key0234 subsys=allocator
level=info msg="Releasing key" key=key0235 subsys=allocator
level=info msg="Releasing key" key=key0236 subsys=allocator
level=info msg="Releasing key" key=key0237 subsys=allocator
level=info msg="Releasing key" key=key0238 subsys=allocator
level=info msg="Releasing key" key=key0239 subsys=allocator
level=info msg="Releasing key" key=key0240 subsys=allocator
level=info msg="Releasing key" key=key0241 subsys=allocator
level=info msg="Releasing key" key=key0242 subsys=allocator
level=info msg="Releasing key" key=key0243 subsys=allocator
level=info msg="Releasing key" key=key0244 subsys=allocator
level=info msg="Releasing key" key=key0245 subsys=allocator
level=info msg="Releasing key" key=key0246 subsys=allocator
level=info msg="Releasing key" key=key0247 subsys=allocator
level=info msg="Releasing key" key=key0248 subsys=allocator
level=info msg="Releasing key" key=key0249 subsys=allocator
level=info msg="Releasing key" key=key0250 subsys=allocator
level=info msg="Releasing key" key=key0251 subsys=allocator
level=info msg="Releasing key" key=key0252 subsys=allocator
level=info msg="Releasing key" key=key0253 subsys=allocator
level=info msg="Releasing key" key=key0254 subsys=allocator
level=info msg="Releasing key" key=key0255 subsys=allocator
level=info msg="Releasing key" key=key0256 subsys=allocator]]></system-out>
		</testcase>
		<testcase name="Test/AllocatorSuite/TestLocalKeys" classname="github.com/cilium/cilium/pkg/allocator" time="0.000"></testcase>
		<testcase name="Test/AllocatorSuite/TestPrefixMask" classname="github.com/cilium/cilium/pkg/allocator" time="0.000"></testcase>
		<testcase name="Test/AllocatorSuite/TestSelectID" classname="github.com/cilium/cilium/pkg/allocator" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/allocator	coverage: 63.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/annotation" tests="6" failures="0" errors="0" id="95" hostname="kind-bpf-next" time="0.006" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestGet" classname="github.com/cilium/cilium/pkg/annotation" time="0.000"></testcase>
		<testcase name="TestGet/the_searched_annotation_is_not_present" classname="github.com/cilium/cilium/pkg/annotation" time="0.000"></testcase>
		<testcase name="TestGet/the_searched_annotation_is_present_(preferred_key)" classname="github.com/cilium/cilium/pkg/annotation" time="0.000"></testcase>
		<testcase name="TestGet/the_searched_annotation_is_present_(alias)" classname="github.com/cilium/cilium/pkg/annotation" time="0.000"></testcase>
		<testcase name="TestGet/the_searched_annotation_is_present_(both_preferred_and_alias_keys)" classname="github.com/cilium/cilium/pkg/annotation" time="0.000"></testcase>
		<testcase name="TestGet/the_searched_annotation_is_present_(both_alias_keys)" classname="github.com/cilium/cilium/pkg/annotation" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/annotation	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/api" tests="15" failures="0" errors="0" id="96" hostname="kind-bpf-next" time="0.013" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestParseSpecPaths" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestParseSpecPaths/Basic_GET_BGP" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestParseSpecPaths/PUT_endpoints_by_ID" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestParseSpecPaths/DELETE_LRP_by_ID_with_suffix" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestParseSpecPaths/POST_kebab-case" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestParseSpecPaths/Multiple_endpoints_PATCH_and_PUT" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestParseSpecPaths/Multiple_methods_PATCH_and_PUT_ipam" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestAllowedFlagsToDeniedPaths" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestAllowedFlagsToDeniedPaths/deny_all" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestAllowedFlagsToDeniedPaths/wildcard:_allow_all" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestAllowedFlagsToDeniedPaths/wildcard:_allow_gets" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestAllowedFlagsToDeniedPaths/allow_invalid_option" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestAllowedFlagsToDeniedPaths/deny_all_empty_string" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestAllowedFlagsToDeniedPaths/wildcard:_invalid_prefix" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<testcase name="TestAllowedFlagsToDeniedPaths/wildcard:_invalid_multiple_wildcard" classname="github.com/cilium/cilium/pkg/api" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/api	coverage: 46.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/api/helpers" tests="5" failures="0" errors="0" id="97" hostname="kind-bpf-next" time="0.168" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/api/helpers" time="0.160"></testcase>
		<testcase name="Test/HelpersSuite" classname="github.com/cilium/cilium/pkg/api/helpers" time="0.160"></testcase>
		<testcase name="Test/HelpersSuite/TestRateLimitBurst" classname="github.com/cilium/cilium/pkg/api/helpers" time="0.010"></testcase>
		<testcase name="Test/HelpersSuite/TestRateLimitWait" classname="github.com/cilium/cilium/pkg/api/helpers" time="0.150"></testcase>
		<testcase name="Test/HelpersSuite/TestSetDelay" classname="github.com/cilium/cilium/pkg/api/helpers" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/api/helpers	coverage: 70.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/api/metrics/mock" tests="3" failures="0" errors="0" id="98" hostname="kind-bpf-next" time="0.005" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/api/metrics/mock" time="0.000"></testcase>
		<testcase name="Test/MockSuite" classname="github.com/cilium/cilium/pkg/api/metrics/mock" time="0.000"></testcase>
		<testcase name="Test/MockSuite/TestMock" classname="github.com/cilium/cilium/pkg/api/metrics/mock" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/api/metrics/mock	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/auth/certs" tests="0" failures="0" errors="0" id="99" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/auth" tests="26" failures="0" errors="0" id="100" hostname="kind-bpf-next" time="0.055" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test_alwaysFailAuthHandler_authenticate" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_alwaysFailAuthHandler_authenticate/Always_fail" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_authMapCache_restoreCache" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_newAuthManager_clashingAuthHandlers" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_newAuthManager" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_authManager_authenticate" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_authManager_authenticate/missing_handler_for_auth_type" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_authManager_authenticate/missing_node_IP_for_node_ID" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_authManager_authenticate/successful_auth" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_verifyPeerCertificate" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_verifyPeerCertificate/valid_certificate_with_SNI_to_match_identity" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_verifyPeerCertificate/valid_certificate_with_no_identity_provided" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_verifyPeerCertificate/error_on_invalid_certificate_because_incorrect_identity_provided" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_verifyPeerCertificate/error_on_invalid_certificate_signed_by_other_CA" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_verifyPeerCertificate/error_on_invalid_certificate_signed_by_other_CA_with_no_identity_provided" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_verifyPeerCertificate/error_on_no_certificates_in_verifiedChains" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_verifyPeerCertificate/error_on_empty_caBundle_provided" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_GetCertificateForIncomingConnection" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_GetCertificateForIncomingConnection/valid_certificate_with_SNI_to_match_identity" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_GetCertificateForIncomingConnection/no_certificate_for_non_existing_identity" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_GetCertificateForIncomingConnection/no_certificate_for_random_non_existing_domain" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_authenticate" classname="github.com/cilium/cilium/pkg/auth" time="0.010">
			<system-out><![CDATA[level=info msg="Starting mTLS auth handler" subsys=auth
level=info msg="Started mTLS listener" port=45167 subsys=auth]]></system-out>
		</testcase>
		<testcase name="Test_mtlsAuthHandler_authenticate/authenticate_two_valid_identities" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_authenticate/error_on_authenticate_when_remote_identity_is_not_valid" classname="github.com/cilium/cilium/pkg/auth" time="0.000"></testcase>
		<testcase name="Test_mtlsAuthHandler_authenticate/error_on__authenticate_when_local_identity_is_not_valid" classname="github.com/cilium/cilium/pkg/auth" time="0.000">
			<system-out><![CDATA[level=error msg="failed to perform TLS handshake" error="no certificate for spiffe://spiffe.cilium/identity/9999" subsys=auth]]></system-out>
		</testcase>
		<testcase name="Test_mtlsAuthHandler_authenticate/error_on_authenticate_when_auth_request_is_bad" classname="github.com/cilium/cilium/pkg/auth" time="0.000">
			<system-out><![CDATA[level=error msg="failed to perform TLS handshake" error="failed to get identity for SNI unknown.spiffe.cilium: strconv.ParseUint: parsing \"unknown\": invalid syntax" subsys=auth
level=info msg="Stopping mTLS auth handler" subsys=auth]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/auth	coverage: 50.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/auth/spire" tests="24" failures="0" errors="0" id="101" hostname="kind-bpf-next" time="0.033" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestSpireDelegateClient_NumericIdentityToSNI" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_NumericIdentityToSNI/convert_valid_numeric_identity" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_SNIToNumericIdentity" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_SNIToNumericIdentity/convert_valid_SNI" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_SNIToNumericIdentity/error_on_convert_invalid_SNI_under_trust_domain" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_SNIToNumericIdentity/error_on_convert_invalid_SNI_outside_trust_domain" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_sniToSPIFFEID" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_sniToSPIFFEID/convert_valid_numeric_identity" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_ValidateIdentity" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_ValidateIdentity/validate_with_correct_SPIFFE_ID" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_ValidateIdentity/not_validate_with_incorrect_SPIFFE_ID" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_ValidateIdentity/error_on_validate_with_incorrect_SPIFFE_cert" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_ValidateIdentity/error_on_validate_with_non_SPIFFE_cert" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_GetTrustBundle" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_GetTrustBundle/get_trust_bundle" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_GetTrustBundle/error_on_no_trust_bundle" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_GetCertificateForIdentity" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.010"></testcase>
		<testcase name="TestSpireDelegateClient_GetCertificateForIdentity/get_certificate_for_numeric_identity" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_GetCertificateForIdentity/error_on_no_certificate_for_numeric_identity" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_GetCertificateForIdentity/error_on_no_certchain_for_numeric_identity" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_GetCertificateForIdentity/error_on_no_private_key_for_numeric_identity" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_SubscribeToRotatedIdentities" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_SubscribeToRotatedIdentities/receive_no_event_on_a_new_ID" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<testcase name="TestSpireDelegateClient_SubscribeToRotatedIdentities/receive_1_updated_event" classname="github.com/cilium/cilium/pkg/auth/spire" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/auth/spire	coverage: 38.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/aws/ec2" tests="8" failures="0" errors="0" id="102" hostname="kind-bpf-next" time="0.026" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestNewSubnetsFilters" classname="github.com/cilium/cilium/pkg/aws/ec2" time="0.000"></testcase>
		<testcase name="TestNewSubnetsFilters/empty_arguments" classname="github.com/cilium/cilium/pkg/aws/ec2" time="0.000"></testcase>
		<testcase name="TestNewSubnetsFilters/ids_only" classname="github.com/cilium/cilium/pkg/aws/ec2" time="0.000"></testcase>
		<testcase name="TestNewSubnetsFilters/tags_only" classname="github.com/cilium/cilium/pkg/aws/ec2" time="0.000"></testcase>
		<testcase name="TestNewSubnetsFilters/tags_and_ids" classname="github.com/cilium/cilium/pkg/aws/ec2" time="0.000"></testcase>
		<testcase name="TestNewTagsFilters" classname="github.com/cilium/cilium/pkg/aws/ec2" time="0.000"></testcase>
		<testcase name="TestNewTagsFilters/empty_arguments" classname="github.com/cilium/cilium/pkg/aws/ec2" time="0.000"></testcase>
		<testcase name="TestNewTagsFilters/tags" classname="github.com/cilium/cilium/pkg/aws/ec2" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/aws/ec2	coverage: 3.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/aws/endpoints" tests="0" failures="0" errors="0" id="103" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/aws/ec2/mock" tests="8" failures="0" errors="0" id="104" hostname="kind-bpf-next" time="0.026" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/aws/ec2/mock" time="0.020"></testcase>
		<testcase name="Test/MockSuite" classname="github.com/cilium/cilium/pkg/aws/ec2/mock" time="0.020"></testcase>
		<testcase name="Test/MockSuite/TestGetNextSubnet" classname="github.com/cilium/cilium/pkg/aws/ec2/mock" time="0.010"></testcase>
		<testcase name="Test/MockSuite/TestMock" classname="github.com/cilium/cilium/pkg/aws/ec2/mock" time="0.000"></testcase>
		<testcase name="Test/MockSuite/TestPrefixCeil" classname="github.com/cilium/cilium/pkg/aws/ec2/mock" time="0.000"></testcase>
		<testcase name="Test/MockSuite/TestPrefixToIps" classname="github.com/cilium/cilium/pkg/aws/ec2/mock" time="0.000"></testcase>
		<testcase name="Test/MockSuite/TestSetLimiter" classname="github.com/cilium/cilium/pkg/aws/ec2/mock" time="0.000"></testcase>
		<testcase name="Test/MockSuite/TestSetMockError" classname="github.com/cilium/cilium/pkg/aws/ec2/mock" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/aws/ec2/mock	coverage: 39.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/aws/eni/types" tests="0" failures="0" errors="0" id="105" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/aws/metadata" tests="0" failures="0" errors="0" id="106" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/aws/types" tests="0" failures="0" errors="0" id="107" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/aws/eni" tests="24" failures="0" errors="0" id="108" hostname="kind-bpf-next" skipped="1" time="6.383" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/aws/eni" time="6.310"></testcase>
		<testcase name="Test/ENISuite" classname="github.com/cilium/cilium/pkg/aws/eni" time="6.310"></testcase>
		<testcase name="Test/ENISuite/TestFindSubnetByIDs" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.000">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=2 numSecurityGroups=2 numSubnets=3 numVPCs=2 subsys=eni
level=info msg="Synchronized ENI information" numInstances=2 numSecurityGroups=4 numSubnets=3 numVPCs=2 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestFindSubnetByTags" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.000">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=2 numSecurityGroups=2 numSubnets=2 numVPCs=2 subsys=eni
level=info msg="Synchronized ENI information" numInstances=2 numSecurityGroups=4 numSubnets=3 numVPCs=2 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestGetMaximumAllocatableIPv4" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.000">
			<system-out><![CDATA[level=warning msg="Could not determine first interface index, maximum allocatable ipv4 addresses will be 0 (unlimited) this could lead to ip allocation overflows if the max-allocate flag is not set" first-interface-index=unknown subsys=eni
level=warning msg="Instance type network adapters limit is lower than the configured FirstInterfaceIndex, maximum allocatable ipv4 addresses will be 0 (unlimited) this could lead to ip allocation overflows if the max-allocate flag is not set" adaptors-limit=3 first-interface-index=4 subsys=eni
level=warning msg="Could not determined instance limits, maximum allocatable ipv4 addresses will be 0 (unlimited) this could lead to ip allocation overflows if the max-allocate flag is not set" adaptors-limit=unknown first-interface-index=0 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestGetNodeNames" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.000">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=0 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=warning msg="Instance not found! Please delete corresponding ciliumnode if instance has already been deleted." instanceID= name=node1 subsys=ipam
level=info msg="Synchronized ENI information" numInstances=0 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node2 subsys=ipam
level=warning msg="Instance not found! Please delete corresponding ciliumnode if instance has already been deleted." instanceID= name=node2 subsys=ipam]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestGetSecurityGroupByTags" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.000">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=2 numSecurityGroups=2 numSubnets=2 numVPCs=2 subsys=eni
level=info msg="Synchronized ENI information" numInstances=2 numSecurityGroups=4 numSubnets=3 numVPCs=2 subsys=eni
level=info msg="Synchronized ENI information" numInstances=2 numSecurityGroups=4 numSubnets=3 numVPCs=2 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestGetSubnet" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.000">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=2 numSecurityGroups=2 numSubnets=2 numVPCs=2 subsys=eni
level=info msg="Synchronized ENI information" numInstances=2 numSecurityGroups=4 numSubnets=3 numVPCs=2 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestGetUsedIPWithPrefixes" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.000"></testcase>
		<testcase name="Test/ENISuite/TestInstanceBeenDeleted" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.000">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=info msg="Synchronized ENI information" numInstances=0 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Instance not found! Please delete corresponding ciliumnode if instance has already been deleted." instanceID=i-testInstanceBeenDeleted-0 name=node1 subsys=ipam
level=warning msg="Instance not found! Please delete corresponding ciliumnode if instance has already been deleted." instanceID=i-testInstanceBeenDeleted-0 name=node1 subsys=ipam]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestInterfaceCreatedInInitialSubnet" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.020">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=2 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testCreateInterfaceInCorrectSubnet-1 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=14 emptyInterfaceSlots=2 instanceID=i-testCreateInterfaceInCorrectSubnet-1 maxIPsToAllocate=16 name=node1 neededIPs=16 remainingInterfaces=3 selectedInterface=51ed0d58-3193-47ce-b069-0f50e8ab1b8a selectedPoolID=s-1 subsys=ipam used=0
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=2 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testCreateInterfaceInCorrectSubnet-1 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=14 availableForAllocation=0 emptyInterfaceSlots=2 instanceID=i-testCreateInterfaceInCorrectSubnet-1 maxIPsToAllocate=2 name=node1 neededIPs=2 remainingInterfaces=2 selectedInterface= selectedPoolID= subsys=ipam used=0
level=info msg="No more IPs available, creating new ENI" addresses=2 instanceID=i-testCreateInterfaceInCorrectSubnet-1 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Created new ENI" addresses=2 eniID=74f3e580-0e50-406d-91cd-b194822c335c instanceID=i-testCreateInterfaceInCorrectSubnet-1 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Attached ENI to instance" addresses=2 attachmentID= eniID=74f3e580-0e50-406d-91cd-b194822c335c index=1 instanceID=i-testCreateInterfaceInCorrectSubnet-1 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=2 numVPCs=1 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestNodeManagerDefaultAllocation" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.020">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerDefaultAllocation-0 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=9 emptyInterfaceSlots=2 instanceID=i-testNodeManagerDefaultAllocation-0 maxIPsToAllocate=8 name=node1 neededIPs=8 remainingInterfaces=3 selectedInterface=4528e1be-104c-44e6-9415-1d2276b39b27 selectedPoolID=s-1 subsys=ipam used=0
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerDefaultAllocation-0 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=8 availableForAllocation=1 emptyInterfaceSlots=2 instanceID=i-testNodeManagerDefaultAllocation-0 maxIPsToAllocate=7 name=node1 neededIPs=7 remainingInterfaces=3 selectedInterface=4528e1be-104c-44e6-9415-1d2276b39b27 selectedPoolID=s-1 subsys=ipam used=7
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerDefaultAllocation-0 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=9 availableForAllocation=0 emptyInterfaceSlots=2 instanceID=i-testNodeManagerDefaultAllocation-0 maxIPsToAllocate=6 name=node1 neededIPs=6 remainingInterfaces=2 selectedInterface= selectedPoolID= subsys=ipam used=7
level=info msg="No more IPs available, creating new ENI" addresses=6 instanceID=i-testNodeManagerDefaultAllocation-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Created new ENI" addresses=6 eniID=162cf911-54c4-489e-9622-26c9e83ddbbe instanceID=i-testNodeManagerDefaultAllocation-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Attached ENI to instance" addresses=6 attachmentID= eniID=162cf911-54c4-489e-9622-26c9e83ddbbe index=1 instanceID=i-testNodeManagerDefaultAllocation-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestNodeManagerENIExcludeInterfaceTags" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.040">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerDefaultAllocation-0 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=0 emptyInterfaceSlots=2 instanceID=i-testNodeManagerDefaultAllocation-0 maxIPsToAllocate=8 name=node1 neededIPs=8 remainingInterfaces=2 selectedInterface= selectedPoolID= subsys=ipam used=0
level=info msg="No more IPs available, creating new ENI" addresses=8 instanceID=i-testNodeManagerDefaultAllocation-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Created new ENI" addresses=8 eniID=ac7d3f06-8b30-4ff5-a8c3-36e3e9ad0f6b instanceID=i-testNodeManagerDefaultAllocation-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Attached ENI to instance" addresses=8 attachmentID= eniID=ac7d3f06-8b30-4ff5-a8c3-36e3e9ad0f6b index=1 instanceID=i-testNodeManagerDefaultAllocation-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerDefaultAllocation-0 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=8 availableForAllocation=1 emptyInterfaceSlots=1 instanceID=i-testNodeManagerDefaultAllocation-0 maxIPsToAllocate=7 name=node1 neededIPs=7 remainingInterfaces=2 selectedInterface=ac7d3f06-8b30-4ff5-a8c3-36e3e9ad0f6b selectedPoolID=s-1 subsys=ipam used=7
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerDefaultAllocation-0 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=9 availableForAllocation=0 emptyInterfaceSlots=1 instanceID=i-testNodeManagerDefaultAllocation-0 maxIPsToAllocate=6 name=node1 neededIPs=6 remainingInterfaces=1 selectedInterface= selectedPoolID= subsys=ipam used=7
level=info msg="No more IPs available, creating new ENI" addresses=6 instanceID=i-testNodeManagerDefaultAllocation-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Created new ENI" addresses=6 eniID=aa9f5b0f-8999-4683-b71b-1107a7c5fafa instanceID=i-testNodeManagerDefaultAllocation-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Attached ENI to instance" addresses=6 attachmentID= eniID=aa9f5b0f-8999-4683-b71b-1107a7c5fafa index=2 instanceID=i-testNodeManagerDefaultAllocation-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestNodeManagerENIWithSGTags" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.030">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerDefaultAllocation-0 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=9 emptyInterfaceSlots=2 instanceID=i-testNodeManagerDefaultAllocation-0 maxIPsToAllocate=8 name=node1 neededIPs=8 remainingInterfaces=3 selectedInterface=787a7839-ff3d-4eb4-8b7f-316beb8abfef selectedPoolID=s-1 subsys=ipam used=0
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerDefaultAllocation-0 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=8 availableForAllocation=1 emptyInterfaceSlots=2 instanceID=i-testNodeManagerDefaultAllocation-0 maxIPsToAllocate=7 name=node1 neededIPs=7 remainingInterfaces=3 selectedInterface=787a7839-ff3d-4eb4-8b7f-316beb8abfef selectedPoolID=s-1 subsys=ipam used=7
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerDefaultAllocation-0 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=9 availableForAllocation=0 emptyInterfaceSlots=2 instanceID=i-testNodeManagerDefaultAllocation-0 maxIPsToAllocate=6 name=node1 neededIPs=6 remainingInterfaces=2 selectedInterface= selectedPoolID= subsys=ipam used=7
level=info msg="No more IPs available, creating new ENI" addresses=6 instanceID=i-testNodeManagerDefaultAllocation-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg-1]" subnetID=s-1 subsys=ipam
level=info msg="Created new ENI" addresses=6 eniID=d708fa27-8162-4938-b992-100c6c5c6691 instanceID=i-testNodeManagerDefaultAllocation-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg-1]" subnetID=s-1 subsys=ipam
level=info msg="Attached ENI to instance" addresses=6 attachmentID= eniID=d708fa27-8162-4938-b992-100c6c5c6691 index=1 instanceID=i-testNodeManagerDefaultAllocation-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg-1]" subnetID=s-1 subsys=ipam
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestNodeManagerExceedENICapacity" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.040">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node2 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerExceedENICapacity-1 name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=14 emptyInterfaceSlots=2 instanceID=i-testNodeManagerExceedENICapacity-1 maxIPsToAllocate=20 name=node2 neededIPs=20 remainingInterfaces=3 selectedInterface=915477d7-fa96-43f9-a45f-7bf10e1a55fa selectedPoolID=s-1 subsys=ipam used=0
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerExceedENICapacity-1 name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=14 availableForAllocation=0 emptyInterfaceSlots=2 instanceID=i-testNodeManagerExceedENICapacity-1 maxIPsToAllocate=6 name=node2 neededIPs=6 remainingInterfaces=2 selectedInterface= selectedPoolID= subsys=ipam used=0
level=info msg="No more IPs available, creating new ENI" addresses=6 instanceID=i-testNodeManagerExceedENICapacity-1 isPrefixDelegated=false name=node2 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Created new ENI" addresses=6 eniID=058cdfd1-9d5a-4605-acce-0d11dcc7a203 instanceID=i-testNodeManagerExceedENICapacity-1 isPrefixDelegated=false name=node2 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Attached ENI to instance" addresses=6 attachmentID= eniID=058cdfd1-9d5a-4605-acce-0d11dcc7a203 index=1 instanceID=i-testNodeManagerExceedENICapacity-1 isPrefixDelegated=false name=node2 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerExceedENICapacity-1 name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=20 availableForAllocation=8 emptyInterfaceSlots=1 instanceID=i-testNodeManagerExceedENICapacity-1 maxIPsToAllocate=22 name=node2 neededIPs=22 remainingInterfaces=2 selectedInterface=058cdfd1-9d5a-4605-acce-0d11dcc7a203 selectedPoolID=s-1 subsys=ipam used=40
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Resolving IP deficit of node" available=28 availableForAllocation=0 emptyInterfaceSlots=1 instanceID=i-testNodeManagerExceedENICapacity-1 maxIPsToAllocate=14 name=node2 neededIPs=14 remainingInterfaces=1 selectedInterface= selectedPoolID= subsys=ipam used=40
level=info msg="No more IPs available, creating new ENI" addresses=14 instanceID=i-testNodeManagerExceedENICapacity-1 isPrefixDelegated=false name=node2 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Created new ENI" addresses=14 eniID=dbafe06c-cc64-4c2c-8009-a55c3e4a90e7 instanceID=i-testNodeManagerExceedENICapacity-1 isPrefixDelegated=false name=node2 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Attached ENI to instance" addresses=14 attachmentID= eniID=dbafe06c-cc64-4c2c-8009-a55c3e4a90e7 index=2 instanceID=i-testNodeManagerExceedENICapacity-1 isPrefixDelegated=false name=node2 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestNodeManagerGet" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.000">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=0 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=warning msg="Instance not found! Please delete corresponding ciliumnode if instance has already been deleted." instanceID= name=node1 subsys=ipam]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestNodeManagerInstanceNotRunning" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.010">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerInstanceNotRunning-0 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=0 emptyInterfaceSlots=1 instanceID=i-testNodeManagerInstanceNotRunning-0 maxIPsToAllocate=8 name=node1 neededIPs=8 remainingInterfaces=1 selectedInterface= selectedPoolID= subsys=ipam used=0
level=info msg="No more IPs available, creating new ENI" addresses=8 instanceID=i-testNodeManagerInstanceNotRunning-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Created new ENI" addresses=8 eniID=24a679f3-d995-42a0-ac7f-d40232e123e7 instanceID=i-testNodeManagerInstanceNotRunning-0 isPrefixDelegated=false name=node1 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Set running false" instanceID=i-testNodeManagerInstanceNotRunning-0 name=node1 subsys=ipam
level=info msg="Marking node as not running" subsys=eni
level=warning msg="Unable to create interface on instance: unable to attach ENI at index 1: foo is not 'running' foo" instanceID=i-testNodeManagerInstanceNotRunning-0 name=node1 subsys=ipam
level=warning msg="Unable to maintain ip pool of node" error="unable to attach ENI at index 1: foo is not 'running' foo" instanceID=i-testNodeManagerInstanceNotRunning-0 name=node1 subsys=ipam
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestNodeManagerManyNodes" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.000">
			<skipped message="Skipped"><![CDATA[    node_manager_test.go:657: This test is flaky, see https://github.com/cilium/cilium/issues/11560]]></skipped>
		</testcase>
		<testcase name="Test/ENISuite/TestNodeManagerMinAllocate20" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.020">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node2 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerMinAllocate20-1 name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=29 emptyInterfaceSlots=7 instanceID=i-testNodeManagerMinAllocate20-1 maxIPsToAllocate=10 name=node2 neededIPs=10 remainingInterfaces=8 selectedInterface=495aa78f-b216-45b7-82b4-28366cee555c selectedPoolID=s-1 subsys=ipam used=0
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerMinAllocate20-1 name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=10 availableForAllocation=19 emptyInterfaceSlots=7 instanceID=i-testNodeManagerMinAllocate20-1 maxIPsToAllocate=10 name=node2 neededIPs=10 remainingInterfaces=8 selectedInterface=495aa78f-b216-45b7-82b4-28366cee555c selectedPoolID=s-1 subsys=ipam used=8
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestNodeManagerMinAllocateAndPreallocate" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.030">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node2 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerMinAllocateAndPreallocate-1 name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=9 emptyInterfaceSlots=2 instanceID=i-testNodeManagerMinAllocateAndPreallocate-1 maxIPsToAllocate=10 name=node2 neededIPs=10 remainingInterfaces=3 selectedInterface=d911594f-937b-49b5-ae8e-6ba5855464d1 selectedPoolID=s-1 subsys=ipam used=0
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerMinAllocateAndPreallocate-1 name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=9 availableForAllocation=0 emptyInterfaceSlots=2 instanceID=i-testNodeManagerMinAllocateAndPreallocate-1 maxIPsToAllocate=1 name=node2 neededIPs=1 remainingInterfaces=2 selectedInterface= selectedPoolID= subsys=ipam used=0
level=info msg="No more IPs available, creating new ENI" addresses=1 instanceID=i-testNodeManagerMinAllocateAndPreallocate-1 isPrefixDelegated=false name=node2 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Created new ENI" addresses=1 eniID=3f6eb655-733d-439b-ab13-211a031bd69c instanceID=i-testNodeManagerMinAllocateAndPreallocate-1 isPrefixDelegated=false name=node2 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Attached ENI to instance" addresses=1 attachmentID= eniID=3f6eb655-733d-439b-ab13-211a031bd69c index=1 instanceID=i-testNodeManagerMinAllocateAndPreallocate-1 isPrefixDelegated=false name=node2 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerMinAllocateAndPreallocate-1 name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=10 availableForAllocation=8 emptyInterfaceSlots=1 instanceID=i-testNodeManagerMinAllocateAndPreallocate-1 maxIPsToAllocate=1 name=node2 neededIPs=1 remainingInterfaces=2 selectedInterface=3f6eb655-733d-439b-ab13-211a031bd69c selectedPoolID=s-1 subsys=ipam used=10
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestNodeManagerPrefixDelegation" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.010">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerDefaultAllocation-0 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=16 availableForAllocation=128 emptyInterfaceSlots=2 instanceID=i-testNodeManagerDefaultAllocation-0 maxIPsToAllocate=4 name=node1 neededIPs=4 remainingInterfaces=3 selectedInterface=290991df-16b3-4fb0-8f39-3ae35aaa891b selectedPoolID=s-1 subsys=ipam used=12
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestNodeManagerReleaseAddress" classname="github.com/cilium/cilium/pkg/aws/eni" time="6.030">
			<system-out><![CDATA[level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Discovered new CiliumNode custom resource" name=node3 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerReleaseAddress-1 name=node3 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=14 emptyInterfaceSlots=3 instanceID=i-testNodeManagerReleaseAddress-1 maxIPsToAllocate=13 name=node3 neededIPs=10 remainingInterfaces=4 selectedInterface=a44b4632-06f3-42f5-8f4c-c82e4c3ed473 selectedPoolID=s-1 subsys=ipam used=0
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerReleaseAddress-1 name=node3 subsys=ipam
level=info msg="Resolving IP deficit of node" available=13 availableForAllocation=1 emptyInterfaceSlots=3 instanceID=i-testNodeManagerReleaseAddress-1 maxIPsToAllocate=5 name=node3 neededIPs=2 remainingInterfaces=4 selectedInterface=a44b4632-06f3-42f5-8f4c-c82e4c3ed473 selectedPoolID=s-1 subsys=ipam used=13
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=i-testNodeManagerReleaseAddress-1 name=node3 subsys=ipam
level=info msg="Resolving IP deficit of node" available=14 availableForAllocation=0 emptyInterfaceSlots=3 instanceID=i-testNodeManagerReleaseAddress-1 maxIPsToAllocate=4 name=node3 neededIPs=1 remainingInterfaces=3 selectedInterface= selectedPoolID= subsys=ipam used=13
level=info msg="No more IPs available, creating new ENI" addresses=4 instanceID=i-testNodeManagerReleaseAddress-1 isPrefixDelegated=false name=node3 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Created new ENI" addresses=4 eniID=72c64fe8-ccaa-44f9-940c-11ecff0160a9 instanceID=i-testNodeManagerReleaseAddress-1 isPrefixDelegated=false name=node3 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Attached ENI to instance" addresses=4 attachmentID= eniID=72c64fe8-ccaa-44f9-940c-11ecff0160a9 index=1 instanceID=i-testNodeManagerReleaseAddress-1 isPrefixDelegated=false name=node3 securityGroupIDs="[sg1 sg2]" subnetID=s-1 subsys=ipam
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni
level=info msg="Releasing excess IPs from node" available=18 excess=3 excessIps="[10.10.11.167 10.10.36.65 10.10.64.191]" instanceID=i-testNodeManagerReleaseAddress-1 name=node3 releasing="[10.10.11.167 10.10.36.65 10.10.64.191]" selectedInterface=72c64fe8-ccaa-44f9-940c-11ecff0160a9 selectedPoolID=s-1 subsys=ipam used=10
level=info msg="Synchronized ENI information" numInstances=1 numSecurityGroups=2 numSubnets=1 numVPCs=1 subsys=eni]]></system-out>
		</testcase>
		<testcase name="Test/ENISuite/TestStartENIGarbageCollector" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.040">
			<system-out><![CDATA[level=info msg="Starting to garbage collect detached ENIs" subsys=eni
level=debug msg="Starting new controller" name=ipam-eni-gc subsys=controller uuid=9903d2dd-0087-4aad-be63-2c2774213d9f
level=debug msg="Marked unattached interfaces for garbage collection" numInterfaces=4 subsys=eni
level=debug msg="Controller func execution time: 31.419µs" name=ipam-eni-gc subsys=controller uuid=9903d2dd-0087-4aad-be63-2c2774213d9f
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipam-eni-gc subsys=controller uuid=9903d2dd-0087-4aad-be63-2c2774213d9f
level=debug msg="Setting resync needed" instanceID=i-testNodeManagerReleaseAddress-1 name=node3 subsys=ipam
level=debug msg="Recalculated needed addresses" available=15 capacity=56 instanceID=i-testNodeManagerReleaseAddress-1 name=node3 remainingInterfaces=3 resyncNeeded="2023-05-31 11:41:36.16224928 +0000 UTC m=+6.338469613" subsys=ipam toAlloc=0 toRelease=0 used=10 waitingForPoolMaintenance=false
level=debug msg="Garbage collecting ENI" eniID=737567e6-c654-41b8-a760-e0107ffcde9a subsys=eni
level=debug msg="Garbage collecting ENI" eniID=a95bd0a9-041c-477f-ae7f-935984ed95c6 subsys=eni
level=debug msg="Garbage collecting ENI" eniID=6099fc81-ac86-41fa-a5c1-13df7f202baf subsys=eni
level=debug msg="Garbage collecting ENI" eniID=129c657a-e869-4b83-891b-4fb6f83766a7 subsys=eni
level=debug msg="Marked unattached interfaces for garbage collection" numInterfaces=4 subsys=eni
level=debug msg="Controller func execution time: 52.909µs" name=ipam-eni-gc subsys=controller uuid=9903d2dd-0087-4aad-be63-2c2774213d9f
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipam-eni-gc subsys=controller uuid=9903d2dd-0087-4aad-be63-2c2774213d9f
level=debug msg="Garbage collecting ENI" eniID=06532ce8-0d68-4b1f-a21e-42bd5fee8e41 subsys=eni
level=debug msg="Garbage collecting ENI" eniID=56351ff6-ca71-485d-ae80-28063498f521 subsys=eni
level=debug msg="Garbage collecting ENI" eniID=0e194f90-775f-44b8-b830-90b82780c36e subsys=eni
level=debug msg="Garbage collecting ENI" eniID=2065db8f-9b51-4953-bd43-e22b4306ea17 subsys=eni
level=debug msg="Marked unattached interfaces for garbage collection" numInterfaces=1 subsys=eni
level=debug msg="Controller func execution time: 111.457µs" name=ipam-eni-gc subsys=controller uuid=9903d2dd-0087-4aad-be63-2c2774213d9f
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipam-eni-gc subsys=controller uuid=9903d2dd-0087-4aad-be63-2c2774213d9f
level=debug msg="Garbage collecting ENI" eniID=c2b20c7d-7bef-41e4-be8d-d50bc55a5f25 subsys=eni
level=debug msg="Failed to garbage collect ENI" error="ENI ID c2b20c7d-7bef-41e4-be8d-d50bc55a5f25 is attached and cannot be deleted" subsys=eni
level=debug msg="Controller func execution time: 62.547µs" name=ipam-eni-gc subsys=controller uuid=9903d2dd-0087-4aad-be63-2c2774213d9f
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipam-eni-gc subsys=controller uuid=9903d2dd-0087-4aad-be63-2c2774213d9f]]></system-out>
		</testcase>
		<testcase name="TestENIIPAMCapacityAccounting" classname="github.com/cilium/cilium/pkg/aws/eni" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/aws/eni	coverage: 79.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/aws/eni/limits" tests="5" failures="0" errors="0" id="109" hostname="kind-bpf-next" time="0.022" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/aws/eni/limits" time="0.000"></testcase>
		<testcase name="Test/ENILimitsSuite" classname="github.com/cilium/cilium/pkg/aws/eni/limits" time="0.000"></testcase>
		<testcase name="Test/ENILimitsSuite/TestGet" classname="github.com/cilium/cilium/pkg/aws/eni/limits" time="0.000"></testcase>
		<testcase name="Test/ENILimitsSuite/TestParseLimitString" classname="github.com/cilium/cilium/pkg/aws/eni/limits" time="0.000"></testcase>
		<testcase name="Test/ENILimitsSuite/TestUpdateFromUserDefinedMappings" classname="github.com/cilium/cilium/pkg/aws/eni/limits" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/aws/eni/limits	coverage: 61.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/azure/api" tests="0" failures="0" errors="0" id="110" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/azure/api/mock" tests="5" failures="0" errors="0" id="111" hostname="kind-bpf-next" time="0.012" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/azure/api/mock" time="0.000"></testcase>
		<testcase name="Test/MockSuite" classname="github.com/cilium/cilium/pkg/azure/api/mock" time="0.000"></testcase>
		<testcase name="Test/MockSuite/TestMock" classname="github.com/cilium/cilium/pkg/azure/api/mock" time="0.000"></testcase>
		<testcase name="Test/MockSuite/TestSetLimiter" classname="github.com/cilium/cilium/pkg/azure/api/mock" time="0.000"></testcase>
		<testcase name="Test/MockSuite/TestSetMockError" classname="github.com/cilium/cilium/pkg/azure/api/mock" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/azure/api/mock	coverage: 84.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/azure/ipam" tests="8" failures="0" errors="0" id="112" hostname="kind-bpf-next" time="0.152" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/azure/ipam" time="0.110"></testcase>
		<testcase name="Test/IPAMSuite" classname="github.com/cilium/cilium/pkg/azure/ipam" time="0.110"></testcase>
		<testcase name="Test/IPAMSuite/TestGetMaximumAllocatableIPv4" classname="github.com/cilium/cilium/pkg/azure/ipam" time="0.000"></testcase>
		<testcase name="Test/IPAMSuite/TestGetVpcsAndSubnets" classname="github.com/cilium/cilium/pkg/azure/ipam" time="0.000">
			<system-out><![CDATA[level=info msg="Synchronized Azure IPAM information" numInstances=2 numSubnets=2 numVirtualNetworks=2 subsys=azure
level=info msg="Synchronized Azure IPAM information" numInstances=2 numSubnets=3 numVirtualNetworks=2 subsys=azure]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestIpamManyNodes" classname="github.com/cilium/cilium/pkg/azure/ipam" time="0.060">
			<system-out><![CDATA[level=info msg="Synchronized Azure IPAM information" numInstances=100 numSubnets=3 numVirtualNetworks=1 subsys=azure
level=info msg="Discovered new CiliumNode custom resource" name=node0 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node2 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node3 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node4 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node5 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm1 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm1 maxIPsToAllocate=10 name=node1 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm1/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=info msg="Discovered new CiliumNode custom resource" name=node6 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node7 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node8 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node9 subsys=ipam
level=info msg="Synchronized Azure IPAM information" numInstances=100 numSubnets=3 numVirtualNetworks=1 subsys=azure
level=info msg="Discovered new CiliumNode custom resource" name=node10 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node11 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node12 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm10 name=node10 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm10 maxIPsToAllocate=10 name=node10 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm10/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm0 name=node0 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm0 maxIPsToAllocate=10 name=node0 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm0/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=info msg="Discovered new CiliumNode custom resource" name=node13 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node14 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm7 name=node7 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm7 maxIPsToAllocate=10 name=node7 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm7/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm11 name=node11 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm11 maxIPsToAllocate=10 name=node11 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm11/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=info msg="Discovered new CiliumNode custom resource" name=node15 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node16 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node17 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node18 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node19 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node20 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node21 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node22 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node23 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node24 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node25 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node26 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node27 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node28 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node29 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node30 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node31 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node32 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node33 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node34 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node35 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node36 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node37 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node38 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node39 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node40 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node41 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node42 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node43 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node44 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node45 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node46 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node47 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node48 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node49 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node50 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node51 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node52 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node53 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node54 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node55 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node56 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node57 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node58 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node59 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node60 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node61 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node62 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node63 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node64 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node65 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node66 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node67 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node68 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node69 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node70 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node71 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm12 name=node12 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm12 maxIPsToAllocate=10 name=node12 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm12/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm2 name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm2 maxIPsToAllocate=10 name=node2 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm2/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=info msg="Discovered new CiliumNode custom resource" name=node72 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node73 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node74 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm3 name=node3 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm3 maxIPsToAllocate=10 name=node3 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm3/networkInterfaces/vmss11 selectedPoolID=s-2 subsys=ipam used=0
level=info msg="Discovered new CiliumNode custom resource" name=node75 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node76 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node77 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node78 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node79 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node80 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node81 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node82 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node83 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node84 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node85 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node86 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node87 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node88 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node89 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node90 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node91 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node92 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node93 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node94 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node95 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node96 subsys=ipam
level=info msg="Synchronized Azure IPAM information" numInstances=100 numSubnets=3 numVirtualNetworks=1 subsys=azure
level=info msg="Discovered new CiliumNode custom resource" name=node97 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node98 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node99 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm6 name=node6 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm6 maxIPsToAllocate=10 name=node6 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm6/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm98 name=node98 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm98 maxIPsToAllocate=10 name=node98 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm98/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm99 name=node99 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm99 maxIPsToAllocate=10 name=node99 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm99/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm44 name=node44 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm44 maxIPsToAllocate=10 name=node44 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm44/networkInterfaces/vmss11 selectedPoolID=s-2 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm45 name=node45 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm45 maxIPsToAllocate=10 name=node45 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm45/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm76 name=node76 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm76 maxIPsToAllocate=10 name=node76 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm76/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm77 name=node77 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm77 maxIPsToAllocate=10 name=node77 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm77/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm78 name=node78 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm78 maxIPsToAllocate=10 name=node78 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm78/networkInterfaces/vmss11 selectedPoolID=s-2 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm79 name=node79 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm79 maxIPsToAllocate=10 name=node79 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm79/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm80 name=node80 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm80 maxIPsToAllocate=10 name=node80 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm80/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm47 name=node47 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm47 maxIPsToAllocate=10 name=node47 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm47/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm48 name=node48 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm48 maxIPsToAllocate=10 name=node48 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm48/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm49 name=node49 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm49 maxIPsToAllocate=10 name=node49 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm49/networkInterfaces/vmss11 selectedPoolID=s-2 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm50 name=node50 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm50 maxIPsToAllocate=10 name=node50 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm50/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm81 name=node81 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm81 maxIPsToAllocate=10 name=node81 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm81/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm82 name=node82 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm82 maxIPsToAllocate=10 name=node82 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm82/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm51 name=node51 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm53 name=node53 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm83 name=node83 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm83 maxIPsToAllocate=10 name=node83 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm83/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm84 name=node84 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm84 maxIPsToAllocate=10 name=node84 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm84/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm85 name=node85 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm85 maxIPsToAllocate=10 name=node85 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm85/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm86 name=node86 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm86 maxIPsToAllocate=10 name=node86 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm86/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm87 name=node87 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm87 maxIPsToAllocate=10 name=node87 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm87/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm88 name=node88 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm88 maxIPsToAllocate=10 name=node88 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm88/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm43 name=node43 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm43 maxIPsToAllocate=10 name=node43 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm43/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm89 name=node89 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm89 maxIPsToAllocate=10 name=node89 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm89/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm90 name=node90 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm90 maxIPsToAllocate=10 name=node90 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm90/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm51 maxIPsToAllocate=10 name=node51 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm51/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm42 name=node42 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm42 maxIPsToAllocate=10 name=node42 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm42/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm95 name=node95 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm95 maxIPsToAllocate=10 name=node95 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm95/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm67 name=node67 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm67 maxIPsToAllocate=10 name=node67 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm67/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm52 name=node52 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm52 maxIPsToAllocate=10 name=node52 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm52/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm5 name=node5 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm5 maxIPsToAllocate=10 name=node5 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm5/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm59 name=node59 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm59 maxIPsToAllocate=10 name=node59 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm59/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=info msg="Synchronized Azure IPAM information" numInstances=100 numSubnets=3 numVirtualNetworks=1 subsys=azure
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm14 name=node14 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm14 maxIPsToAllocate=10 name=node14 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm14/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm64 name=node64 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm64 maxIPsToAllocate=10 name=node64 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm64/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm26 name=node26 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm26 maxIPsToAllocate=10 name=node26 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm26/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm27 name=node27 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm27 maxIPsToAllocate=10 name=node27 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm27/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm17 name=node17 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm17 maxIPsToAllocate=10 name=node17 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm17/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm28 name=node28 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm28 maxIPsToAllocate=10 name=node28 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm28/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm40 name=node40 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm40 maxIPsToAllocate=10 name=node40 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm40/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm57 name=node57 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm57 maxIPsToAllocate=10 name=node57 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm57/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm29 name=node29 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm29 maxIPsToAllocate=10 name=node29 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm29/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm58 name=node58 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm58 maxIPsToAllocate=10 name=node58 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm58/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm54 name=node54 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm54 maxIPsToAllocate=10 name=node54 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm54/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm73 name=node73 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm73 maxIPsToAllocate=10 name=node73 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm73/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm71 name=node71 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm71 maxIPsToAllocate=10 name=node71 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm71/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm30 name=node30 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm30 maxIPsToAllocate=10 name=node30 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm30/networkInterfaces/vmss11 selectedPoolID=s-2 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm69 name=node69 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm69 maxIPsToAllocate=10 name=node69 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm69/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm22 name=node22 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm22 maxIPsToAllocate=10 name=node22 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm22/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm46 name=node46 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm46 maxIPsToAllocate=10 name=node46 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm46/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm75 name=node75 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm75 maxIPsToAllocate=10 name=node75 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm75/networkInterfaces/vmss11 selectedPoolID=s-2 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm34 name=node34 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm34 maxIPsToAllocate=10 name=node34 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm34/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm23 name=node23 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm23 maxIPsToAllocate=10 name=node23 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm23/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm62 name=node62 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm62 maxIPsToAllocate=10 name=node62 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm62/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm19 name=node19 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm19 maxIPsToAllocate=10 name=node19 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm19/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm20 name=node20 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm20 maxIPsToAllocate=10 name=node20 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm20/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm35 name=node35 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm35 maxIPsToAllocate=10 name=node35 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm35/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm92 name=node92 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm92 maxIPsToAllocate=10 name=node92 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm92/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm63 name=node63 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm63 maxIPsToAllocate=10 name=node63 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm63/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm72 name=node72 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm72 maxIPsToAllocate=10 name=node72 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm72/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm21 name=node21 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm21 maxIPsToAllocate=10 name=node21 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm21/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm36 name=node36 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm36 maxIPsToAllocate=10 name=node36 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm36/networkInterfaces/vmss11 selectedPoolID=s-2 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm93 name=node93 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm93 maxIPsToAllocate=10 name=node93 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm93/networkInterfaces/vmss11 selectedPoolID=s-2 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm37 name=node37 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm37 maxIPsToAllocate=10 name=node37 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm37/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm38 name=node38 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm38 maxIPsToAllocate=10 name=node38 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm38/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm94 name=node94 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm94 maxIPsToAllocate=10 name=node94 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm94/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm24 name=node24 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm24 maxIPsToAllocate=10 name=node24 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm24/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm65 name=node65 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm65 maxIPsToAllocate=10 name=node65 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm65/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm39 name=node39 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm39 maxIPsToAllocate=10 name=node39 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm39/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm25 name=node25 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm25 maxIPsToAllocate=10 name=node25 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm25/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm70 name=node70 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm70 maxIPsToAllocate=10 name=node70 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm70/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm55 name=node55 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm55 maxIPsToAllocate=10 name=node55 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm55/networkInterfaces/vmss11 selectedPoolID=s-2 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm4 name=node4 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm4 maxIPsToAllocate=10 name=node4 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm4/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm68 name=node68 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm68 maxIPsToAllocate=10 name=node68 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm68/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm66 name=node66 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm66 maxIPsToAllocate=10 name=node66 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm66/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm96 name=node96 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm96 maxIPsToAllocate=10 name=node96 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm96/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm9 name=node9 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm9 maxIPsToAllocate=10 name=node9 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm9/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm97 name=node97 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm97 maxIPsToAllocate=10 name=node97 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm97/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm8 name=node8 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm8 maxIPsToAllocate=10 name=node8 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm8/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm41 name=node41 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm41 maxIPsToAllocate=10 name=node41 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm41/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm13 name=node13 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm13 maxIPsToAllocate=10 name=node13 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm13/networkInterfaces/vmss11 selectedPoolID=s-2 subsys=ipam used=0
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm53 maxIPsToAllocate=10 name=node53 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm53/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm18 name=node18 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm18 maxIPsToAllocate=10 name=node18 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm18/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm74 name=node74 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm74 maxIPsToAllocate=10 name=node74 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm74/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm56 name=node56 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm56 maxIPsToAllocate=10 name=node56 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm56/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm31 name=node31 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm31 maxIPsToAllocate=10 name=node31 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm31/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm15 name=node15 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm15 maxIPsToAllocate=10 name=node15 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm15/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm16 name=node16 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm16 maxIPsToAllocate=10 name=node16 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm16/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm32 name=node32 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm32 maxIPsToAllocate=10 name=node32 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm32/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm91 name=node91 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm91 maxIPsToAllocate=10 name=node91 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm91/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm60 name=node60 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm60 maxIPsToAllocate=10 name=node60 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm60/networkInterfaces/vmss11 selectedPoolID=s-3 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm33 name=node33 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm33 maxIPsToAllocate=10 name=node33 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm33/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm61 name=node61 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=256 emptyInterfaceSlots=0 instanceID=vm61 maxIPsToAllocate=10 name=node61 neededIPs=10 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm61/networkInterfaces/vmss11 selectedPoolID=s-1 subsys=ipam used=0
level=info msg="Synchronized Azure IPAM information" numInstances=100 numSubnets=3 numVirtualNetworks=1 subsys=azure
level=info msg="Synchronized Azure IPAM information" numInstances=100 numSubnets=3 numVirtualNetworks=1 subsys=azure
level=info msg="Synchronized Azure IPAM information" numInstances=100 numSubnets=3 numVirtualNetworks=1 subsys=azure]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestIpamMinAllocate10" classname="github.com/cilium/cilium/pkg/azure/ipam" time="0.020">
			<system-out><![CDATA[level=info msg="Synchronized Azure IPAM information" numInstances=1 numSubnets=1 numVirtualNetworks=1 subsys=azure
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm1 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=1 availableForAllocation=255 emptyInterfaceSlots=0 instanceID=vm1 maxIPsToAllocate=9 name=node1 neededIPs=9 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm1/networkInterfaces/vmss11 selectedPoolID=subnet-1 subsys=ipam used=0
level=info msg="Synchronized Azure IPAM information" numInstances=1 numSubnets=1 numVirtualNetworks=1 subsys=azure
level=info msg="Synchronized Azure IPAM information" numInstances=100 numSubnets=3 numVirtualNetworks=1 subsys=azure
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm1 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=10 availableForAllocation=246 emptyInterfaceSlots=0 instanceID=vm1 maxIPsToAllocate=5 name=node1 neededIPs=5 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm1/networkInterfaces/vmss11 selectedPoolID=subnet-1 subsys=ipam used=7
level=info msg="Synchronized Azure IPAM information" numInstances=1 numSubnets=1 numVirtualNetworks=1 subsys=azure]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestIpamPreAllocate8" classname="github.com/cilium/cilium/pkg/azure/ipam" time="0.020">
			<system-out><![CDATA[level=info msg="Synchronized Azure IPAM information" numInstances=1 numSubnets=1 numVirtualNetworks=1 subsys=azure
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm1 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=1 availableForAllocation=255 emptyInterfaceSlots=0 instanceID=vm1 maxIPsToAllocate=7 name=node1 neededIPs=7 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm1/networkInterfaces/vmss11 selectedPoolID=subnet-1 subsys=ipam used=0
level=info msg="Synchronized Azure IPAM information" numInstances=1 numSubnets=1 numVirtualNetworks=1 subsys=azure
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID=vm1 name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=8 availableForAllocation=248 emptyInterfaceSlots=0 instanceID=vm1 maxIPsToAllocate=7 name=node1 neededIPs=7 remainingInterfaces=1 selectedInterface=/subscriptions/xxx/resourceGroups/g1/providers/Microsoft.Compute/virtualMachineScaleSets/vmss11/virtualMachines/vm1/networkInterfaces/vmss11 selectedPoolID=subnet-1 subsys=ipam used=7
level=info msg="Synchronized Azure IPAM information" numInstances=1 numSubnets=1 numVirtualNetworks=1 subsys=azure]]></system-out>
		</testcase>
		<testcase name="TestENIIPAMCapacityAccounting" classname="github.com/cilium/cilium/pkg/azure/ipam" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/azure/ipam	coverage: 76.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/azure/types" tests="4" failures="0" errors="0" id="113" hostname="kind-bpf-next" time="0.010" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/azure/types" time="0.000"></testcase>
		<testcase name="Test/TypesSuite" classname="github.com/cilium/cilium/pkg/azure/types" time="0.000"></testcase>
		<testcase name="Test/TypesSuite/TestExtractIDs" classname="github.com/cilium/cilium/pkg/azure/types" time="0.000"></testcase>
		<testcase name="Test/TypesSuite/TestForeachAddresses" classname="github.com/cilium/cilium/pkg/azure/types" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/azure/types	coverage: 18.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bandwidth" tests="0" failures="0" errors="0" id="114" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/backoff" tests="6" failures="0" errors="0" id="115" hostname="kind-bpf-next" time="0.013" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/backoff" time="0.010"></testcase>
		<testcase name="Test/BackoffSuite" classname="github.com/cilium/cilium/pkg/backoff" time="0.010"></testcase>
		<testcase name="Test/BackoffSuite/TestClusterSizeDependantInterval" classname="github.com/cilium/cilium/pkg/backoff" time="0.010">
			<system-out><![CDATA[nodes      4        16       128       512      1024      2048
1:        2s      3.1s      6.5s      8.7s     11.2s     12.2s
2:      4.4s      7.9s     11.5s     23.3s     26.8s     17.6s
3:      6.4s      4.2s     28.9s     25.2s     47.5s       47s
4:     11.3s     21.5s     16.1s     37.9s     36.7s     14.2s
5:      3.1s   1m27.8s      2m0s      2m0s   1m25.7s      2m0s
6:       42s     19.1s      2m0s      2m0s      2m0s      2m0s
7:      2m0s     34.4s      2m0s      2m0s      2m0s      2m0s
8:   1m33.3s      2m0s      2m0s      2m0s      2m0s      2m0s]]></system-out>
		</testcase>
		<testcase name="Test/BackoffSuite/TestJitter" classname="github.com/cilium/cilium/pkg/backoff" time="0.000"></testcase>
		<testcase name="Test/BackoffSuite/TestJitterDistribution" classname="github.com/cilium/cilium/pkg/backoff" time="0.000">
			<system-out><![CDATA[1: 2s
2: 4s
3: 8s
4: 16s
5: 32s
6: 1m4s
7: 2m8s
8: 4m16s]]></system-out>
		</testcase>
		<testcase name="Test/BackoffSuite/TestNewNodeManager" classname="github.com/cilium/cilium/pkg/backoff" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/backoff	coverage: 61.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgp/k8s" tests="0" failures="0" errors="0" id="116" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgp/log" tests="0" failures="0" errors="0" id="117" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgp/config" tests="3" failures="0" errors="0" id="118" hostname="kind-bpf-next" time="0.008" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/bgp/config" time="0.000"></testcase>
		<testcase name="Test/BGPConfigTestSuite" classname="github.com/cilium/cilium/pkg/bgp/config" time="0.000"></testcase>
		<testcase name="Test/BGPConfigTestSuite/TestParse" classname="github.com/cilium/cilium/pkg/bgp/config" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/bgp/config	coverage: 85.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgp/fence" tests="4" failures="0" errors="0" id="119" hostname="kind-bpf-next" time="0.005" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestFence" classname="github.com/cilium/cilium/pkg/bgp/fence" time="0.000"></testcase>
		<testcase name="TestFence/No_fence" classname="github.com/cilium/cilium/pkg/bgp/fence" time="0.000"></testcase>
		<testcase name="TestFence/No_fence_-_replay_same_rev" classname="github.com/cilium/cilium/pkg/bgp/fence" time="0.000"></testcase>
		<testcase name="TestFence/Fence_-_stale_rev" classname="github.com/cilium/cilium/pkg/bgp/fence" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/bgp/fence	coverage: 53.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgp/mock" tests="0" failures="0" errors="0" id="120" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgp/manager" tests="2" failures="0" errors="0" id="121" hostname="kind-bpf-next" time="0.052" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestManagerEventNoService" classname="github.com/cilium/cilium/pkg/bgp/manager" time="0.000"></testcase>
		<testcase name="TestManagerEvent" classname="github.com/cilium/cilium/pkg/bgp/manager" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/bgp/manager	coverage: 32.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgpv1" tests="0" failures="0" errors="0" id="122" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgp/speaker" tests="6" failures="0" errors="0" id="123" hostname="kind-bpf-next" time="0.080" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestSpeakerOnUpdateService" classname="github.com/cilium/cilium/pkg/bgp/speaker" time="0.000">
			<system-out><![CDATA[level=info msg="processing new event." component=MetalLBSpeaker.run subsys=bgp-speaker]]></system-out>
		</testcase>
		<testcase name="TestSpeakerOnDeleteService" classname="github.com/cilium/cilium/pkg/bgp/speaker" time="0.000">
			<system-out><![CDATA[level=info msg="processing new event." component=MetalLBSpeaker.run subsys=bgp-speaker]]></system-out>
		</testcase>
		<testcase name="TestSpeakerOnUpdateEndpoints" classname="github.com/cilium/cilium/pkg/bgp/speaker" time="0.000">
			<system-out><![CDATA[level=info msg="processing new event." component=MetalLBSpeaker.run subsys=bgp-speaker]]></system-out>
		</testcase>
		<testcase name="TestSpeakerOnUpdateNode" classname="github.com/cilium/cilium/pkg/bgp/speaker" time="0.010">
			<system-out><![CDATA[level=info msg="processing new event." component=MetalLBSpeaker.run subsys=bgp-speaker
level=info msg="Advertising CIDRs to all available session" advertisements="[1.1.0.0/16]" component=MetalLBSpeaker.announcePodCidrs subsys=bgp-speaker]]></system-out>
		</testcase>
		<testcase name="TestSpeakerOnDeleteNode" classname="github.com/cilium/cilium/pkg/bgp/speaker" time="0.000">
			<system-out><![CDATA[level=info msg="processing new event." component=MetalLBSpeaker.run subsys=bgp-speaker
level=info msg="node is leaving the cluster, speaker will shutdown." cidrs="&[1.1.0.0/16]" component=MetalLBSpeaker.handleNodeEvent labels="&map[TestLabel:TestLabel]" subsys=bgp-speaker
level=info msg="speaker shutting down." component=MetalLBSpeaker.run subsys=bgp-speaker]]></system-out>
		</testcase>
		<testcase name="TestSpeakerOnUpdateAndDeleteCiliumNode" classname="github.com/cilium/cilium/pkg/bgp/speaker" time="0.000">
			<system-out><![CDATA[level=info msg="processing new event." component=MetalLBSpeaker.run subsys=bgp-speaker
level=info msg="Advertising CIDRs to all available session" advertisements="[10.0.0.0/16]" component=MetalLBSpeaker.announcePodCidrs subsys=bgp-speaker
level=info msg="processing new event." component=MetalLBSpeaker.run subsys=bgp-speaker
level=info msg="Advertising CIDRs to all available session" advertisements="[10.0.0.0/16 10.1.0.0/16 10.2.0.0/16]" component=MetalLBSpeaker.announcePodCidrs subsys=bgp-speaker
level=info msg="processing new event." component=MetalLBSpeaker.run subsys=bgp-speaker
level=info msg="node is leaving the cluster, speaker will shutdown." cidrs="&[10.0.0.0/16 10.1.0.0/16 10.2.0.0/16]" component=MetalLBSpeaker.handleNodeEvent labels="&map[TestLabel:TestLabel]" subsys=bgp-speaker
level=info msg="speaker shutting down." component=MetalLBSpeaker.run subsys=bgp-speaker]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/bgp/speaker	coverage: 57.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgpv1/agent" tests="17" failures="0" errors="0" id="124" hostname="kind-bpf-next" time="0.045" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestAnnotation" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestAnnotation/Test_parsing_of_router-id" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestControllerSanity" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestControllerSanity/successful_reconcile" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestControllerSanity/policy_defaulting_on_successful_reconcile" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestControllerSanity/podcidr_listing_error" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestControllerSanity/annotations_listing_error" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestControllerSanity/label_listening_error" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestControllerSanity/configure_peers_error" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestControllerSanity/connect_retry_time_validation_error" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestControllerSanity/hold_time_validation_error" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestPolicySelection" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestPolicySelection/no_policies" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestPolicySelection/policy_match" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestPolicySelection/policy_no_match" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestPolicySelection/multi_policy_match,_no_conflict" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<testcase name="TestPolicySelection/multi_policy_match,_conflict" classname="github.com/cilium/cilium/pkg/bgpv1/agent" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/bgpv1/agent	coverage: 35.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgpv1/gobgp" tests="5" failures="0" errors="0" id="125" hostname="kind-bpf-next" time="0.031" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestGetPeerState" classname="github.com/cilium/cilium/pkg/bgpv1/gobgp" time="0.010"></testcase>
		<testcase name="TestGetPeerState/test_add_neighbor" classname="github.com/cilium/cilium/pkg/bgpv1/gobgp" time="0.000">
			<system-out><![CDATA[level=info msg="Add a peer configuration" Key=192.168.0.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64124 neighbor_address:\"192.168.0.1\" peer_asn:64125} state:{local_asn:64124 neighbor_address:\"192.168.0.1\" peer_asn:64125 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64124 neighbor_address:\"192.168.0.1\" peer_asn:64125} state:{local_asn:64124 neighbor_address:\"192.168.0.1\" peer_asn:64125 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="Delete a peer configuration" Key=192.168.0.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestGetPeerState/test_add_+_update_neighbors" classname="github.com/cilium/cilium/pkg/bgpv1/gobgp" time="0.000">
			<system-out><![CDATA[level=info msg="Add a peer configuration" Key=192.168.0.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Add a peer configuration" Key=192.168.66.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64124 neighbor_address:\"192.168.0.1\" peer_asn:64125} state:{local_asn:64124 neighbor_address:\"192.168.0.1\" peer_asn:64125 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64124 neighbor_address:\"192.168.66.1\" peer_asn:64126} state:{local_asn:64124 neighbor_address:\"192.168.66.1\" peer_asn:64126 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64124 neighbor_address:\"192.168.66.1\" peer_asn:64126} state:{local_asn:64124 neighbor_address:\"192.168.66.1\" peer_asn:64126 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="Add a peer configuration" Key=192.168.77.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Update timer configuration" Err="<nil>" Key=192.168.0.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Update timer configuration" Err="<nil>" Key=192.168.66.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Update timer configuration" Err="<nil>" Key=192.168.77.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.66.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.77.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64124 neighbor_address:\"192.168.77.1\" peer_asn:64127} state:{local_asn:64124 neighbor_address:\"192.168.77.1\" peer_asn:64127 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64124 neighbor_address:\"192.168.0.1\" peer_asn:64125} state:{local_asn:64124 neighbor_address:\"192.168.0.1\" peer_asn:64125 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64124 neighbor_address:\"192.168.77.1\" peer_asn:64127} state:{local_asn:64124 neighbor_address:\"192.168.77.1\" peer_asn:64127 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"]]></system-out>
		</testcase>
		<testcase name="TestGetPeerState/test_add_invalid_neighbor" classname="github.com/cilium/cilium/pkg/bgpv1/gobgp" time="0.000"></testcase>
		<testcase name="TestGetPeerState/test_invalid_neighbor_update" classname="github.com/cilium/cilium/pkg/bgpv1/gobgp" time="0.000">
			<system-out><![CDATA[level=info msg="Add a peer configuration" Key=192.168.0.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.1 Topic=Peer asn=64124 component=gobgp.BgpServerInstance subsys=bgp-control-plane]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/bgpv1/gobgp	coverage: 53.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgpv1/mock" tests="0" failures="0" errors="0" id="126" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgpv1/types" tests="0" failures="0" errors="0" id="127" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/cgroups" tests="0" failures="0" errors="0" id="128" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgpv1/manager" tests="31" failures="0" errors="0" id="129" hostname="kind-bpf-next" time="3.272" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestDiffSignal" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.100">
			<system-out><![CDATA[level=info msg=Invoked duration="50.875µs" function="manager.newDiffStoreFixture.func3 (diffstore_test.go:54)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="3.467µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.226135ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="7.594µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="39.684µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestDiffUpsertCoalesce" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="3.100">
			<system-out><![CDATA[level=info msg=Invoked duration="29.615µs" function="manager.newDiffStoreFixture.func3 (diffstore_test.go:54)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=872ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.398679ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="27.601µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="28.874µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestPreflightReconciler" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000"></testcase>
		<testcase name="TestPreflightReconciler/no_change" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000"></testcase>
		<testcase name="TestPreflightReconciler/router-id_change" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="Virtual router with ASN 64125 router ID has changed from 192.168.0.1 to 192.168.0.2" component=manager.preflightReconciler subsys=bgp-control-plane
level=info msg="Recreating virtual router with ASN 64125 for changes to take effect" component=manager.preflightReconciler subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestPreflightReconciler/local-port_change" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="Virtual router with ASN 64125 local port has changed from 45450 to 45451" component=manager.preflightReconciler subsys=bgp-control-plane
level=info msg="Recreating virtual router with ASN 64125 for changes to take effect" component=manager.preflightReconciler subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestPreflightReconciler/local-port,_router-id_change" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="Virtual router with ASN 64125 local port has changed from 45450 to 45451" component=manager.preflightReconciler subsys=bgp-control-plane
level=info msg="Virtual router with ASN 64125 router ID has changed from 192.168.0.1 to 192.168.0.2" component=manager.preflightReconciler subsys=bgp-control-plane
level=info msg="Recreating virtual router with ASN 64125 for changes to take effect" component=manager.preflightReconciler subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestNeighborReconciler" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.010"></testcase>
		<testcase name="TestNeighborReconciler/no_change" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="Add a peer configuration" Key=192.168.0.1 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Add a peer configuration" Key=192.168.0.2 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.1 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.2 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"]]></system-out>
		</testcase>
		<testcase name="TestNeighborReconciler/additional_neighbor" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="Add a peer configuration" Key=192.168.0.1 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Add a peer configuration" Key=192.168.0.2 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Reconciling peers for virtual router with local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Adding peer 192.168.0.3/32 64124 to local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Add a peer configuration" Key=192.168.0.3 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="Done reconciling peers for virtual router with local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.1 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.2 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.3 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.3\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.3\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"]]></system-out>
		</testcase>
		<testcase name="TestNeighborReconciler/remove_neighbor" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="Add a peer configuration" Key=192.168.0.1 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Add a peer configuration" Key=192.168.0.2 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Add a peer configuration" Key=192.168.0.3 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.3\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.3\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.3\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.3\" peer_asn:64124 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="Reconciling peers for virtual router with local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Removing peer 192.168.0.3/32 64124 from local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.3 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.2 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.1 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestNeighborReconciler/update_neighbor" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="Add a peer configuration" Key=192.168.0.1 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Add a peer configuration" Key=192.168.0.2 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="Add a peer configuration" Key=192.168.0.3 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Reconciling peers for virtual router with local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Updating peer 192.168.0.1/32 64124 in local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Update timer configuration" Err="<nil>" Key=192.168.0.1 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.3 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.1 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.2 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.3\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.3\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"]]></system-out>
		</testcase>
		<testcase name="TestNeighborReconciler/remove_all_neighbors" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="Add a peer configuration" Key=192.168.0.1 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="Add a peer configuration" Key=192.168.0.2 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="Add a peer configuration" Key=192.168.0.3 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.1\" peer_asn:64124 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="Reconciling peers for virtual router with local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Removing peer 192.168.0.1/32 64124 from local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.1 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.3\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.3\" peer_asn:64124 session_state:IDLE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.2\" peer_asn:64124 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE peer:{conf:{local_asn:64125 neighbor_address:\"192.168.0.3\" peer_asn:64124} state:{local_asn:64125 neighbor_address:\"192.168.0.3\" peer_asn:64124 session_state:ACTIVE router_id:\"<nil>\"} transport:{local_address:\"<nil>\"}}"
level=info msg="Removing peer 192.168.0.2/32 64124 from local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.2 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Removing peer 192.168.0.3/32 64124 from local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=192.168.0.3 Topic=Peer asn=64125 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 64125" component=manager.neighborReconciler subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestExportPodCIDRReconciler" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000"></testcase>
		<testcase name="TestExportPodCIDRReconciler/disable" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="[] []" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestExportPodCIDRReconciler/enable" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="[{Prefix:192.168.0.0/24 GoBGPPathUUID:[140 38 117 82 193 212 64 194 171 13 34 43 145 148 39 6]}] [192.168.0.0/24]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestExportPodCIDRReconciler/no_change" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="[{Prefix:192.168.0.0/24 GoBGPPathUUID:[158 212 59 115 49 252 67 233 176 250 248 208 188 96 33 17]}] [192.168.0.0/24]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestExportPodCIDRReconciler/additional_network" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="[{Prefix:192.168.0.0/24 GoBGPPathUUID:[]} {Prefix:192.168.1.0/24 GoBGPPathUUID:[29 34 185 252 90 211 70 144 160 46 138 42 153 230 253 145]}] [192.168.0.0/24 192.168.1.0/24]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestExportPodCIDRReconciler/removal_of_both_networks" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="[] []" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestLBServiceReconciler" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000"></testcase>
		<testcase name="TestLBServiceReconciler/lb-svc-1-ingress" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="map[default/svc-1:[{Prefix:1.2.3.4/32 GoBGPPathUUID:[175 202 97 82 111 212 79 30 163 149 194 225 5 9 39 244]}]] map[default/svc-1:[1.2.3.4/32]]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestLBServiceReconciler/lb-svc-2-ingress" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="map[default/svc-1:[{Prefix:1.2.3.4/32 GoBGPPathUUID:[225 59 40 2 223 164 75 241 140 63 198 93 203 112 111 151]} {Prefix:ff::203:405/128 GoBGPPathUUID:[61 106 73 253 75 2 77 122 144 107 54 118 85 168 129 203]}]] map[default/svc-1:[1.2.3.4/32 ff::2.3.4.5/128]]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestLBServiceReconciler/delete-svc" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="map[] map[]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestLBServiceReconciler/update-service-no-match" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="map[default/svc-1:[]] map[]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestLBServiceReconciler/update-vrouter-selector" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="map[default/svc-1:[]] map[]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestLBServiceReconciler/update-1-to-2-ingress" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="map[default/svc-1:[{Prefix:1.2.3.4/32 GoBGPPathUUID:[245 232 99 0 224 201 73 137 168 51 223 51 121 117 208 180]} {Prefix:2.3.4.5/32 GoBGPPathUUID:[216 111 136 70 196 11 70 10 130 216 120 156 200 135 21 31]}]] map[default/svc-1:[1.2.3.4/32 2.3.4.5/32]]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestLBServiceReconciler/no-selector" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="map[] map[]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestLBServiceReconciler/svc-namespace-selector" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="map[default/svc-1:[{Prefix:1.2.3.4/32 GoBGPPathUUID:[21 162 90 244 132 129 71 89 157 167 125 171 229 155 88 170]}]] map[default/svc-1:[1.2.3.4/32]]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestLBServiceReconciler/svc-name-selector" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="map[default/svc-1:[{Prefix:1.2.3.4/32 GoBGPPathUUID:[65 148 71 17 81 214 77 194 134 81 157 63 194 3 122 109]}] non-default/svc-1:[{Prefix:2.3.4.5/32 GoBGPPathUUID:[65 218 13 133 212 189 69 55 132 242 51 132 196 180 108 191]}]] map[default/svc-1:[1.2.3.4/32] non-default/svc-1:[2.3.4.5/32]]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestLBServiceReconciler/non-lb_svc" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="map[] map[]" subsys=bgp-control-plane]]></system-out>
		</testcase>
		<testcase name="TestReconcileAfterServerReinit" classname="github.com/cilium/cilium/pkg/bgpv1/manager" time="0.000">
			<system-out><![CDATA[level=info msg="Virtual router with ASN 64125 local port has changed from -1 to 45450" component=manager.preflightReconciler subsys=bgp-control-plane
level=info msg="Virtual router with ASN 64125 router ID has changed from 127.0.0.1 to 192.168.0.2" component=manager.preflightReconciler subsys=bgp-control-plane
level=info msg="Recreating virtual router with ASN 64125 for changes to take effect" component=manager.preflightReconciler subsys=bgp-control-plane]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/bgpv1/manager	coverage: 58.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/components" tests="0" failures="0" errors="0" id="130" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/crypto/certificatemanager" tests="0" failures="0" errors="0" id="131" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/connector" tests="0" failures="0" errors="0" id="132" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/ipcache" tests="0" failures="0" errors="0" id="133" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/linux/bigtcp" tests="0" failures="0" errors="0" id="134" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/linux/linux_defaults" tests="0" failures="0" errors="0" id="135" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/loader/metrics" tests="0" failures="0" errors="0" id="136" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/option" tests="0" failures="0" errors="0" id="137" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/prefilter" tests="0" failures="0" errors="0" id="138" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/types" tests="0" failures="0" errors="0" id="139" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/defaults" tests="0" failures="0" errors="0" id="140" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ebpf" tests="0" failures="0" errors="0" id="141" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bgpv1/test" tests="17" failures="0" errors="0" id="142" hostname="kind-bpf-next" time="23.222" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test_PodCIDRAdvert" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="8.070">
			<system-out><![CDATA[time="2023-05-31T11:41:55Z" level=info msg="deleting dummy links"
time="2023-05-31T11:41:55Z" level=info msg="adding dummy links"
time="2023-05-31T11:41:55Z" level=info msg="GoBGP test instance: starting"
time="2023-05-31T11:41:55Z" level=info msg="Add a peer configuration" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:41:55Z" level=debug msg="IdleHoldTimer expired" Duration=0 Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:41:55Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_ACTIVE old=BGP_FSM_IDLE reason=idle-hold-timer-expired subsys=basic
level=info msg=Invoked duration="174.855µs" function="test.newFixture.func6 (fixtures.go:155)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="4.138µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.867493ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.956µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumBGPPeeringPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="3.196µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration=761ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="61.003µs" function="*agent.kubernetesNodeSpecer.Start" subsys=hive
level=info msg="Start hook executed" duration=201.059718ms function="*agent.Controller.Start" subsys=hive
time="2023-05-31T11:41:55Z" level=info msg="GoBGP test instance: Peer Event: {0 IDLE}"
level=info msg="Cilium BGP Control Plane Controller now running..." component=Controller.Run subsys=bgp-control-plane
level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
time="2023-05-31T11:41:55Z" level=info msg="GoBGP test instance: Peer Event: {65001 IDLE}"
time="2023-05-31T11:41:55Z" level=info msg="GoBGP test instance: Peer Event: {65001 ACTIVE}"
level=info msg="Registering BGP servers for policy with local ASN 65001" component=manager.registerBGPServer subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Successfully registered GoBGP servers for policy with local ASN 65001" component=manager.registerBGPServer subsys=bgp-control-plane
level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Adding peer 172.16.100.2/32 65011 to local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Add a peer configuration" Key=172.16.100.2 Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:IDLE  router_id:\"<nil>\"}  transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:ACTIVE  router_id:\"<nil>\"}  transport:{local_address:\"<nil>\"}}"
time="2023-05-31T11:42:03Z" level=debug msg="try to connect" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_OPENSENT old=BGP_FSM_ACTIVE reason=new-connection subsys=basic
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Peer Event: {65001 OPENSENT}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:OPENSENT  router_id:\"<nil>\"}  transport:{local_address:\"172.16.100.1\"  local_port:1790  remote_port:38053}}"
time="2023-05-31T11:42:03Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_OPENCONFIRM old=BGP_FSM_OPENSENT reason=open-msg-received subsys=basic
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Peer Event: {65001 OPENCONFIRM}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:OPENCONFIRM  router_id:\"172.16.100.2\"}  transport:{local_address:\"172.16.100.1\"  local_port:1790  remote_port:38053}}"
time="2023-05-31T11:42:03Z" level=info msg="Peer Up" Key=172.16.100.1 State=BGP_FSM_OPENCONFIRM Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_ESTABLISHED old=BGP_FSM_OPENCONFIRM reason=open-msg-negotiated subsys=basic
level=info msg="Peer Up" Key=172.16.100.2 State=BGP_FSM_OPENCONFIRM Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Peer Event: {65001 ESTABLISHED}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:ESTABLISHED  router_id:\"172.16.100.2\"}  transport:{local_address:\"172.16.100.1\"  local_port:1790  remote_port:38053}}"]]></system-out>
		</testcase>
		<testcase name="Test_PodCIDRAdvert/advertise_pod_CIDRs" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="0.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
time="2023-05-31T11:42:03Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{Origin: i} 65001 {Nexthop: 172.16.100.1}]" component=tests.BGP nlri="[10.1.0.0/16]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:03Z" level=debug msg="create Destination" Nlri=10.1.0.0/16 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="create Destination" Nlri=10.1.0.0/16 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="From me, ignore" Data="{ 10.1.0.0/16 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1 }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{Origin: i} 65001 {MpReach(ipv6-unicast): {Nexthop: 172.16.100.1, NLRIs: [aaaa::/64]}}]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:03Z" level=debug msg="create Destination" Nlri="aaaa::/64" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="create Destination" Nlri="aaaa::/64" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="From me, ignore" Data="{ aaaa::/64 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1 }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Route Event: {65001 10.1.0.0 16 false}"
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Route Event: {65001 aaaa:: 64 false}"]]></system-out>
		</testcase>
		<testcase name="Test_PodCIDRAdvert/delete_pod_CIDRs" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="0.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
time="2023-05-31T11:42:03Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{MpUnreach(ipv6-unicast): {NLRIs: [aaaa::/64]}}]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:03Z" level=debug msg="Removing withdrawals" Key="aaaa::/64" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="From me, ignore" Data="{ aaaa::/64 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1, withdraw }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[10.1.0.0/16]"
time="2023-05-31T11:42:03Z" level=debug msg="Removing withdrawals" Key=10.1.0.0/16 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="From me, ignore" Data="{ 10.1.0.0/16 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1, withdraw }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Route Event: {65001 aaaa:: 64 true}"
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Route Event: {65001 10.1.0.0 16 true}"]]></system-out>
		</testcase>
		<testcase name="Test_PodCIDRAdvert/re-add_pod_CIDRs" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="0.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
time="2023-05-31T11:42:03Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{Origin: i} 65001 {Nexthop: 172.16.100.1}]" component=tests.BGP nlri="[10.1.0.0/16]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:03Z" level=debug msg="create Destination" Nlri=10.1.0.0/16 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="create Destination" Nlri=10.1.0.0/16 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="From me, ignore" Data="{ 10.1.0.0/16 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1 }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{Origin: i} 65001 {MpReach(ipv6-unicast): {Nexthop: 172.16.100.1, NLRIs: [aaaa::/64]}}]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:03Z" level=debug msg="create Destination" Nlri="aaaa::/64" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="create Destination" Nlri="aaaa::/64" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="From me, ignore" Data="{ aaaa::/64 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1 }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Route Event: {65001 10.1.0.0 16 false}"
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Route Event: {65001 aaaa:: 64 false}"]]></system-out>
		</testcase>
		<testcase name="Test_PodCIDRAdvert/update_pod_CIDRs" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="0.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
time="2023-05-31T11:42:03Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{Origin: i} 65001 {Nexthop: 172.16.100.1}]" component=tests.BGP nlri="[10.2.0.0/16]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:03Z" level=debug msg="create Destination" Nlri=10.2.0.0/16 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="create Destination" Nlri=10.2.0.0/16 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="From me, ignore" Data="{ 10.2.0.0/16 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1 }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{Origin: i} 65001 {MpReach(ipv6-unicast): {Nexthop: 172.16.100.1, NLRIs: [bbbb::/64]}}]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:03Z" level=debug msg="create Destination" Nlri="bbbb::/64" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="create Destination" Nlri="bbbb::/64" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="From me, ignore" Data="{ bbbb::/64 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1 }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[10.1.0.0/16]"
time="2023-05-31T11:42:03Z" level=debug msg="Removing withdrawals" Key=10.1.0.0/16 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Route Event: {65001 10.2.0.0 16 false}"
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Route Event: {65001 bbbb:: 64 false}"
time="2023-05-31T11:42:03Z" level=debug msg="From me, ignore" Data="{ 10.1.0.0/16 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1, withdraw }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{MpUnreach(ipv6-unicast): {NLRIs: [aaaa::/64]}}]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:03Z" level=debug msg="Removing withdrawals" Key="aaaa::/64" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="From me, ignore" Data="{ aaaa::/64 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1, withdraw }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Route Event: {65001 10.1.0.0 16 true}"
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Route Event: {65001 aaaa:: 64 true}"
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: stopping"
time="2023-05-31T11:42:03Z" level=info msg="Delete a peer configuration" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="Removing withdrawals" Key=10.2.0.0/16 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="Removing withdrawals" Key="bbbb::/64" Topic=Table asn=65011 component=tests.BGP subsys=basic
level=warning msg="received notification" Code=6 Data="[]" Key=172.16.100.2 Subcode=3 Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
time="2023-05-31T11:42:03Z" level=info msg="Peer Down" Key=172.16.100.1 Reason=dying State=BGP_FSM_ESTABLISHED Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="freed fsm.h" Key=172.16.100.1 State=BGP_FSM_ESTABLISHED Topic=Peer asn=65011 component=tests.BGP subsys=basic
level=info msg="Peer Down" Key=172.16.100.2 Reason="notification-received code 6(cease) subcode 3(peer deconfigured)" State=BGP_FSM_ESTABLISHED Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg=Stopping subsys=hive
level=info msg="Delete a peer configuration" Key=172.16.100.2 Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Removed BGP server with local ASN 65001" component=manager.remove subsys=bgp-control-plane
level=info msg="Cilium BGP Control Plane Controller shut down" component=Controller.Run subsys=bgp-control-plane
level=info msg="Stop hook executed" duration="98.483µs" function="*agent.Controller.Stop" subsys=hive
level=info msg="Stop hook executed" duration="10.189µs" function="*agent.kubernetesNodeSpecer.Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.265µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="41.016µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="10.559µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumBGPPeeringPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="10.079µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="8.877µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
time="2023-05-31T11:42:03Z" level=info msg="deleting dummy links"]]></system-out>
		</testcase>
		<testcase name="Test_LBEgressAdvertisement" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="5.050">
			<system-out><![CDATA[time="2023-05-31T11:42:03Z" level=info msg="deleting dummy links"
time="2023-05-31T11:42:03Z" level=info msg="adding dummy links"
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: starting"
time="2023-05-31T11:42:03Z" level=info msg="Add a peer configuration" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="IdleHoldTimer expired" Duration=0 Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:03Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_ACTIVE old=BGP_FSM_IDLE reason=idle-hold-timer-expired subsys=basic
level=info msg=Invoked duration="121.786µs" function="test.newFixture.func6 (fixtures.go:155)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="2.666µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=200.905648ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="2.936µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumBGPPeeringPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.112µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="2.114µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="25.655µs" function="*agent.kubernetesNodeSpecer.Start" subsys=hive
level=info msg="Start hook executed" duration=100.59522ms function="*agent.Controller.Start" subsys=hive
level=info msg="Cilium BGP Control Plane Controller now running..." component=Controller.Run subsys=bgp-control-plane
level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Peer Event: {0 IDLE}"
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Peer Event: {65001 IDLE}"
time="2023-05-31T11:42:03Z" level=info msg="GoBGP test instance: Peer Event: {65001 ACTIVE}"
level=info msg="Registering BGP servers for policy with local ASN 65001" component=manager.registerBGPServer subsys=bgp-control-plane
level=info msg="Reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Adding peer 172.16.100.2/32 65011 to local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Add a peer configuration" Key=172.16.100.2 Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Successfully registered GoBGP servers for policy with local ASN 65001" component=manager.registerBGPServer subsys=bgp-control-plane
level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:IDLE  router_id:\"<nil>\"}  transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:ACTIVE  router_id:\"<nil>\"}  transport:{local_address:\"<nil>\"}}"
time="2023-05-31T11:42:08Z" level=debug msg="try to connect" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_OPENSENT old=BGP_FSM_ACTIVE reason=new-connection subsys=basic
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Peer Event: {65001 OPENSENT}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:OPENSENT  router_id:\"<nil>\"}  transport:{local_address:\"172.16.100.1\"  local_port:1790  remote_port:45279}}"
time="2023-05-31T11:42:08Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_OPENCONFIRM old=BGP_FSM_OPENSENT reason=open-msg-received subsys=basic
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:OPENCONFIRM  router_id:\"172.16.100.2\"}  transport:{local_address:\"172.16.100.1\"  local_port:1790  remote_port:45279}}"
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Peer Event: {65001 OPENCONFIRM}"
time="2023-05-31T11:42:08Z" level=info msg="Peer Up" Key=172.16.100.1 State=BGP_FSM_OPENCONFIRM Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_ESTABLISHED old=BGP_FSM_OPENCONFIRM reason=open-msg-negotiated subsys=basic
level=info msg="Peer Up" Key=172.16.100.2 State=BGP_FSM_OPENCONFIRM Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Peer Event: {65001 ESTABLISHED}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:ESTABLISHED  router_id:\"172.16.100.2\"}  transport:{local_address:\"172.16.100.1\"  local_port:1790  remote_port:45279}}"]]></system-out>
		</testcase>
		<testcase name="Test_LBEgressAdvertisement/advertise_service_IP" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="0.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
time="2023-05-31T11:42:08Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{Origin: i} 65001 {Nexthop: 172.16.100.1}]" component=tests.BGP nlri="[10.100.1.1/32]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:08Z" level=debug msg="create Destination" Nlri=10.100.1.1/32 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="create Destination" Nlri=10.100.1.1/32 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="From me, ignore" Data="{ 10.100.1.1/32 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1 }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Route Event: {65001 10.100.1.1 32 false}"]]></system-out>
		</testcase>
		<testcase name="Test_LBEgressAdvertisement/withdraw_service_IP" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="0.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
time="2023-05-31T11:42:08Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[10.100.1.1/32]"
time="2023-05-31T11:42:08Z" level=debug msg="Removing withdrawals" Key=10.100.1.1/32 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="From me, ignore" Data="{ 10.100.1.1/32 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1, withdraw }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Route Event: {65001 10.100.1.1 32 true}"]]></system-out>
		</testcase>
		<testcase name="Test_LBEgressAdvertisement/re-advertise_service_IP" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="0.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
time="2023-05-31T11:42:08Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{Origin: i} 65001 {Nexthop: 172.16.100.1}]" component=tests.BGP nlri="[10.100.1.1/32]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:08Z" level=debug msg="create Destination" Nlri=10.100.1.1/32 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="create Destination" Nlri=10.100.1.1/32 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="From me, ignore" Data="{ 10.100.1.1/32 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1 }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Route Event: {65001 10.100.1.1 32 false}"]]></system-out>
		</testcase>
		<testcase name="Test_LBEgressAdvertisement/update_service_IP" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="0.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
time="2023-05-31T11:42:08Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{Origin: i} 65001 {Nexthop: 172.16.100.1}]" component=tests.BGP nlri="[10.200.1.1/32]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:08Z" level=debug msg="create Destination" Nlri=10.200.1.1/32 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="create Destination" Nlri=10.200.1.1/32 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="From me, ignore" Data="{ 10.200.1.1/32 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1 }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[10.100.1.1/32]"
time="2023-05-31T11:42:08Z" level=debug msg="Removing withdrawals" Key=10.100.1.1/32 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="From me, ignore" Data="{ 10.100.1.1/32 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1, withdraw }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Route Event: {65001 10.200.1.1 32 false}"
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Route Event: {65001 10.100.1.1 32 true}"]]></system-out>
		</testcase>
		<testcase name="Test_LBEgressAdvertisement/advertise_v6_service_IP" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="0.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
time="2023-05-31T11:42:08Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{Origin: i} 65001 {MpReach(ipv6-unicast): {Nexthop: 172.16.100.1, NLRIs: [cccc::1/128]}}]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:08Z" level=debug msg="create Destination" Nlri="cccc::1/128" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="create Destination" Nlri="cccc::1/128" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="From me, ignore" Data="{ cccc::1/128 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1 }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Route Event: {65001 cccc::1 128 false}"]]></system-out>
		</testcase>
		<testcase name="Test_LBEgressAdvertisement/withdraw_v6_service_IP" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="0.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
time="2023-05-31T11:42:08Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{MpUnreach(ipv6-unicast): {NLRIs: [cccc::1/128]}}]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:08Z" level=debug msg="Removing withdrawals" Key="cccc::1/128" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="From me, ignore" Data="{ cccc::1/128 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1, withdraw }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Route Event: {65001 cccc::1 128 true}"]]></system-out>
		</testcase>
		<testcase name="Test_LBEgressAdvertisement/re-advertise_v6_service_IP" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="0.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
time="2023-05-31T11:42:08Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{Origin: i} 65001 {MpReach(ipv6-unicast): {Nexthop: 172.16.100.1, NLRIs: [cccc::1/128]}}]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:08Z" level=debug msg="create Destination" Nlri="cccc::1/128" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="create Destination" Nlri="cccc::1/128" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="From me, ignore" Data="{ cccc::1/128 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1 }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Route Event: {65001 cccc::1 128 false}"]]></system-out>
		</testcase>
		<testcase name="Test_LBEgressAdvertisement/update_v6_service_IP" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="0.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
time="2023-05-31T11:42:08Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{Origin: i} 65001 {MpReach(ipv6-unicast): {Nexthop: 172.16.100.1, NLRIs: [dddd::1/128]}}]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:08Z" level=debug msg="create Destination" Nlri="dddd::1/128" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="create Destination" Nlri="dddd::1/128" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="From me, ignore" Data="{ dddd::1/128 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1 }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="received update" Key=172.16.100.1 Topic=Peer asn=65011 attributes="[{MpUnreach(ipv6-unicast): {NLRIs: [cccc::1/128]}}]" component=tests.BGP nlri="[]" subsys=basic withdrawals="[]"
time="2023-05-31T11:42:08Z" level=debug msg="Removing withdrawals" Key="cccc::1/128" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="From me, ignore" Data="{ cccc::1/128 | src: { 172.16.100.1 | as: 65001, id: 172.16.100.1 }, nh: 172.16.100.1, withdraw }" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Route Event: {65001 dddd::1 128 false}"
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Route Event: {65001 cccc::1 128 true}"
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: stopping"
time="2023-05-31T11:42:08Z" level=info msg="Delete a peer configuration" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="Removing withdrawals" Key=10.200.1.1/32 Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="Removing withdrawals" Key="dddd::1/128" Topic=Table asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=info msg="Peer Down" Key=172.16.100.1 Reason=dying State=BGP_FSM_ESTABLISHED Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="freed fsm.h" Key=172.16.100.1 State=BGP_FSM_ESTABLISHED Topic=Peer asn=65011 component=tests.BGP subsys=basic
level=info msg=Stopping subsys=hive
level=info msg="Delete a peer configuration" Key=172.16.100.2 Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Peer Down" Key=172.16.100.2 Reason=dying State=BGP_FSM_ESTABLISHED Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Removed BGP server with local ASN 65001" component=manager.remove subsys=bgp-control-plane
level=info msg="Cilium BGP Control Plane Controller shut down" component=Controller.Run subsys=bgp-control-plane
level=info msg="Stop hook executed" duration="184.634µs" function="*agent.Controller.Stop" subsys=hive
level=info msg="Stop hook executed" duration="12.062µs" function="*agent.kubernetesNodeSpecer.Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.016µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="61.224µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="9.658µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumBGPPeeringPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="8.786µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="8.146µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
time="2023-05-31T11:42:08Z" level=info msg="deleting dummy links"]]></system-out>
		</testcase>
		<testcase name="Test_NeighborAddDel" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="10.060">
			<system-out><![CDATA[time="2023-05-31T11:42:08Z" level=info msg="deleting dummy links"
time="2023-05-31T11:42:08Z" level=info msg="adding dummy links"
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: starting"
time="2023-05-31T11:42:08Z" level=info msg="Add a peer configuration" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="IdleHoldTimer expired" Duration=0 Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: starting"
time="2023-05-31T11:42:08Z" level=info msg="Add a peer configuration" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="IdleHoldTimer expired" Duration=0 Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_ACTIVE old=BGP_FSM_IDLE reason=idle-hold-timer-expired subsys=basic
time="2023-05-31T11:42:08Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_ACTIVE old=BGP_FSM_IDLE reason=idle-hold-timer-expired subsys=basic
level=info msg=Invoked duration="142.455µs" function="test.newFixture.func6 (fixtures.go:155)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="19.417µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.168218ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="1.403µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumBGPPeeringPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.222µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="2.375µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="110.486µs" function="*agent.kubernetesNodeSpecer.Start" subsys=hive
level=info msg="Start hook executed" duration=100.556796ms function="*agent.Controller.Start" subsys=hive]]></system-out>
		</testcase>
		<testcase name="Test_NeighborAddDel/add_two_neighbors" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="8.800">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller now running..." component=Controller.Run subsys=bgp-control-plane
level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Peer Event: {0 IDLE}"
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Peer Event: {65001 IDLE}"
time="2023-05-31T11:42:08Z" level=info msg="GoBGP test instance: Peer Event: {65001 ACTIVE}"
level=info msg="Registering BGP servers for policy with local ASN 65001" component=manager.registerBGPServer subsys=bgp-control-plane
level=info msg="Reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Adding peer 172.16.100.2/32 65011 to local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Add a peer configuration" Key=172.16.100.2 Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Successfully registered GoBGP servers for policy with local ASN 65001" component=manager.registerBGPServer subsys=bgp-control-plane
level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Adding peer 172.16.100.3/32 65012 to local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Add a peer configuration" Key=172.16.100.3 Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:IDLE  router_id:\"<nil>\"}  transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:ACTIVE  router_id:\"<nil>\"}  transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.3\"  peer_asn:65012}  state:{local_asn:65001  neighbor_address:\"172.16.100.3\"  peer_asn:65012  session_state:IDLE  router_id:\"<nil>\"}  transport:{local_address:\"<nil>\"}}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.3\"  peer_asn:65012}  state:{local_asn:65001  neighbor_address:\"172.16.100.3\"  peer_asn:65012  session_state:ACTIVE  router_id:\"<nil>\"}  transport:{local_address:\"<nil>\"}}"
time="2023-05-31T11:42:15Z" level=debug msg="try to connect" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:15Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_OPENSENT old=BGP_FSM_ACTIVE reason=new-connection subsys=basic
time="2023-05-31T11:42:15Z" level=info msg="GoBGP test instance: Peer Event: {65001 OPENSENT}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:OPENSENT  router_id:\"<nil>\"}  transport:{local_address:\"172.16.100.1\"  local_port:1790  remote_port:44815}}"
time="2023-05-31T11:42:15Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_OPENCONFIRM old=BGP_FSM_OPENSENT reason=open-msg-received subsys=basic
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:OPENCONFIRM  router_id:\"172.16.100.2\"}  transport:{local_address:\"172.16.100.1\"  local_port:1790  remote_port:44815}}"
time="2023-05-31T11:42:15Z" level=info msg="GoBGP test instance: Peer Event: {65001 OPENCONFIRM}"
level=info msg="Peer Up" Key=172.16.100.2 State=BGP_FSM_OPENCONFIRM Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
time="2023-05-31T11:42:15Z" level=info msg="Peer Up" Key=172.16.100.1 State=BGP_FSM_OPENCONFIRM Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:15Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_ESTABLISHED old=BGP_FSM_OPENCONFIRM reason=open-msg-negotiated subsys=basic
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011}  state:{local_asn:65001  neighbor_address:\"172.16.100.2\"  peer_asn:65011  session_state:ESTABLISHED  router_id:\"172.16.100.2\"}  transport:{local_address:\"172.16.100.1\"  local_port:1790  remote_port:44815}}"
time="2023-05-31T11:42:15Z" level=info msg="GoBGP test instance: Peer Event: {65001 ESTABLISHED}"
time="2023-05-31T11:42:15Z" level=info msg="GoBGP test instance: Peer Event: {0 IDLE}"
time="2023-05-31T11:42:15Z" level=info msg="GoBGP test instance: Peer Event: {65001 IDLE}"
time="2023-05-31T11:42:15Z" level=info msg="GoBGP test instance: Peer Event: {65001 ACTIVE}"
time="2023-05-31T11:42:16Z" level=debug msg="try to connect" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:16Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_OPENSENT old=BGP_FSM_ACTIVE reason=new-connection subsys=basic
time="2023-05-31T11:42:16Z" level=info msg="GoBGP test instance: Peer Event: {65001 OPENSENT}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.3\"  peer_asn:65012}  state:{local_asn:65001  neighbor_address:\"172.16.100.3\"  peer_asn:65012  session_state:OPENSENT  router_id:\"<nil>\"}  transport:{local_address:\"172.16.100.1\"  local_port:1790  remote_port:47875}}"
time="2023-05-31T11:42:16Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_OPENCONFIRM old=BGP_FSM_OPENSENT reason=open-msg-received subsys=basic
time="2023-05-31T11:42:16Z" level=info msg="GoBGP test instance: Peer Event: {65001 OPENCONFIRM}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.3\"  peer_asn:65012}  state:{local_asn:65001  neighbor_address:\"172.16.100.3\"  peer_asn:65012  session_state:OPENCONFIRM  router_id:\"172.16.100.2\"}  transport:{local_address:\"172.16.100.1\"  local_port:1790  remote_port:47875}}"
level=info msg="Peer Up" Key=172.16.100.3 State=BGP_FSM_OPENCONFIRM Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
time="2023-05-31T11:42:16Z" level=info msg="Peer Up" Key=172.16.100.1 State=BGP_FSM_OPENCONFIRM Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:16Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_ESTABLISHED old=BGP_FSM_OPENCONFIRM reason=open-msg-negotiated subsys=basic
time="2023-05-31T11:42:16Z" level=info msg="GoBGP test instance: Peer Event: {65001 ESTABLISHED}"
level=info msg="type:STATE  peer:{conf:{local_asn:65001  neighbor_address:\"172.16.100.3\"  peer_asn:65012}  state:{local_asn:65001  neighbor_address:\"172.16.100.3\"  peer_asn:65012  session_state:ESTABLISHED  router_id:\"172.16.100.2\"}  transport:{local_address:\"172.16.100.1\"  local_port:1790  remote_port:47875}}"]]></system-out>
		</testcase>
		<testcase name="Test_NeighborAddDel/delete_both_neighbors" classname="github.com/cilium/cilium/pkg/bgpv1/test" time="1.000">
			<system-out><![CDATA[level=info msg="Cilium BGP Control Plane Controller woken for reconciliation" component=Controller.Run subsys=bgp-control-plane
level=info msg="Reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Removing peer 172.16.100.2/32 65011 from local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=172.16.100.2 Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Removing peer 172.16.100.3/32 65012 from local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Delete a peer configuration" Key=172.16.100.3 Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
level=info msg="Done reconciling peers for virtual router with local ASN 65001" component=manager.neighborReconciler subsys=bgp-control-plane
level=info msg="Peer Down" Key=172.16.100.2 Reason=dying State=BGP_FSM_ESTABLISHED Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
time="2023-05-31T11:42:17Z" level=warning msg="received notification" Code=6 Data="[]" Key=172.16.100.1 Subcode=3 Topic=Peer asn=65011 component=tests.BGP subsys=basic
level=info msg="Peer Down" Key=172.16.100.3 Reason=dying State=BGP_FSM_ESTABLISHED Topic=Peer asn=65001 component=gobgp.BgpServerInstance subsys=bgp-control-plane
time="2023-05-31T11:42:17Z" level=info msg="Peer Down" Key=172.16.100.1 Reason="notification-received code 6(cease) subcode 3(peer deconfigured)" State=BGP_FSM_ESTABLISHED Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:17Z" level=warning msg="received notification" Code=6 Data="[]" Key=172.16.100.1 Subcode=3 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:17Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_IDLE old=BGP_FSM_ESTABLISHED reason="notification-received code 6(cease) subcode 3(peer deconfigured)" subsys=basic
time="2023-05-31T11:42:17Z" level=info msg="Peer Down" Key=172.16.100.1 Reason="notification-received code 6(cease) subcode 3(peer deconfigured)" State=BGP_FSM_ESTABLISHED Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:17Z" level=debug msg="state changed" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP new=BGP_FSM_IDLE old=BGP_FSM_ESTABLISHED reason="notification-received code 6(cease) subcode 3(peer deconfigured)" subsys=basic
time="2023-05-31T11:42:17Z" level=info msg="GoBGP test instance: Peer Event: {65001 IDLE}"
time="2023-05-31T11:42:17Z" level=info msg="GoBGP test instance: Peer Event: {65001 IDLE}"
time="2023-05-31T11:42:18Z" level=info msg="GoBGP test instance: stopping"
time="2023-05-31T11:42:18Z" level=info msg="Delete a peer configuration" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:18Z" level=debug msg="freed fsm.h" Key=172.16.100.1 State=BGP_FSM_IDLE Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:18Z" level=info msg="GoBGP test instance: stopping"
time="2023-05-31T11:42:18Z" level=info msg="Delete a peer configuration" Key=172.16.100.1 Topic=Peer asn=65011 component=tests.BGP subsys=basic
time="2023-05-31T11:42:18Z" level=debug msg="freed fsm.h" Key=172.16.100.1 State=BGP_FSM_IDLE Topic=Peer asn=65011 component=tests.BGP subsys=basic
level=info msg=Stopping subsys=hive
level=info msg="Removed BGP server with local ASN 65001" component=manager.remove subsys=bgp-control-plane
level=info msg="Cilium BGP Control Plane Controller shut down" component=Controller.Run subsys=bgp-control-plane
level=info msg="Stop hook executed" duration="101.41µs" function="*agent.Controller.Stop" subsys=hive
level=info msg="Stop hook executed" duration="33.853µs" function="*agent.kubernetesNodeSpecer.Stop" subsys=hive
level=info msg="Stop hook executed" duration="4.048µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="66.88µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="353.659µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumBGPPeeringPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="144.098µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="14.858µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
time="2023-05-31T11:42:18Z" level=info msg="deleting dummy links"]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/bgpv1/test	coverage: 85.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bpf" tests="24" failures="0" errors="0" id="143" hostname="kind-bpf-next" time="0.156" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/bpf" time="0.120"></testcase>
		<testcase name="Test/BPFTestSuite" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFTestSuite/TestDefaultMapFlags" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFTestSuite/TestEndpointKeyToString" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFTestSuite/TestExtractCommonName" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFTestSuite/TestPreallocationFlags" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFPrivilegedTestSuite" classname="github.com/cilium/cilium/pkg/bpf" time="0.110">
			<system-out><![CDATA[level=info msg="Detected mounted BPF filesystem at /sys/fs/bpf" subsys=bpf]]></system-out>
		</testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestBasicManipulation" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestCheckAndUpgrade" classname="github.com/cilium/cilium/pkg/bpf" time="0.000">
			<system-out><![CDATA[level=warning msg="Flags mismatch for BPF map" file-path=/sys/fs/bpf/tc/globals/cilium_test_upgrade new=0 old=1 subsys=bpf
level=warning msg="Removing map to allow for property upgrade (expect map data loss)" file-path=/sys/fs/bpf/tc/globals/cilium_test_upgrade subsys=bpf]]></system-out>
		</testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestCreateUnpinned" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestDeleteAll" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestDump" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestDumpReliablyWithCallback" classname="github.com/cilium/cilium/pkg/bpf" time="0.110"></testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestGetMapInfo" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestGetModel" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestOpen" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestOpenMap" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestOpenOrCreate" classname="github.com/cilium/cilium/pkg/bpf" time="0.000">
			<system-out><![CDATA[level=info msg="Unpinning map with incompatible properties" file-path=/sys/fs/bpf/tc/globals/cilium_test new="Type:Hash KeySize:4 ValueSize:4 MaxEntries:16 Flags:0" old="Type:Hash KeySize:4 ValueSize:4 MaxEntries:16 Flags:1" subsys=bpf]]></system-out>
		</testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestRecreateMap" classname="github.com/cilium/cilium/pkg/bpf" time="0.000">
			<system-out><![CDATA[level=info msg="Removed map pin at /sys/fs/bpf/tc/globals/cilium_test, recreating and re-pinning map cilium_test" file-path=/sys/fs/bpf/tc/globals/cilium_test name=cilium_test subsys=bpf]]></system-out>
		</testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestSubscribe" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="Test/BPFPrivilegedTestSuite/TestUnpin" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="TestLoadCollectionResizeLogBuffer" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="TestInlineGlobalData" classname="github.com/cilium/cilium/pkg/bpf" time="0.000"></testcase>
		<testcase name="TestEventsSubscribe" classname="github.com/cilium/cilium/pkg/bpf" time="0.020">
			<system-out><![CDATA[level=warning msg="subscription channel buffer 0 was full, closing subscription" error="timed out waiting to send sub map event" subsys=bpf]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/bpf	coverage: 54.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/bpf/binary" tests="5" failures="0" errors="0" id="144" hostname="kind-bpf-next" time="0.003" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestLittleEndianRead" classname="github.com/cilium/cilium/pkg/bpf/binary" time="0.000"></testcase>
		<testcase name="TestBigEndianRead" classname="github.com/cilium/cilium/pkg/bpf/binary" time="0.000"></testcase>
		<testcase name="TestReadSlice" classname="github.com/cilium/cilium/pkg/bpf/binary" time="0.000"></testcase>
		<testcase name="TestReadBool" classname="github.com/cilium/cilium/pkg/bpf/binary" time="0.000"></testcase>
		<testcase name="TestReadBoolSlice" classname="github.com/cilium/cilium/pkg/bpf/binary" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/bpf/binary	coverage: 72.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/byteorder" tests="5" failures="0" errors="0" id="145" hostname="kind-bpf-next" time="0.005" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/byteorder" time="0.000"></testcase>
		<testcase name="Test/ByteorderSuite" classname="github.com/cilium/cilium/pkg/byteorder" time="0.000"></testcase>
		<testcase name="Test/ByteorderSuite/TestHostToNetwork" classname="github.com/cilium/cilium/pkg/byteorder" time="0.000"></testcase>
		<testcase name="Test/ByteorderSuite/TestNativeIsInitialized" classname="github.com/cilium/cilium/pkg/byteorder" time="0.000"></testcase>
		<testcase name="Test/ByteorderSuite/TestNetIPv4ToHost32" classname="github.com/cilium/cilium/pkg/byteorder" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/byteorder	coverage: 55.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/cgroups/manager" tests="8" failures="0" errors="0" id="146" hostname="kind-bpf-next" time="0.022" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/cgroups/manager" time="0.000"></testcase>
		<testcase name="Test/ManagerSuite" classname="github.com/cilium/cilium/pkg/cgroups/manager" time="0.000"></testcase>
		<testcase name="Test/ManagerSuite/TestGetPodMetadataOnManagerDisabled" classname="github.com/cilium/cilium/pkg/cgroups/manager" time="0.000"></testcase>
		<testcase name="Test/ManagerSuite/TestGetPodMetadataOnPodAdd" classname="github.com/cilium/cilium/pkg/cgroups/manager" time="0.000">
			<system-out><![CDATA[level=info msg="Cgroup metadata manager is enabled" subsys=cgroup-manager]]></system-out>
		</testcase>
		<testcase name="Test/ManagerSuite/TestGetPodMetadataOnPodUpdate" classname="github.com/cilium/cilium/pkg/cgroups/manager" time="0.000">
			<system-out><![CDATA[level=info msg="Cgroup metadata manager is enabled" subsys=cgroup-manager]]></system-out>
		</testcase>
		<testcase name="Test/ProviderSuite" classname="github.com/cilium/cilium/pkg/cgroups/manager" time="0.000"></testcase>
		<testcase name="Test/ProviderSuite/TestGetBasePath" classname="github.com/cilium/cilium/pkg/cgroups/manager" time="0.000"></testcase>
		<testcase name="Test/ProviderSuite/TestGetContainerPath" classname="github.com/cilium/cilium/pkg/cgroups/manager" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/cgroups/manager	coverage: 69.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/checker" tests="3" failures="0" errors="0" id="147" hostname="kind-bpf-next" time="0.005" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/checker" time="0.000"></testcase>
		<testcase name="Test/CheckerSuite" classname="github.com/cilium/cilium/pkg/checker" time="0.000"></testcase>
		<testcase name="Test/CheckerSuite/TestDeepEqualsCheck" classname="github.com/cilium/cilium/pkg/checker" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/checker	coverage: 5.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/cidr" tests="11" failures="0" errors="0" id="148" hostname="kind-bpf-next" time="0.008" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/cidr" time="0.000"></testcase>
		<testcase name="Test/CidrTestSuite" classname="github.com/cilium/cilium/pkg/cidr" time="0.000"></testcase>
		<testcase name="Test/CidrTestSuite/TestAvailableIPs" classname="github.com/cilium/cilium/pkg/cidr" time="0.000"></testcase>
		<testcase name="Test/CidrTestSuite/TestDeepCopy" classname="github.com/cilium/cilium/pkg/cidr" time="0.000"></testcase>
		<testcase name="Test/CidrTestSuite/TestDiffIPNetLists" classname="github.com/cilium/cilium/pkg/cidr" time="0.000"></testcase>
		<testcase name="Test/CidrTestSuite/TestEqual" classname="github.com/cilium/cilium/pkg/cidr" time="0.000"></testcase>
		<testcase name="Test/CidrTestSuite/TestIllegalMustParseCIDR" classname="github.com/cilium/cilium/pkg/cidr" time="0.000"></testcase>
		<testcase name="Test/CidrTestSuite/TestIllegalParseCIDR" classname="github.com/cilium/cilium/pkg/cidr" time="0.000"></testcase>
		<testcase name="Test/CidrTestSuite/TestNewCIDRNil" classname="github.com/cilium/cilium/pkg/cidr" time="0.000"></testcase>
		<testcase name="Test/CidrTestSuite/TestNilDeepCopy" classname="github.com/cilium/cilium/pkg/cidr" time="0.000"></testcase>
		<testcase name="Test/CidrTestSuite/TestRemoveAll" classname="github.com/cilium/cilium/pkg/cidr" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/cidr	coverage: 78.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/cleanup" tests="3" failures="0" errors="0" id="149" hostname="kind-bpf-next" time="0.004" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/cleanup" time="0.000"></testcase>
		<testcase name="Test/CleanupTestSuite" classname="github.com/cilium/cilium/pkg/cleanup" time="0.000"></testcase>
		<testcase name="Test/CleanupTestSuite/TestHandleCleanup" classname="github.com/cilium/cilium/pkg/cleanup" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/cleanup	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/client" tests="5" failures="0" errors="0" id="150" hostname="kind-bpf-next" time="0.012" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/client" time="0.000"></testcase>
		<testcase name="Test/ClientTestSuite" classname="github.com/cilium/cilium/pkg/client" time="0.000"></testcase>
		<testcase name="Test/ClientTestSuite/TestClusterReadiness" classname="github.com/cilium/cilium/pkg/client" time="0.000"></testcase>
		<testcase name="Test/ClientTestSuite/TestHint" classname="github.com/cilium/cilium/pkg/client" time="0.000"></testcase>
		<testcase name="Test/ClientTestSuite/TestNumReadyClusters" classname="github.com/cilium/cilium/pkg/client" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/client	coverage: 2.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/clustermesh" tests="3" failures="0" errors="0" id="151" hostname="kind-bpf-next" skipped="2" time="0.053" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/clustermesh" time="0.000"></testcase>
		<testcase name="Test/ClusterMeshTestSuite" classname="github.com/cilium/cilium/pkg/clustermesh" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/ClusterMeshServicesTestSuite" classname="github.com/cilium/cilium/pkg/clustermesh" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/clustermesh	coverage: 0.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/clustermesh/types" tests="41" failures="0" errors="0" id="152" hostname="kind-bpf-next" time="0.007" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestParseAddrCluster" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParseAddrCluster/valid_bare_IPv4_address" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParseAddrCluster/invalid_bare_IPv4_address" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParseAddrCluster/valid_IPv4_address_and_valid_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParseAddrCluster/invalid_IPv4_address_and_valid_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParseAddrCluster/valid_IPv4_address_and_invalid_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParseAddrCluster/valid_IPv4_address_and_enpty_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParseAddrCluster/valid_bare_IPv6_address" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParseAddrCluster/invalid_bare_IPv6_address" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParseAddrCluster/valid_IPv6_address_and_valid_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParseAddrCluster/invalid_IPv6_address_and_valid_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParseAddrCluster/valid_IPv6_address_and_invalid_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParseAddrCluster/valid_IPv6_address_and_enpty_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_Equal" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_Equal/same_IP_and_same_ClusterID" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_Equal/same_IP_and_different_ClusterID" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_Equal/different_IP_and_same_ClusterID" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_Equal/different_IP_and_different_ClusterID" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_Less" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_Less/same_IP_and_same_ClusterID" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_Less/larger_IP_and_same_ClusterID" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_Less/smaller_IP_and_smaller_ClusterID" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_Less/same_IP_and_larger_ClusterID" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_Less/same_IP_and_smaller_ClusterID" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_String" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_String/zero_ClusterID" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestAddrCluster_String/non-zero_ClusterID" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/valid_bare_IPv4_prefix" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/invalid_bare_IPv4_prefix_1" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/invalid_bare_IPv4_prefix_2" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/valid_IPv4_prefix_and_valid_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/invalid_IPv4_prefix_and_valid_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/valid_IPv4_prefix_and_invalid_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/valid_IPv4_prefix_and_empty_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/valid_bare_IPv6_prefix" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/invalid_bare_IPv6_prefix" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/valid_IPv6_prefix_and_valid_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/invalid_IPv6_prefix_and_valid_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/valid_IPv6_prefix_and_invalid_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<testcase name="TestParsePrefixCluster/valid_IPv6_prefix_and_empty_cluster-id" classname="github.com/cilium/cilium/pkg/clustermesh/types" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/clustermesh/types	coverage: 45.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/command" tests="46" failures="0" errors="0" id="153" hostname="kind-bpf-next" time="0.012" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestGetStringMapString" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_json_format" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_empty_json" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/invalid_json_format_with_extra_comma_at_the_end" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_single_kv_format" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_kv_format" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_kv_format_with_@" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_kv_format_with_empty_value" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_kv_format_with_a_single_key_and_commas_in_value" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_kv_format_with_multiple_keys_with_commas_in_value" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/another_valid_kv_format_with_comma_in_value" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_kv_format_with_forward_slash" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_kv_format_with_hyphens" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_kv_format_with_space" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_kv_format_from_issue_#20666" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/valid_kv_format_for_cluster-pool-map" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/invalid_kv_format_with_extra_comma" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/invalid_kv_format_with_extra_equal" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/invalid_kv_format_with_wrong_space_in_between" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/malformed_json_format" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapString/staring_with_valid_json_beginning_value_e.g._t,_f,_n,_0,_-,_&#34;" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="TestGetStringMapStringConversion" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/valid_format_with_one_pair" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/valid_format_with_hyphen_in_k_and_v" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/valid_format_with_multiple_hyphens" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/valid_format_with_colon" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/valid_format_with_forward_slash" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/valid_format_with_multiple_pairs" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/valid_format_with_multiple_pairs_with_commas" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/empty_value" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/space_in_between" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/insufficient_value" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/no_pair_at_all" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/ending_with_comma" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/ending_with_equal" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/kv_separator_as_space" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/space_in_key" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/value_starts_with_space" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/last_value_starts_with_space" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/value_ends_with_space" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test_isValidKeyValuePair/last_value_ends_with_space" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test/CMDHelpersSuite" classname="github.com/cilium/cilium/pkg/command" time="0.000"></testcase>
		<testcase name="Test/CMDHelpersSuite/TestDumpJSON" classname="github.com/cilium/cilium/pkg/command" time="0.000">
			<system-out><![CDATA[{
  "ID": 1,
  "Name": "test"
}


Couldn't parse jsonpath expression: 'unrecognized character in action: U+007B '{'']]></system-out>
		</testcase>
		<testcase name="Test/CMDHelpersSuite/TestDumpYAML" classname="github.com/cilium/cilium/pkg/command" time="0.000">
			<system-out><![CDATA[id: 1
name: test
]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/command	coverage: 54.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/command/exec" tests="7" failures="0" errors="0" id="154" hostname="kind-bpf-next" time="0.528" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/command/exec" time="0.520"></testcase>
		<testcase name="Test/ExecTestSuite" classname="github.com/cilium/cilium/pkg/command/exec" time="0.520"></testcase>
		<testcase name="Test/ExecTestSuite/TestCanceled" classname="github.com/cilium/cilium/pkg/command/exec" time="0.000"></testcase>
		<testcase name="Test/ExecTestSuite/TestCombinedOutput" classname="github.com/cilium/cilium/pkg/command/exec" time="0.010"></testcase>
		<testcase name="Test/ExecTestSuite/TestCombinedOutputFailedTimeout" classname="github.com/cilium/cilium/pkg/command/exec" time="0.250">
			<system-out><![CDATA[level=error msg="Command execution failed" cmd="[sleep inf]" error="signal: killed" foo=bar]]></system-out>
		</testcase>
		<testcase name="Test/ExecTestSuite/TestWithCancel" classname="github.com/cilium/cilium/pkg/command/exec" time="0.010"></testcase>
		<testcase name="Test/ExecTestSuite/TestWithTimeout" classname="github.com/cilium/cilium/pkg/command/exec" time="0.250"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/command/exec	coverage: 47.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/common" tests="5" failures="0" errors="0" id="155" hostname="kind-bpf-next" time="0.009" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/common" time="0.000"></testcase>
		<testcase name="Test/CommonSuite" classname="github.com/cilium/cilium/pkg/common" time="0.000"></testcase>
		<testcase name="Test/CommonSuite/TestC2GoArray" classname="github.com/cilium/cilium/pkg/common" time="0.000"></testcase>
		<testcase name="Test/CommonSuite/TestGetNumPossibleCPUsFromReader" classname="github.com/cilium/cilium/pkg/common" time="0.000">
			<system-out><![CDATA[level=error msg="failed to scan \"\" to retrieve number of possible CPUs!" error=EOF subsys=utils-test
level=error msg="failed to scan \"foobar\" to retrieve number of possible CPUs!" error="expected integer" subsys=utils-test]]></system-out>
		</testcase>
		<testcase name="Test/CommonSuite/TestGoArray2C" classname="github.com/cilium/cilium/pkg/common" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/common	coverage: 65.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/comparator" tests="5" failures="0" errors="0" id="156" hostname="kind-bpf-next" time="0.004" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/comparator" time="0.000"></testcase>
		<testcase name="Test/ComparatorSuite" classname="github.com/cilium/cilium/pkg/comparator" time="0.000"></testcase>
		<testcase name="Test/ComparatorSuite/TestMapBoolEquals" classname="github.com/cilium/cilium/pkg/comparator" time="0.000"></testcase>
		<testcase name="Test/ComparatorSuite/TestMapStringEquals" classname="github.com/cilium/cilium/pkg/comparator" time="0.000"></testcase>
		<testcase name="Test/ComparatorSuite/TestMapStringEqualsIgnoreKeys" classname="github.com/cilium/cilium/pkg/comparator" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/comparator	coverage: 79.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/completion" tests="12" failures="0" errors="0" id="157" hostname="kind-bpf-next" time="1.008" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/completion" time="1.000"></testcase>
		<testcase name="Test/CompletionSuite" classname="github.com/cilium/cilium/pkg/completion" time="1.000"></testcase>
		<testcase name="Test/CompletionSuite/TestCompletionAfterWait" classname="github.com/cilium/cilium/pkg/completion" time="0.250"></testcase>
		<testcase name="Test/CompletionSuite/TestCompletionBeforeAndAfterWait" classname="github.com/cilium/cilium/pkg/completion" time="0.250"></testcase>
		<testcase name="Test/CompletionSuite/TestCompletionBeforeWait" classname="github.com/cilium/cilium/pkg/completion" time="0.000"></testcase>
		<testcase name="Test/CompletionSuite/TestCompletionMultipleCompleteCalls" classname="github.com/cilium/cilium/pkg/completion" time="0.000"></testcase>
		<testcase name="Test/CompletionSuite/TestCompletionTimeout" classname="github.com/cilium/cilium/pkg/completion" time="0.250"></testcase>
		<testcase name="Test/CompletionSuite/TestCompletionWithCallback" classname="github.com/cilium/cilium/pkg/completion" time="0.000"></testcase>
		<testcase name="Test/CompletionSuite/TestCompletionWithCallbackError" classname="github.com/cilium/cilium/pkg/completion" time="0.000"></testcase>
		<testcase name="Test/CompletionSuite/TestCompletionWithCallbackOtherError" classname="github.com/cilium/cilium/pkg/completion" time="0.000"></testcase>
		<testcase name="Test/CompletionSuite/TestCompletionWithCallbackTimeout" classname="github.com/cilium/cilium/pkg/completion" time="0.250"></testcase>
		<testcase name="Test/CompletionSuite/TestNoCompletion" classname="github.com/cilium/cilium/pkg/completion" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/completion	coverage: 97.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/container" tests="8" failures="0" errors="0" id="158" hostname="kind-bpf-next" time="0.569" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestRingBuffer_AddingAndIterating" classname="github.com/cilium/cilium/pkg/container" time="0.000"></testcase>
		<testcase name="TestEventBuffer_GC" classname="github.com/cilium/cilium/pkg/container" time="0.000"></testcase>
		<testcase name="TestEventBuffer_GC2" classname="github.com/cilium/cilium/pkg/container" time="0.000"></testcase>
		<testcase name="TestEventBuffer_GCFullBufferWithOverlap" classname="github.com/cilium/cilium/pkg/container" time="0.000"></testcase>
		<testcase name="TestEventBuffer_GCFullBuffer" classname="github.com/cilium/cilium/pkg/container" time="0.000"></testcase>
		<testcase name="TestEventBuffer_GCNotFullBuffer" classname="github.com/cilium/cilium/pkg/container" time="0.000"></testcase>
		<testcase name="Test_firstValidIndex" classname="github.com/cilium/cilium/pkg/container" time="0.000"></testcase>
		<testcase name="Test_firstValidIndex2" classname="github.com/cilium/cilium/pkg/container" time="0.570"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/container	coverage: 98.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/contexthelpers" tests="3" failures="0" errors="0" id="159" hostname="kind-bpf-next" time="0.145" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/contexthelpers" time="0.140"></testcase>
		<testcase name="Test/ContextSuite" classname="github.com/cilium/cilium/pkg/contexthelpers" time="0.140"></testcase>
		<testcase name="Test/ContextSuite/TestConditionalTimeoutContext" classname="github.com/cilium/cilium/pkg/contexthelpers" time="0.140"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/contexthelpers	coverage: 77.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/controller" tests="9" failures="0" errors="0" id="160" hostname="kind-bpf-next" time="1.120" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/controller" time="1.100"></testcase>
		<testcase name="Test/ControllerSuite" classname="github.com/cilium/cilium/pkg/controller" time="1.100"></testcase>
		<testcase name="Test/ControllerSuite/TestCancellation" classname="github.com/cilium/cilium/pkg/controller" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestRemoveAll" classname="github.com/cilium/cilium/pkg/controller" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestRunController" classname="github.com/cilium/cilium/pkg/controller" time="0.100"></testcase>
		<testcase name="Test/ControllerSuite/TestSelfExit" classname="github.com/cilium/cilium/pkg/controller" time="1.000"></testcase>
		<testcase name="Test/ControllerSuite/TestStopFunc" classname="github.com/cilium/cilium/pkg/controller" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestUpdateRemoveController" classname="github.com/cilium/cilium/pkg/controller" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestWaitForTermination" classname="github.com/cilium/cilium/pkg/controller" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/controller	coverage: 80.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/counter" tests="8" failures="0" errors="0" id="161" hostname="kind-bpf-next" time="0.008" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/counter" time="0.000"></testcase>
		<testcase name="Test/CounterTestSuite" classname="github.com/cilium/cilium/pkg/counter" time="0.000"></testcase>
		<testcase name="Test/CounterTestSuite/TestCheckLimits" classname="github.com/cilium/cilium/pkg/counter" time="0.000"></testcase>
		<testcase name="Test/CounterTestSuite/TestCounter" classname="github.com/cilium/cilium/pkg/counter" time="0.000"></testcase>
		<testcase name="Test/CounterTestSuite/TestDefaultPrefixLengthCounter" classname="github.com/cilium/cilium/pkg/counter" time="0.000"></testcase>
		<testcase name="Test/CounterTestSuite/TestReferenceTracker" classname="github.com/cilium/cilium/pkg/counter" time="0.000"></testcase>
		<testcase name="Test/CounterTestSuite/TestStringCounter" classname="github.com/cilium/cilium/pkg/counter" time="0.000"></testcase>
		<testcase name="Test/CounterTestSuite/TestToBPFData" classname="github.com/cilium/cilium/pkg/counter" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/counter	coverage: 95.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/crypto/certloader" tests="73" failures="0" errors="0" id="162" hostname="kind-bpf-next" time="1.758" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestWatchedClientConfigIsMutualTLS" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestWatchedClientConfigIsMutualTLS/empty" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestWatchedClientConfigIsMutualTLS/keypair_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestWatchedClientConfigIsMutualTLS/CA_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestWatchedClientConfigIsMutualTLS/CA_and_keypair" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestNewWatchedClientConfig" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestWatchedClientConfigRotation" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.200"></testcase>
		<testcase name="TestNewFileReloaderErrors" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasKeypair" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasKeypair/empty_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasKeypair/empty" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasKeypair/keypair_only_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasKeypair/keypair_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasKeypair/CA_only_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasKeypair/CA_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasKeypair/CA_and_keypair_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasKeypair/CA_and_keypair" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasCustomCA" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasCustomCA/empty_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasCustomCA/empty" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasCustomCA/keypair_only_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasCustomCA/keypair_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasCustomCA/CA_only_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasCustomCA/CA_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasCustomCA/CA_and_keypair_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestHasCustomCA/CA_and_keypair" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReady" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReady/empty_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReady/empty" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReady/keypair_only_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReady/keypair_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReady/CA_only_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReady/CA_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReady/CA_and_keypair_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReady/CA_and_keypair" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestKeypairAndCACertPool" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestKeypairAndCACertPool/empty_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestKeypairAndCACertPool/empty" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestKeypairAndCACertPool/keypair_only_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestKeypairAndCACertPool/keypair_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestKeypairAndCACertPool/CA_only_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestKeypairAndCACertPool/CA_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestKeypairAndCACertPool/CA_and_keypair_(ready)" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestKeypairAndCACertPool/CA_and_keypair" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReload" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReload/empty" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReload/keypair_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReload/CA_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReload/CA_and_keypair" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReloadKeypair" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReloadKeypair/empty" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReloadKeypair/keypair_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReloadKeypair/CA_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReloadKeypair/CA_and_keypair" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReloadCA" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReloadCA/empty" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReloadCA/keypair_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReloadCA/CA_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReloadCA/CA_and_keypair" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestReloadError" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestNewWatchedServerConfigErrors" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestWatchedServerConfigIsMutualTLS" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestWatchedServerConfigIsMutualTLS/keypair_only" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestWatchedServerConfigIsMutualTLS/CA_and_keypair" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestFutureWatchedServerConfig" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.300"></testcase>
		<testcase name="TestNewWatchedServerConfig" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestWatchedServerConfigRotation" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.200"></testcase>
		<testcase name="TestNewWatcherError" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestNewWatcher" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestRotation" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.210"></testcase>
		<testcase name="TestFutureWatcherImmediately" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.000"></testcase>
		<testcase name="TestFutureWatcher" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.300"></testcase>
		<testcase name="TestKubernetesMount" classname="github.com/cilium/cilium/pkg/crypto/certloader" time="0.500"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/crypto/certloader	coverage: 91.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/alignchecker" tests="1" failures="0" errors="0" id="163" hostname="kind-bpf-next" skipped="1" time="0.021" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestCheckStructAlignments" classname="github.com/cilium/cilium/pkg/datapath/alignchecker" time="0.000">
			<skipped message="Skipped"><![CDATA[    file.go:15: Skipping due to missing file ../../../bpf/bpf_alignchecker.o]]></skipped>
		</testcase>
		<system-out><![CDATA[testing: warning: no tests to run
	github.com/cilium/cilium/pkg/datapath	coverage: 0.0% of statements
ok  	github.com/cilium/cilium/pkg/datapath	0.051s	coverage: 0.0% of statements [no tests to run]
	github.com/cilium/cilium/pkg/datapath/alignchecker	coverage: 0.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/fake" tests="3" failures="0" errors="0" id="164" hostname="kind-bpf-next" time="0.024" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/datapath/fake" time="0.000"></testcase>
		<testcase name="Test/fakeTestSuite" classname="github.com/cilium/cilium/pkg/datapath/fake" time="0.000"></testcase>
		<testcase name="Test/fakeTestSuite/TestNewDatapath" classname="github.com/cilium/cilium/pkg/datapath/fake" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/fake	coverage: 38.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/garp" tests="3" failures="0" errors="0" id="165" hostname="kind-bpf-next" time="0.028" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/datapath/garp" time="0.010"></testcase>
		<testcase name="Test/garpSuite" classname="github.com/cilium/cilium/pkg/datapath/garp" time="0.010"></testcase>
		<testcase name="Test/garpSuite/TestGARPCell" classname="github.com/cilium/cilium/pkg/datapath/garp" time="0.010">
			<system-out><![CDATA[level=info msg="initialised gratuitous arp sender" interface=lo subsys=garp
level=info msg=Invoked duration=8.251204ms function="garp.(*garpSuite).TestGARPCell.func1 (garp_test.go:34)" subsys=hive]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/garp	coverage: 64.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/iptables" tests="10" failures="0" errors="0" id="166" hostname="kind-bpf-next" time="0.030" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/datapath/iptables" time="0.000"></testcase>
		<testcase name="Test/iptablesTestSuite" classname="github.com/cilium/cilium/pkg/datapath/iptables" time="0.000"></testcase>
		<testcase name="Test/iptablesTestSuite/TestAddProxyRulesv4" classname="github.com/cilium/cilium/pkg/datapath/iptables" time="0.000"></testcase>
		<testcase name="Test/iptablesTestSuite/TestAddProxyRulesv6" classname="github.com/cilium/cilium/pkg/datapath/iptables" time="0.000"></testcase>
		<testcase name="Test/iptablesTestSuite/TestCopyProxyRulesv4" classname="github.com/cilium/cilium/pkg/datapath/iptables" time="0.000"></testcase>
		<testcase name="Test/iptablesTestSuite/TestCopyProxyRulesv6" classname="github.com/cilium/cilium/pkg/datapath/iptables" time="0.000"></testcase>
		<testcase name="Test/iptablesTestSuite/TestGetProxyPort" classname="github.com/cilium/cilium/pkg/datapath/iptables" time="0.000"></testcase>
		<testcase name="Test/iptablesTestSuite/TestRemoveCiliumRulesv4" classname="github.com/cilium/cilium/pkg/datapath/iptables" time="0.000"></testcase>
		<testcase name="Test/iptablesTestSuite/TestRemoveCiliumRulesv6" classname="github.com/cilium/cilium/pkg/datapath/iptables" time="0.000"></testcase>
		<testcase name="Test/iptablesTestSuite/TestRenameCustomChain" classname="github.com/cilium/cilium/pkg/datapath/iptables" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/iptables	coverage: 12.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/link" tests="4" failures="0" errors="0" id="167" hostname="kind-bpf-next" time="0.044" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/datapath/link" time="0.020"></testcase>
		<testcase name="Test/LinkSuite" classname="github.com/cilium/cilium/pkg/datapath/link" time="0.020"></testcase>
		<testcase name="Test/LinkSuite/TestDeleteByName" classname="github.com/cilium/cilium/pkg/datapath/link" time="0.010"></testcase>
		<testcase name="Test/LinkSuite/TestRename" classname="github.com/cilium/cilium/pkg/datapath/link" time="0.020"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/link	coverage: 22.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/endpoint/regeneration" tests="0" failures="0" errors="0" id="168" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/flowdebug" tests="0" failures="0" errors="0" id="169" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/fqdn/proxy" tests="0" failures="0" errors="0" id="170" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/fqdn/re" tests="0" failures="0" errors="0" id="171" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/fqdn/restore" tests="0" failures="0" errors="0" id="172" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/gops" tests="0" failures="0" errors="0" id="173" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/health/defaults" tests="0" failures="0" errors="0" id="174" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/health/probe" tests="0" failures="0" errors="0" id="175" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/health/probe/responder" tests="0" failures="0" errors="0" id="176" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hive/cell" tests="0" failures="0" errors="0" id="177" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hive/example" tests="0" failures="0" errors="0" id="178" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hive/example/mini" tests="0" failures="0" errors="0" id="179" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hive/hivetest" tests="0" failures="0" errors="0" id="180" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hive/internal" tests="0" failures="0" errors="0" id="181" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/api/v1" tests="0" failures="0" errors="0" id="182" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/defaults" tests="0" failures="0" errors="0" id="183" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/exporter/exporteroption" tests="0" failures="0" errors="0" id="184" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/metrics/dns" tests="0" failures="0" errors="0" id="185" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/metrics/icmp" tests="0" failures="0" errors="0" id="186" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/metrics/kafka" tests="0" failures="0" errors="0" id="187" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/observer/observeroption" tests="0" failures="0" errors="0" id="188" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/observer/types" tests="0" failures="0" errors="0" id="189" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/parser/common" tests="0" failures="0" errors="0" id="190" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/parser/errors" tests="0" failures="0" errors="0" id="191" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/parser/getters" tests="0" failures="0" errors="0" id="192" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/parser/options" tests="0" failures="0" errors="0" id="193" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/peer/serviceoption" tests="0" failures="0" errors="0" id="194" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/recorder" tests="0" failures="0" errors="0" id="195" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/recorder/pcap" tests="0" failures="0" errors="0" id="196" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/recorder/recorderoption" tests="0" failures="0" errors="0" id="197" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/recorder/sink" tests="0" failures="0" errors="0" id="198" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/relay/defaults" tests="0" failures="0" errors="0" id="199" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/relay/pool/types" tests="0" failures="0" errors="0" id="200" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/server" tests="0" failures="0" errors="0" id="201" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/server/serveroption" tests="0" failures="0" errors="0" id="202" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/identity/key" tests="0" failures="0" errors="0" id="203" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/identity/model" tests="0" failures="0" errors="0" id="204" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/allocator/alibabacloud" tests="0" failures="0" errors="0" id="205" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/allocator/aws" tests="0" failures="0" errors="0" id="206" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/allocator/azure" tests="0" failures="0" errors="0" id="207" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/allocator/clusterpool" tests="0" failures="0" errors="0" id="208" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/metrics" tests="0" failures="0" errors="0" id="209" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/option" tests="0" failures="0" errors="0" id="210" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/service/ipallocator" tests="0" failures="0" errors="0" id="211" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/stats" tests="0" failures="0" errors="0" id="212" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipcache/types" tests="0" failures="0" errors="0" id="213" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/apis" tests="0" failures="0" errors="0" id="214" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/apis/cilium.io" tests="0" failures="0" errors="0" id="215" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1" tests="0" failures="0" errors="0" id="216" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/clientset/versioned" tests="0" failures="0" errors="0" id="217" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/clientset/versioned/fake" tests="0" failures="0" errors="0" id="218" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/clientset/versioned/scheme" tests="0" failures="0" errors="0" id="219" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/clientset/versioned/typed/cilium.io/v2" tests="0" failures="0" errors="0" id="220" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/clientset/versioned/typed/cilium.io/v2/fake" tests="0" failures="0" errors="0" id="221" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/clientset/versioned/typed/cilium.io/v2alpha1" tests="0" failures="0" errors="0" id="222" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/clientset/versioned/typed/cilium.io/v2alpha1/fake" tests="0" failures="0" errors="0" id="223" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/informers/externalversions" tests="0" failures="0" errors="0" id="224" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/informers/externalversions/cilium.io" tests="0" failures="0" errors="0" id="225" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/informers/externalversions/cilium.io/v2" tests="0" failures="0" errors="0" id="226" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/informers/externalversions/cilium.io/v2alpha1" tests="0" failures="0" errors="0" id="227" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/informers/externalversions/internalinterfaces" tests="0" failures="0" errors="0" id="228" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/listers/cilium.io/v2" tests="0" failures="0" errors="0" id="229" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client/listers/cilium.io/v2alpha1" tests="0" failures="0" errors="0" id="230" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/constants" tests="0" failures="0" errors="0" id="231" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/informer" tests="0" failures="0" errors="0" id="232" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/resource/example" tests="0" failures="0" errors="0" id="233" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1" tests="0" failures="0" errors="0" id="234" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/api/discovery/v1" tests="0" failures="0" errors="0" id="235" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/api/discovery/v1beta1" tests="0" failures="0" errors="0" id="236" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/api/networking/v1" tests="0" failures="0" errors="0" id="237" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apiextensions-client/clientset/versioned" tests="0" failures="0" errors="0" id="238" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apiextensions-client/clientset/versioned/fake" tests="0" failures="0" errors="0" id="239" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apiextensions-client/clientset/versioned/scheme" tests="0" failures="0" errors="0" id="240" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apiextensions-client/clientset/versioned/typed/apiextensions/v1" tests="0" failures="0" errors="0" id="241" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apiextensions-client/clientset/versioned/typed/apiextensions/v1/fake" tests="0" failures="0" errors="0" id="242" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apiextensions-clientset" tests="0" failures="0" errors="0" id="243" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apis/apiextensions/v1" tests="0" failures="0" errors="0" id="244" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apis/labels" tests="0" failures="0" errors="0" id="245" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apis/meta/v1" tests="0" failures="0" errors="0" id="246" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apis/meta/v1/validation" tests="0" failures="0" errors="0" id="247" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apis/meta/v1beta1" tests="0" failures="0" errors="0" id="248" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apis/selection" tests="0" failures="0" errors="0" id="249" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/apis/util/intstr" tests="0" failures="0" errors="0" id="250" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned" tests="0" failures="0" errors="0" id="251" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned/fake" tests="0" failures="0" errors="0" id="252" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned/scheme" tests="0" failures="0" errors="0" id="253" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned/typed/core/v1" tests="0" failures="0" errors="0" id="254" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned/typed/core/v1/fake" tests="0" failures="0" errors="0" id="255" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned/typed/discovery/v1" tests="0" failures="0" errors="0" id="256" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned/typed/discovery/v1/fake" tests="0" failures="0" errors="0" id="257" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned/typed/discovery/v1beta1" tests="0" failures="0" errors="0" id="258" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned/typed/discovery/v1beta1/fake" tests="0" failures="0" errors="0" id="259" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned/typed/networking/v1" tests="0" failures="0" errors="0" id="260" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned/typed/networking/v1/fake" tests="0" failures="0" errors="0" id="261" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/slim/k8s/clientset" tests="0" failures="0" errors="0" id="262" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/types" tests="0" failures="0" errors="0" id="263" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/version" tests="0" failures="0" errors="0" id="264" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/watchers/resources" tests="0" failures="0" errors="0" id="265" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/watchers/subscriber" tests="0" failures="0" errors="0" id="266" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/labels/model" tests="0" failures="0" errors="0" id="267" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/launcher" tests="0" failures="0" errors="0" id="268" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/loadinfo" tests="0" failures="0" errors="0" id="269" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/logging/hooks" tests="0" failures="0" errors="0" id="270" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/logging/logfields" tests="0" failures="0" errors="0" id="271" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps" tests="0" failures="0" errors="0" id="272" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/authmap/fake" tests="0" failures="0" errors="0" id="273" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/bwmap" tests="0" failures="0" errors="0" id="274" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/callsmap" tests="0" failures="0" errors="0" id="275" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/cidrmap" tests="0" failures="0" errors="0" id="276" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/configmap" tests="0" failures="0" errors="0" id="277" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/ctmap/gc" tests="0" failures="0" errors="0" id="278" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/encrypt" tests="0" failures="0" errors="0" id="279" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/eventsmap" tests="0" failures="0" errors="0" id="280" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/fragmap" tests="0" failures="0" errors="0" id="281" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/ipcache" tests="0" failures="0" errors="0" id="282" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/ipmasq" tests="0" failures="0" errors="0" id="283" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/lxcmap" tests="0" failures="0" errors="0" id="284" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/metricsmap" tests="0" failures="0" errors="0" id="285" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/neighborsmap" tests="0" failures="0" errors="0" id="286" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/nodemap" tests="0" failures="0" errors="0" id="287" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/recorder" tests="0" failures="0" errors="0" id="288" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/signalmap" tests="0" failures="0" errors="0" id="289" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/signalmap/fake" tests="0" failures="0" errors="0" id="290" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/srv6map" tests="0" failures="0" errors="0" id="291" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/vtep" tests="0" failures="0" errors="0" id="292" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/worldcidrsmap" tests="0" failures="0" errors="0" id="293" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/monitor/agent/consumer" tests="0" failures="0" errors="0" id="294" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/monitor/agent/listener" tests="0" failures="0" errors="0" id="295" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/monitor/alignchecker" tests="0" failures="0" errors="0" id="296" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/monitor/notifications" tests="0" failures="0" errors="0" id="297" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/netns" tests="0" failures="0" errors="0" id="298" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/node/addressing" tests="0" failures="0" errors="0" id="299" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/node/store" tests="0" failures="0" errors="0" id="300" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/nodediscovery" tests="0" failures="0" errors="0" id="301" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/option/fake" tests="0" failures="0" errors="0" id="302" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/policy/api/kafka" tests="0" failures="0" errors="0" id="303" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/policy/groups/aws" tests="0" failures="0" errors="0" id="304" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/policy/trafficdirection" tests="0" failures="0" errors="0" id="305" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/proxy/accesslog" tests="0" failures="0" errors="0" id="306" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/proxy/logger/test" tests="0" failures="0" errors="0" id="307" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/recorder" tests="0" failures="0" errors="0" id="308" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/statedb/example" tests="0" failures="0" errors="0" id="309" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/testutils/identity" tests="0" failures="0" errors="0" id="310" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/testutils/ipcache" tests="0" failures="0" errors="0" id="311" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/testutils/mockmaps" tests="0" failures="0" errors="0" id="312" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/tuple" tests="0" failures="0" errors="0" id="313" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/u8proto" tests="0" failures="0" errors="0" id="314" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/wireguard/types" tests="0" failures="0" errors="0" id="315" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/plugins/cilium-cni" tests="0" failures="0" errors="0" id="316" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/plugins/cilium-cni/chaining/awscni" tests="0" failures="0" errors="0" id="317" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/plugins/cilium-cni/chaining/azure" tests="0" failures="0" errors="0" id="318" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/plugins/cilium-cni/chaining/flannel" tests="0" failures="0" errors="0" id="319" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/plugins/cilium-cni/chaining/generic-veth" tests="0" failures="0" errors="0" id="320" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/plugins/cilium-cni/lib" tests="0" failures="0" errors="0" id="321" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/plugins/cilium-docker/driver" tests="0" failures="0" errors="0" id="322" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/plugins/cilium-docker" tests="0" failures="0" errors="0" id="323" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/accesslog" tests="0" failures="0" errors="0" id="324" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/memcached" tests="0" failures="0" errors="0" id="325" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/memcached/meta" tests="0" failures="0" errors="0" id="326" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/memcached/text" tests="0" failures="0" errors="0" id="327" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/test" tests="0" failures="0" errors="0" id="328" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/testparsers" tests="0" failures="0" errors="0" id="329" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/config" tests="0" failures="0" errors="0" id="330" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/controlplane/ciliumnetworkpolicies" tests="0" failures="0" errors="0" id="331" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/controlplane/node" tests="0" failures="0" errors="0" id="332" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/controlplane/node/ciliumnodes" tests="0" failures="0" errors="0" id="333" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/controlplane/services/dualstack" tests="0" failures="0" errors="0" id="334" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/controlplane/services/graceful-termination" tests="0" failures="0" errors="0" id="335" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/controlplane/services/helpers" tests="0" failures="0" errors="0" id="336" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/controlplane/services/nodeport" tests="0" failures="0" errors="0" id="337" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/controlplane/suite" tests="0" failures="0" errors="0" id="338" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/ginkgo-ext" tests="0" failures="0" errors="0" id="339" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/helpers" tests="0" failures="0" errors="0" id="340" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/helpers/constants" tests="0" failures="0" errors="0" id="341" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/k8s" tests="0" failures="0" errors="0" id="342" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/logger" tests="0" failures="0" errors="0" id="343" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/test/runtime" tests="0" failures="0" errors="0" id="344" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/tools/alignchecker" tests="0" failures="0" errors="0" id="345" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/tools/crdcheck" tests="0" failures="0" errors="0" id="346" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/tools/dev-doctor" tests="0" failures="0" errors="0" id="347" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/tools/licensegen" tests="0" failures="0" errors="0" id="348" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/tools/mount" tests="0" failures="0" errors="0" id="349" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/tools/spdxconv" tests="0" failures="0" errors="0" id="350" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/tools/sysctlfix" tests="0" failures="0" errors="0" id="351" hostname="kind-bpf-next" time="0.000" timestamp="2023-05-31T11:45:55Z"></testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/linux" tests="57" failures="0" errors="0" id="352" hostname="kind-bpf-next" time="226.736" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/datapath/linux" time="226.690"></testcase>
		<testcase name="Test/linuxTestSuite" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.020"></testcase>
		<testcase name="Test/linuxTestSuite/TestCreateNodeRoute" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.010"></testcase>
		<testcase name="Test/linuxTestSuite/TestCreateNodeRouteSpecMtu" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="Test/linuxTestSuite/TestNewDatapath" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.010"></testcase>
		<testcase name="Test/linuxTestSuite/TestStoreLoadNeighLinks" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="Test/linuxTestSuite/TestTunnelCIDRUpdateRequired" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="Test/DevicesSuite" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.100"></testcase>
		<testcase name="Test/DevicesSuite/TestDetect" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.050">
			<system-out><![CDATA[level=info msg="Detected devices" devices="[]" subsys=linux-datapath
level=info msg="Detected devices" devices="[dummy0]" subsys=linux-datapath
level=info msg="Detected devices" devices="[dummy0]" subsys=linux-datapath
level=info msg="Direct routing device detected" direct-routing-device=dummy1 subsys=linux-datapath
level=info msg="Detected devices" devices="[dummy0 dummy1 dummy2]" subsys=linux-datapath
level=info msg="Direct routing device detected" direct-routing-device=cilium_foo subsys=linux-datapath
level=info msg="IPv6 multicast device detected" ipv6-mcast-device=cilium_foo subsys=linux-datapath
level=info msg="Detected devices" devices="[cilium_foo dummy0 dummy1 dummy2]" subsys=linux-datapath
level=info msg="Direct routing device detected" direct-routing-device=cilium_foo subsys=linux-datapath
level=info msg="Detected devices" devices="[cilium_foo dummy0 dummy1 dummy2]" subsys=linux-datapath
level=info msg="Detected devices" devices="[cilium_foo dummy0 dummy1 dummy2 veth0]" subsys=linux-datapath
level=info msg="Detected devices" devices="[cilium_foo dummy0 dummy1 dummy2 dummy3 veth0]" subsys=linux-datapath
level=info msg="Detected devices" devices="[cilium_foo dummy0 dummy1 dummy2 dummy3 veth0]" subsys=linux-datapath
level=info msg="Detected devices" devices="[cilium_foo dummy0 dummy1 dummy2 veth0]" subsys=linux-datapath
level=info msg="Detected devices" devices="[bond0 cilium_foo dummy0 dummy1 veth0]" subsys=linux-datapath]]></system-out>
		</testcase>
		<testcase name="Test/DevicesSuite/TestExpandDevices" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="Test/DevicesSuite/TestExpandDirectRoutingDevice" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="Test/DevicesSuite/TestListenAfterDelete" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.020">
			<system-out><![CDATA[level=info msg="Detected devices" devices="[dummy0 dummy1]" subsys=linux-datapath
level=info msg="Devices changed" devices="[dummy0]" subsys=linux-datapath
level=info msg="Listening for device changes" subsys=linux-datapath]]></system-out>
		</testcase>
		<testcase name="Test/DevicesSuite/TestListenForNewDevices" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.020">
			<system-out><![CDATA[level=info msg="Listening for device changes" subsys=linux-datapath
level=info msg="Devices changed" devices="[dummy0]" subsys=linux-datapath
level=info msg="Devices changed" devices="[dummy0 dummy1]" subsys=linux-datapath
level=info msg="Devices changed" devices="[dummy0 dummy1 veth0]" subsys=linux-datapath
level=info msg="Devices changed" devices="[dummy1 veth0]" subsys=linux-datapath]]></system-out>
		</testcase>
		<testcase name="Test/DevicesSuite/TestListenForNewDevicesFiltered" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000">
			<system-out><![CDATA[level=info msg="Listening for device changes" subsys=linux-datapath
level=info msg="Devices changed" devices="[dummy0]" subsys=linux-datapath
level=info msg="Devices changed" devices="[dummy0 dummy1]" subsys=linux-datapath]]></system-out>
		</testcase>
		<testcase name="Test/linuxPrivilegedIPv6OnlyTestSuite" classname="github.com/cilium/cilium/pkg/datapath/linux" time="117.810"></testcase>
		<testcase name="Test/linuxPrivilegedIPv6OnlyTestSuite/TestAgentRestartOptionChanges" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.050">
			<system-out><![CDATA[level=info msg="Unpinning map with incompatible properties" file-path=/sys/fs/bpf/tc/globals/cilium_tunnel_map new="Type:Hash KeySize:20 ValueSize:20 MaxEntries:65536 Flags:1" old="Type:Hash KeySize:20 ValueSize:20 MaxEntries:65536 Flags:0" subsys=bpf]]></system-out>
		</testcase>
		<testcase name="Test/linuxPrivilegedIPv6OnlyTestSuite/TestArpPingHandling" classname="github.com/cilium/cilium/pkg/datapath/linux" time="115.230">
			<system-out><![CDATA[level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath
level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath
level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath
level=info msg="Successfully removed non-GC'ed neighbor entries previously installed by cilium-agent" count=1 subsys=linux-datapath
level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath
level=info msg="Successfully removed non-GC'ed neighbor entries previously installed by cilium-agent" count=1 subsys=linux-datapath
level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath]]></system-out>
		</testcase>
		<testcase name="Test/linuxPrivilegedIPv6OnlyTestSuite/TestArpPingHandlingForMultiDevice" classname="github.com/cilium/cilium/pkg/datapath/linux" time="2.200"></testcase>
		<testcase name="Test/linuxPrivilegedIPv6OnlyTestSuite/TestAuxiliaryPrefixes" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.050"></testcase>
		<testcase name="Test/linuxPrivilegedIPv6OnlyTestSuite/TestNodeUpdateDirectRouting" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.090">
			<system-out><![CDATA[level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath]]></system-out>
		</testcase>
		<testcase name="Test/linuxPrivilegedIPv6OnlyTestSuite/TestNodeUpdateEncapsulation" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.040"></testcase>
		<testcase name="Test/linuxPrivilegedIPv6OnlyTestSuite/TestNodeValidationDirectRouting" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.030"></testcase>
		<testcase name="Test/linuxPrivilegedIPv6OnlyTestSuite/TestSingleClusterPrefix" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.050"></testcase>
		<testcase name="Test/linuxPrivilegedIPv6OnlyTestSuite/TestUpdateNodeRoute" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.060"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4OnlyTestSuite" classname="github.com/cilium/cilium/pkg/datapath/linux" time="108.380"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4OnlyTestSuite/TestAgentRestartOptionChanges" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.050"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4OnlyTestSuite/TestArpPingHandling" classname="github.com/cilium/cilium/pkg/datapath/linux" time="107.910">
			<system-out><![CDATA[level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath
level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath
level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath
level=info msg="Successfully removed non-GC'ed neighbor entries previously installed by cilium-agent" count=1 subsys=linux-datapath
level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath
level=info msg="Successfully removed non-GC'ed neighbor entries previously installed by cilium-agent" count=1 subsys=linux-datapath
level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath]]></system-out>
		</testcase>
		<testcase name="Test/linuxPrivilegedIPv4OnlyTestSuite/TestArpPingHandlingForMultiDevice" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.110"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4OnlyTestSuite/TestAuxiliaryPrefixes" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.050"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4OnlyTestSuite/TestNodeUpdateDirectRouting" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.080">
			<system-out><![CDATA[level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath]]></system-out>
		</testcase>
		<testcase name="Test/linuxPrivilegedIPv4OnlyTestSuite/TestNodeUpdateEncapsulation" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.050"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4OnlyTestSuite/TestNodeValidationDirectRouting" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.040"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4OnlyTestSuite/TestSingleClusterPrefix" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.040"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4OnlyTestSuite/TestUpdateNodeRoute" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.040"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4AndIPv6TestSuite" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.380"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4AndIPv6TestSuite/TestAgentRestartOptionChanges" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.040"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4AndIPv6TestSuite/TestAuxiliaryPrefixes" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.040"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4AndIPv6TestSuite/TestNodeUpdateDirectRouting" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.110">
			<system-out><![CDATA[level=warning msg="Attempted to deallocate a node ID that wasn't allocated" nodeID=0 subsys=linux-datapath]]></system-out>
		</testcase>
		<testcase name="Test/linuxPrivilegedIPv4AndIPv6TestSuite/TestNodeUpdateEncapsulation" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.050"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4AndIPv6TestSuite/TestNodeValidationDirectRouting" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.040"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4AndIPv6TestSuite/TestSingleClusterPrefix" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.040"></testcase>
		<testcase name="Test/linuxPrivilegedIPv4AndIPv6TestSuite/TestUpdateNodeRoute" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.050"></testcase>
		<testcase name="TestFilterLocalAddresses" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/simple" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/multiple" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/scopeMaxLink" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/scopeMaxHost" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/scopeMaxNowhere" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/exclude" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/excludeMultiple" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/ipv6_simple" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/ipv6_multiple" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/v4/v6_mix" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/v6_exclude" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/include_link-local_v4" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="TestFilterLocalAddresses/include_link-local_v6" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<testcase name="FuzzNodeHandler" classname="github.com/cilium/cilium/pkg/datapath/linux" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/linux	coverage: 53.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/linux/config" tests="9" failures="0" errors="0" id="353" hostname="kind-bpf-next" time="0.134" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/datapath/linux/config" time="0.100"></testcase>
		<testcase name="Test/ConfigSuite" classname="github.com/cilium/cilium/pkg/datapath/linux/config" time="0.100"></testcase>
		<testcase name="Test/ConfigSuite/TestVLANBypassConfig" classname="github.com/cilium/cilium/pkg/datapath/linux/config" time="0.100">
			<system-out><![CDATA[level=info msg="Using autogenerated IPv4 allocation range" subsys=node v4Prefix=10.15.0.0/16
level=info msg="Using autogenerated IPv6 allocation range" subsys=node v6Prefix="f00d::a0f:0:0:0/96"]]></system-out>
		</testcase>
		<testcase name="Test/ConfigSuite/TestWriteEndpointConfig" classname="github.com/cilium/cilium/pkg/datapath/linux/config" time="0.000"></testcase>
		<testcase name="Test/ConfigSuite/TestWriteNetdevConfig" classname="github.com/cilium/cilium/pkg/datapath/linux/config" time="0.000"></testcase>
		<testcase name="Test/ConfigSuite/TestWriteNodeConfig" classname="github.com/cilium/cilium/pkg/datapath/linux/config" time="0.000"></testcase>
		<testcase name="Test/ConfigSuite/TestWriteStaticData" classname="github.com/cilium/cilium/pkg/datapath/linux/config" time="0.000"></testcase>
		<testcase name="Test/ConfigSuite/TestdefineIPv6" classname="github.com/cilium/cilium/pkg/datapath/linux/config" time="0.000"></testcase>
		<testcase name="Test/ConfigSuite/TestdefineMAC" classname="github.com/cilium/cilium/pkg/datapath/linux/config" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/linux/config	coverage: 54.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/linux/ethtool" tests="1" failures="0" errors="0" id="354" hostname="kind-bpf-next" time="0.022" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestIsVirtualDriver" classname="github.com/cilium/cilium/pkg/datapath/linux/ethtool" time="0.000">
			<system-out><![CDATA[    ethtool_linux_test.go:28: IsVirtualDriver("enp0s2") = false
    ethtool_linux_test.go:28: IsVirtualDriver("sit0") = false
    ethtool_linux_test.go:28: IsVirtualDriver("docker0") = false
    ethtool_linux_test.go:28: IsVirtualDriver("cilium_vxlan") = false
    ethtool_linux_test.go:28: IsVirtualDriver("vethdc7a1a6") = true
    ethtool_linux_test.go:28: IsVirtualDriver("veth73d9c63") = true
    ethtool_linux_test.go:28: IsVirtualDriver("cilium_net") = true
    ethtool_linux_test.go:28: IsVirtualDriver("cilium_host") = true
    ethtool_linux_test.go:28: IsVirtualDriver("lxc_health") = true
    ethtool_linux_test.go:28: IsVirtualDriver("cilium-bgp") = false
    ethtool_linux_test.go:28: IsVirtualDriver("instance1") = false
    ethtool_linux_test.go:28: IsVirtualDriver("instance2") = false
    ethtool_linux_test.go:28: IsVirtualDriver("dummy_external") = false
    ethtool_linux_test.go:28: IsVirtualDriver("dummy_host") = false
    ethtool_linux_test.go:28: IsVirtualDriver("veth0") = true]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/linux/ethtool	coverage: 87.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/linux/ipsec" tests="10" failures="0" errors="0" id="355" hostname="kind-bpf-next" time="0.058" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/datapath/linux/ipsec" time="0.020"></testcase>
		<testcase name="Test/IPSecSuitePrivileged" classname="github.com/cilium/cilium/pkg/datapath/linux/ipsec" time="0.020"></testcase>
		<testcase name="Test/IPSecSuitePrivileged/TestInvalidLoadKeys" classname="github.com/cilium/cilium/pkg/datapath/linux/ipsec" time="0.000"></testcase>
		<testcase name="Test/IPSecSuitePrivileged/TestLoadKeys" classname="github.com/cilium/cilium/pkg/datapath/linux/ipsec" time="0.000">
			<system-out><![CDATA[level=warning msg="IPsec secrets with an IP address as the last argument are deprecated and will be unsupported in v1.13." subsys=ipsec]]></system-out>
		</testcase>
		<testcase name="Test/IPSecSuitePrivileged/TestLoadKeysNoFile" classname="github.com/cilium/cilium/pkg/datapath/linux/ipsec" time="0.000">
			<system-out><![CDATA[level=info msg="Loading IPsec keyfile" file-path=ipsec_keys_test subsys=ipsec]]></system-out>
		</testcase>
		<testcase name="Test/IPSecSuitePrivileged/TestUpsertIPSecEndpoint" classname="github.com/cilium/cilium/pkg/datapath/linux/ipsec" time="0.020"></testcase>
		<testcase name="Test/IPSecSuitePrivileged/TestUpsertIPSecEquals" classname="github.com/cilium/cilium/pkg/datapath/linux/ipsec" time="0.000"></testcase>
		<testcase name="Test/IPSecSuitePrivileged/TestUpsertIPSecKeyMissing" classname="github.com/cilium/cilium/pkg/datapath/linux/ipsec" time="0.000"></testcase>
		<testcase name="Test/XFRMCollectorTest" classname="github.com/cilium/cilium/pkg/datapath/linux/ipsec" time="0.000"></testcase>
		<testcase name="Test/XFRMCollectorTest/Test_xfrmCollector_Collect" classname="github.com/cilium/cilium/pkg/datapath/linux/ipsec" time="0.000">
			<system-out><![CDATA[level=error msg="Error while getting xfrm stats" error="error due to some reason" subsys=ipsec
level=error msg="Error while getting xfrm stats" error="error due to some reason" subsys=ipsec
level=error msg="Error while getting xfrm stats" error="error due to some reason" subsys=ipsec]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/linux/ipsec	coverage: 45.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/linux/probes" tests="20" failures="0" errors="0" id="356" hostname="kind-bpf-next" time="0.198" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestKernelHZ" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.100"></testcase>
		<testcase name="TestNearest" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestNearest/single_value" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestNearest/equal_distance_to_multiple_values" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestNearest/in_higher_than_last_value" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestNearest/in_lower_than_first_value" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestNearest/in_max_value" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestKtimeInterpolate" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestKtimeInterpolate/10_jiffies_over_10_milliseconds_=_1000_hz" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestKtimeInterpolate/100_jiffies_over_123_milliseconds_=_813_hz" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestKtimeInterpolate/1_jiffy_over_1_second_=_1_hz" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestKtimeInterpolate/0_jiffies_over_1_second_=_0_hz" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestKtimeInterpolateErrors" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestManagedNeighbors" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestSystemConfigProbes" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000">
			<system-out><![CDATA[level=warning msg="CONFIG_CGROUP_BPF optional kernel parameter is not in kernel (needed for: Host Reachable Services and Sockmap optimization)" subsys=probes
level=warning msg="CONFIG_LWTUNNEL_BPF optional kernel parameter is not in kernel (needed for: Lightweight Tunnel hook for IP-in-IP encapsulation)" subsys=probes
level=warning msg="CONFIG_BPF_EVENTS optional kernel parameter is not in kernel (needed for: Visibility and congestion management with datapath)" subsys=probes
level=warning msg="CONFIG_CGROUP_BPF optional kernel parameter is not in kernel (needed for: Host Reachable Services and Sockmap optimization)" subsys=probes
level=warning msg="CONFIG_LWTUNNEL_BPF optional kernel parameter is not in kernel (needed for: Lightweight Tunnel hook for IP-in-IP encapsulation)" subsys=probes
level=warning msg="CONFIG_BPF_EVENTS optional kernel parameter is not in kernel (needed for: Visibility and congestion management with datapath)" subsys=probes]]></system-out>
		</testcase>
		<testcase name="TestWriteFeatureHeader" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestExecuteSystemConfigProbes" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.080">
			<system-out><![CDATA[level=warning msg="CONFIG_LWTUNNEL_BPF optional kernel parameter is not in kernel (needed for: Lightweight Tunnel hook for IP-in-IP encapsulation)" subsys=probes]]></system-out>
		</testcase>
		<testcase name="TestExecuteHeaderProbes" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestOuterSourceIPProbe" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<testcase name="TestIPv6Support" classname="github.com/cilium/cilium/pkg/datapath/linux/probes" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/linux/probes	coverage: 63.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/linux/route" tests="10" failures="0" errors="0" id="357" hostname="kind-bpf-next" time="0.025" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/datapath/linux/route" time="0.010"></testcase>
		<testcase name="Test/RouteSuitePrivileged" classname="github.com/cilium/cilium/pkg/datapath/linux/route" time="0.010"></testcase>
		<testcase name="Test/RouteSuitePrivileged/TestListRules" classname="github.com/cilium/cilium/pkg/datapath/linux/route" time="0.010"></testcase>
		<testcase name="Test/RouteSuitePrivileged/TestReplaceNexthopRoute" classname="github.com/cilium/cilium/pkg/datapath/linux/route" time="0.000"></testcase>
		<testcase name="Test/RouteSuitePrivileged/TestReplaceRoute" classname="github.com/cilium/cilium/pkg/datapath/linux/route" time="0.000"></testcase>
		<testcase name="Test/RouteSuitePrivileged/TestReplaceRule" classname="github.com/cilium/cilium/pkg/datapath/linux/route" time="0.000"></testcase>
		<testcase name="Test/RouteSuitePrivileged/TestReplaceRule6" classname="github.com/cilium/cilium/pkg/datapath/linux/route" time="0.000"></testcase>
		<testcase name="Test/RouteSuitePrivileged/TestRule_String" classname="github.com/cilium/cilium/pkg/datapath/linux/route" time="0.000"></testcase>
		<testcase name="Test/RouteSuite" classname="github.com/cilium/cilium/pkg/datapath/linux/route" time="0.000"></testcase>
		<testcase name="Test/RouteSuite/TestToIPCommand" classname="github.com/cilium/cilium/pkg/datapath/linux/route" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/linux/route	coverage: 69.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/linux/routing" tests="13" failures="0" errors="0" id="358" hostname="kind-bpf-next" time="0.125" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.100"></testcase>
		<testcase name="Test/MigrateSuite" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.050"></testcase>
		<testcase name="Test/MigrateSuite/TestMigrateENIDatapathDowngradeFailure" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.010">
			<system-out><![CDATA[level=warning msg="Failed to downgrade endpoint to original ENI datapath. Previous datapath is still intact and endpoint connectivity is not affected." error="failed to create new rule: unable to add new rule: fake error" rule="ip rule 111: from 172.16.1.0/24 to all table 11" subsys=linux-routing]]></system-out>
		</testcase>
		<testcase name="Test/MigrateSuite/TestMigrateENIDatapathDowngradeSuccess" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.010"></testcase>
		<testcase name="Test/MigrateSuite/TestMigrateENIDatapathPartial" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.010"></testcase>
		<testcase name="Test/MigrateSuite/TestMigrateENIDatapathUpgradeFailure" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.010">
			<system-out><![CDATA[level=warning msg="Failed to migrate endpoint to new ENI datapath. Previous datapath is still intact and endpoint connectivity is not affected." error="failed to delete old rule: fake error" rule="ip rule 110: from 172.16.1.0/24 to all table 5" subsys=linux-routing]]></system-out>
		</testcase>
		<testcase name="Test/MigrateSuite/TestMigrateENIDatapathUpgradeSuccess" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.010"></testcase>
		<testcase name="Test/LinuxRoutingSuite" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.060"></testcase>
		<testcase name="Test/LinuxRoutingSuite/TestConfigure" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.020"></testcase>
		<testcase name="Test/LinuxRoutingSuite/TestConfigureRoutewithIncompatibleIP" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.000">
			<system-out><![CDATA[level=warning msg="Unable to configure rules and routes because IP is not an IPv4 address" endpointIP="fd00::2" subsys=linux-routing]]></system-out>
		</testcase>
		<testcase name="Test/LinuxRoutingSuite/TestDelete" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.040">
			<system-out><![CDATA[level=warning msg="No rule matching found" rule="20: from all to 192.168.2.233/32 lookup main proto unspec" subsys=linux-routing
level=warning msg="Found too many rules matching, skipping deletion" candidates="[ip rule 20: from all to 192.168.2.233/32 table 254 ip rule 20: from 192.168.2.123/32 to 192.168.2.233/32 table 254]" rule="20: from all to 192.168.2.233/32 lookup main proto unspec" subsys=linux-routing
level=warning msg="Found too many rules matching, skipping deletion" candidates="[ip rule 111: from 192.168.2.123/32 to 192.168.0.0/16 table 11 ip rule 111: from 192.168.2.123/32 to 192.170.0.0/16 table 11]" rule="111: from 192.168.2.123/32 to all lookup 0 proto unspec" subsys=linux-routing]]></system-out>
		</testcase>
		<testcase name="Test/LinuxRoutingSuite/TestDeleteRoutewithIncompatibleIP" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.000">
			<system-out><![CDATA[level=warning msg="Unable to delete rules because IP is not an IPv4 address" endpointIP="fd00::2" subsys=linux-routing]]></system-out>
		</testcase>
		<testcase name="Test/LinuxRoutingSuite/TestParse" classname="github.com/cilium/cilium/pkg/datapath/linux/routing" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/linux/routing	coverage: 65.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/linux/utime" tests="4" failures="0" errors="0" id="359" hostname="kind-bpf-next" time="0.267" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/datapath/linux/utime" time="0.250"></testcase>
		<testcase name="Test/utimeSuite" classname="github.com/cilium/cilium/pkg/datapath/linux/utime" time="0.250"></testcase>
		<testcase name="Test/utimeSuite/TestGetBoottime" classname="github.com/cilium/cilium/pkg/datapath/linux/utime" time="0.000">
			<system-out><![CDATA[level=info msg="Adjusted boot time: 2023-05-31 11:34:18.00000021 +0000 UTC" subsys=utime]]></system-out>
		</testcase>
		<testcase name="Test/utimeSuite/TestUTime" classname="github.com/cilium/cilium/pkg/datapath/linux/utime" time="0.250"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/linux/utime	coverage: 57.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/loader" tests="13" failures="0" errors="0" id="360" hostname="kind-bpf-next" time="3.976" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/datapath/loader" time="3.950"></testcase>
		<testcase name="Test/LoaderTestSuite" classname="github.com/cilium/cilium/pkg/datapath/loader" time="3.930"></testcase>
		<testcase name="Test/LoaderTestSuite/TestCompileAndLoadDefaultEndpoint" classname="github.com/cilium/cilium/pkg/datapath/loader" time="0.350">
			<system-out><![CDATA[level=info msg="Using autogenerated IPv4 allocation range" subsys=node v4Prefix=10.15.0.0/16
level=info msg="Using autogenerated IPv6 allocation range" subsys=node v6Prefix="f00d::a0f:0:0:0/96"]]></system-out>
		</testcase>
		<testcase name="Test/LoaderTestSuite/TestCompileAndLoadHostEndpoint" classname="github.com/cilium/cilium/pkg/datapath/loader" time="0.320">
			<system-out><![CDATA[level=warning msg="Skipping symbol substitution" subsys=elf symbol=ROUTER_IP_1
level=warning msg="Skipping symbol substitution" subsys=elf symbol=ROUTER_IP_2
level=warning msg="Skipping symbol substitution" subsys=elf symbol=HOST_IP_1
level=warning msg="Skipping symbol substitution" subsys=elf symbol=HOST_IP_2
level=warning msg="Skipping symbol substitution" subsys=elf symbol=LXC_IP_1
level=warning msg="Skipping symbol substitution" subsys=elf symbol=LXC_IP_2
level=warning msg="Skipping symbol substitution" subsys=elf symbol=LXC_ID
level=warning msg="Skipping symbol substitution" subsys=elf symbol=test_cilium_policy
level=warning msg="Skipping symbol substitution" subsys=elf symbol=to-netdev
level=warning msg="Skipping symbol substitution" subsys=elf symbol=from-netdev
level=warning msg="Skipping symbol substitution" subsys=elf symbol=.BTF.ext
level=warning msg="Skipping symbol substitution" subsys=elf symbol=to-host
level=warning msg="Skipping symbol substitution" subsys=elf symbol=from-host
level=warning msg="Skipping symbol substitution" subsys=elf symbol=test_cilium_events
level=warning msg="Skipping symbol substitution" subsys=elf symbol=test_cilium_signals
level=warning msg="Skipping symbol substitution" subsys=elf symbol=.debug_ranges
level=warning msg="Skipping symbol substitution" subsys=elf symbol=test_cilium_metrics
level=warning msg="Skipping symbol substitution" subsys=elf symbol=test_cilium_node_map
level=warning msg="Skipping symbol substitution" subsys=elf symbol=.debug_info
level=warning msg="Skipping symbol substitution" subsys=elf symbol=test_cilium_auth
level=warning msg="Skipping symbol substitution" subsys=elf symbol=test_cilium_runtime_config
level=warning msg="Skipping symbol substitution" subsys=elf symbol=test_cilium_encrypt_state
level=warning msg="Skipping symbol substitution" subsys=elf symbol=.debug_line
level=warning msg="Skipping symbol substitution" subsys=elf symbol=.debug_frame
level=warning msg="Skipping symbol substitution" subsys=elf symbol=test_cilium_ipcache
level=warning msg="Skipping symbol substitution" subsys=elf symbol=test_cilium_lxc
level=warning msg="Skipping symbol substitution" subsys=elf symbol=.debug_loc
level=warning msg="Skipping symbol substitution" subsys=elf symbol=.BTF
level=warning msg="Skipping symbol substitution" subsys=elf symbol=2/1]]></system-out>
		</testcase>
		<testcase name="Test/LoaderTestSuite/TestCompileFailureDefaultEndpoint" classname="github.com/cilium/cilium/pkg/datapath/loader" time="0.100"></testcase>
		<testcase name="Test/LoaderTestSuite/TestCompileFailureHostEndpoint" classname="github.com/cilium/cilium/pkg/datapath/loader" time="0.100"></testcase>
		<testcase name="Test/LoaderTestSuite/TestReload" classname="github.com/cilium/cilium/pkg/datapath/loader" time="0.280"></testcase>
		<testcase name="Test/LoaderTestSuite/TestWrap" classname="github.com/cilium/cilium/pkg/datapath/loader" time="0.000"></testcase>
		<testcase name="Test/LoaderTestSuite/TesthashDatapath" classname="github.com/cilium/cilium/pkg/datapath/loader" time="0.000"></testcase>
		<testcase name="Test/LoaderTestSuite/TestobjectCache" classname="github.com/cilium/cilium/pkg/datapath/loader" time="0.710">
			<system-out><![CDATA[level=info msg="Compiled new BPF template" BPFCompilationTime=287.886015ms file-path=/tmp/cilium_test458225064/templates/ceb5f7032932da2fcc8bd92ffbae00bd5102d25b21d6b320854b87caf56c11f2/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=415.597821ms file-path=/tmp/cilium_test458225064/templates/e95fa830ff9ff61785648b1404bbae5c4d7f9a3c4bc54d4cc2188d30dc309e53/bpf_lxc.o subsys=datapath-loader
level=warning msg="Failed to watch templates directory" error="no such file or directory" file-path=/tmp/cilium_test458225064/templates/e95fa830ff9ff61785648b1404bbae5c4d7f9a3c4bc54d4cc2188d30dc309e53/bpf_lxc.o subsys=datapath-loader]]></system-out>
		</testcase>
		<testcase name="Test/LoaderTestSuite/TestobjectCacheParallel" classname="github.com/cilium/cilium/pkg/datapath/loader" time="2.060">
			<system-out><![CDATA[level=info msg="Compiled new BPF template" BPFCompilationTime=439.860086ms file-path=/tmp/cilium_test2260190995/templates/238caece70940d3c5dff696dbf5e5bcb8e7a8bbdde19a2e375966a13ef011329/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=470.522438ms file-path=/tmp/cilium_test2260190995/templates/faf3bb662f1495d789d31645bf39a08217125f8c688cd68b042a382a06830826/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=495.437447ms file-path=/tmp/cilium_test2260190995/templates/83f6d1b19838b012771df828991b50bb258e6eed19b42f7b573296899e4aa516/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=505.185116ms file-path=/tmp/cilium_test2260190995/templates/238caece70940d3c5dff696dbf5e5bcb8e7a8bbdde19a2e375966a13ef011329/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=508.540133ms file-path=/tmp/cilium_test2260190995/templates/1d97ed644546c41e89c5adbc3c69c96a57218ed504c81e88b6e1a28738ce6141/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=846.596184ms file-path=/tmp/cilium_test2260190995/templates/83f6d1b19838b012771df828991b50bb258e6eed19b42f7b573296899e4aa516/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=888.069582ms file-path=/tmp/cilium_test2260190995/templates/238caece70940d3c5dff696dbf5e5bcb8e7a8bbdde19a2e375966a13ef011329/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=935.134344ms file-path=/tmp/cilium_test2260190995/templates/a5d48b295187393634b5667ddaccedd1c37882612fd06efb09d0c5056ae81166/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=958.827666ms file-path=/tmp/cilium_test2260190995/templates/f2027ad0976fd644de95b250758c7f6813bc3f2cb8e3afb09cbf28be4d52b40a/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=1.035661983s file-path=/tmp/cilium_test2260190995/templates/c1954515763a9dacb12850bc8f80499b20eefe6e194ee1322916a6c0d7eccff2/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=1.080128373s file-path=/tmp/cilium_test2260190995/templates/faf3bb662f1495d789d31645bf39a08217125f8c688cd68b042a382a06830826/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=1.093395456s file-path=/tmp/cilium_test2260190995/templates/1d97ed644546c41e89c5adbc3c69c96a57218ed504c81e88b6e1a28738ce6141/bpf_lxc.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=1.1081417s file-path=/tmp/cilium_test2260190995/templates/c2c1acb5a6bf4a403d7a078dba5e51afb9e8c820c58fd63cba04499e3edf2752/bpf_lxc.o subsys=datapath-loader]]></system-out>
		</testcase>
		<testcase name="Test/NetlinkTestSuite" classname="github.com/cilium/cilium/pkg/datapath/loader" time="0.020"></testcase>
		<testcase name="Test/NetlinkTestSuite/TestSetupDev" classname="github.com/cilium/cilium/pkg/datapath/loader" time="0.020">
			<system-out><![CDATA[level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv6.conf.dummy9.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.dummy9.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.dummy9.rp_filter sysParamValue=0
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.dummy9.accept_local sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.dummy9.send_redirects sysParamValue=0]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/loader	coverage: 36.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/datapath/maps" tests="3" failures="0" errors="0" id="361" hostname="kind-bpf-next" time="0.029" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/datapath/maps" time="0.000"></testcase>
		<testcase name="Test/MapTestSuite" classname="github.com/cilium/cilium/pkg/datapath/maps" time="0.000"></testcase>
		<testcase name="Test/MapTestSuite/TestCollectStaleMapGarbage" classname="github.com/cilium/cilium/pkg/datapath/maps" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/datapath/maps	coverage: 37.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/debug" tests="3" failures="0" errors="0" id="362" hostname="kind-bpf-next" time="0.006" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/debug" time="0.000"></testcase>
		<testcase name="Test/DebugTestSuite" classname="github.com/cilium/cilium/pkg/debug" time="0.000"></testcase>
		<testcase name="Test/DebugTestSuite/TestSubsystem" classname="github.com/cilium/cilium/pkg/debug" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/debug	coverage: 85.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/egressgateway" tests="4" failures="0" errors="0" id="363" hostname="kind-bpf-next" time="0.121" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/egressgateway" time="0.060"></testcase>
		<testcase name="Test/EgressGatewayTestSuite" classname="github.com/cilium/cilium/pkg/egressgateway" time="0.060">
			<system-out><![CDATA[level=info msg="Detected mounted BPF filesystem at /sys/fs/bpf" subsys=bpf]]></system-out>
		</testcase>
		<testcase name="Test/EgressGatewayTestSuite/TestEgressGatewayManager" classname="github.com/cilium/cilium/pkg/egressgateway" time="0.060">
			<system-out><![CDATA[level=info msg=Invoked duration="167.281µs" function="egressgateway.(*EgressGatewayTestSuite).SetUpTest.func5 (manager_privileged_test.go:142)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="254.244µs" function="egressmap.createPolicyMap.func1 (policy.go:92)" subsys=hive
level=info msg="Start hook executed" duration=1.588279ms function="egressgateway.NewEgressGatewayManager.func1 (manager.go:146)" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="3.637µs" function="egressgateway.NewEgressGatewayManager.func2 (manager.go:154)" subsys=hive
level=info msg="Stop hook executed" duration="7.263µs" function="egressmap.createPolicyMap.func2 (policy.go:95)" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestCell" classname="github.com/cilium/cilium/pkg/egressgateway" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/egressgateway	coverage: 68.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/elf" tests="2" failures="0" errors="0" id="364" hostname="kind-bpf-next" skipped="1" time="0.031" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/elf" time="0.000"></testcase>
		<testcase name="Test/ELFTestSuite" classname="github.com/cilium/cilium/pkg/elf" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/elf	coverage: 0.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/endpoint" tests="7" failures="0" errors="0" id="365" hostname="kind-bpf-next" skipped="2" time="0.066" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/endpoint" time="0.000"></testcase>
		<testcase name="Test/EndpointSuite" classname="github.com/cilium/cilium/pkg/endpoint" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/RedirectSuite" classname="github.com/cilium/cilium/pkg/endpoint" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="TestEndpoint_GetK8sPodLabels" classname="github.com/cilium/cilium/pkg/endpoint" time="0.000"></testcase>
		<testcase name="TestEndpoint_GetK8sPodLabels/has_all_k8s_labels" classname="github.com/cilium/cilium/pkg/endpoint" time="0.000"></testcase>
		<testcase name="TestEndpoint_GetK8sPodLabels/the_namespace_labels,_service_account_and_namespace_should_be_ignored_as_they_don&#39;t_belong_to_pod_labels" classname="github.com/cilium/cilium/pkg/endpoint" time="0.000"></testcase>
		<testcase name="TestEndpoint_GetK8sPodLabels/labels_with_other_source_than_k8s_should_also_be_ignored" classname="github.com/cilium/cilium/pkg/endpoint" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/endpoint	coverage: 0.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/endpoint/id" tests="5" failures="0" errors="0" id="366" hostname="kind-bpf-next" time="0.006" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/endpoint/id" time="0.000"></testcase>
		<testcase name="Test/IDSuite" classname="github.com/cilium/cilium/pkg/endpoint/id" time="0.000"></testcase>
		<testcase name="Test/IDSuite/TestNewIPPrefix" classname="github.com/cilium/cilium/pkg/endpoint/id" time="0.000"></testcase>
		<testcase name="Test/IDSuite/TestParse" classname="github.com/cilium/cilium/pkg/endpoint/id" time="0.000"></testcase>
		<testcase name="Test/IDSuite/TestSplitID" classname="github.com/cilium/cilium/pkg/endpoint/id" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/endpoint/id	coverage: 54.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/endpointmanager" tests="15" failures="0" errors="0" id="367" hostname="kind-bpf-next" time="0.160" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.110"></testcase>
		<testcase name="Test/EndpointManagerSuite" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.110"></testcase>
		<testcase name="Test/EndpointManagerSuite/TestErrInvalidPrefix_Error" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.000"></testcase>
		<testcase name="Test/EndpointManagerSuite/TestHasGlobalCT" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.000">
			<system-out><![CDATA[level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint]]></system-out>
		</testcase>
		<testcase name="Test/EndpointManagerSuite/TestIsErrInvalidPrefix" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.000"></testcase>
		<testcase name="Test/EndpointManagerSuite/TestIsErrUnsupportedID" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.000"></testcase>
		<testcase name="Test/EndpointManagerSuite/TestLookup" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.000">
			<system-out><![CDATA[level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1234 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1234 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="New endpoint" containerID=1234 datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=492 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=10 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=10 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=10 ipv4= ipv6= k8sPodName=default/foo subsys=endpoint
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=10 ipv4= ipv6= k8sPodName=/ subsys=endpoint]]></system-out>
		</testcase>
		<testcase name="Test/EndpointManagerSuite/TestLookupCiliumID" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.000">
			<system-out><![CDATA[level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint]]></system-out>
		</testcase>
		<testcase name="Test/EndpointManagerSuite/TestLookupContainerID" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.000">
			<system-out><![CDATA[level=info msg="New endpoint" containerID=foo datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3 ipv4= ipv6= k8sPodName=/ subsys=endpoint]]></system-out>
		</testcase>
		<testcase name="Test/EndpointManagerSuite/TestLookupIPv4" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.000">
			<system-out><![CDATA[level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint]]></system-out>
		</testcase>
		<testcase name="Test/EndpointManagerSuite/TestLookupPodName" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.000">
			<system-out><![CDATA[level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=5 ipv4= ipv6= k8sPodName=default/foo subsys=endpoint]]></system-out>
		</testcase>
		<testcase name="Test/EndpointManagerSuite/TestRemove" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.000">
			<system-out><![CDATA[level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint]]></system-out>
		</testcase>
		<testcase name="Test/EndpointManagerSuite/TestUpdateReferences" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.000">
			<system-out><![CDATA[level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint]]></system-out>
		</testcase>
		<testcase name="Test/EndpointManagerSuite/TestWaitForEndpointsAtPolicyRev" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.100">
			<system-out><![CDATA[level=info msg="New endpoint" containerID= datapathPolicyRevision=5 desiredPolicyRevision=0 endpointID=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="New endpoint" containerID= datapathPolicyRevision=5 desiredPolicyRevision=0 endpointID=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="New endpoint" containerID= datapathPolicyRevision=4 desiredPolicyRevision=0 endpointID=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint]]></system-out>
		</testcase>
		<testcase name="Test/EndpointManagerSuite/TestmarkAndSweep" classname="github.com/cilium/cilium/pkg/endpointmanager" time="0.000">
			<system-out><![CDATA[level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=5 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=7 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=2 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=warning msg="Stray endpoint found. You may be affected by upstream Kubernetes issue #86944." containerID= endpointID=2 k8sPodName=/ subsys=endpoint-manager url="https://github.com/kubernetes/kubernetes/issues/86944"]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/endpointmanager	coverage: 46.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/endpointmanager/idallocator" tests="5" failures="0" errors="0" id="368" hostname="kind-bpf-next" time="0.035" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/endpointmanager/idallocator" time="0.030"></testcase>
		<testcase name="Test/IDSuite" classname="github.com/cilium/cilium/pkg/endpointmanager/idallocator" time="0.030"></testcase>
		<testcase name="Test/IDSuite/TestAllocation" classname="github.com/cilium/cilium/pkg/endpointmanager/idallocator" time="0.020"></testcase>
		<testcase name="Test/IDSuite/TestRelease" classname="github.com/cilium/cilium/pkg/endpointmanager/idallocator" time="0.000"></testcase>
		<testcase name="Test/IDSuite/TestReuse" classname="github.com/cilium/cilium/pkg/endpointmanager/idallocator" time="0.010"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/endpointmanager/idallocator	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/envoy" tests="50" failures="0" errors="0" id="369" hostname="kind-bpf-next" skipped="2" time="0.146" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/envoy" time="0.060"></testcase>
		<testcase name="Test/AccessLogServerSuite" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/AccessLogServerSuite/TestKafkaLogMultipleTopics" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/AccessLogServerSuite/TestKafkaLogNoTopic" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/AccessLogServerSuite/TestKafkaLogSingleTopic" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/AccessLogServerSuite/TestParseURL" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/JSONSuite" classname="github.com/cilium/cilium/pkg/envoy" time="0.020"></testcase>
		<testcase name="Test/JSONSuite/TestCiliumEnvoyConfig" classname="github.com/cilium/cilium/pkg/envoy" time="0.010"></testcase>
		<testcase name="Test/JSONSuite/TestCiliumEnvoyConfigMulti" classname="github.com/cilium/cilium/pkg/envoy" time="0.010"></testcase>
		<testcase name="Test/JSONSuite/TestCiliumEnvoyConfigNoAddress" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/JSONSuite/TestCiliumEnvoyConfigSpec" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/JSONSuite/TestCiliumEnvoyConfigTCPProxy" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/JSONSuite/TestCiliumEnvoyConfigTCPProxyTermination" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/JSONSuite/TestCiliumEnvoyConfigValidation" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/JSONSuite/TestListenersAddedOrDeleted" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/EnvoySuite" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/EnvoySuite/TestEnvoy" classname="github.com/cilium/cilium/pkg/envoy" time="0.000">
			<skipped message="Skipped"><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
    embedded_envoy_test.go:54: skipping envoy unit test; CILIUM_ENABLE_ENVOY_UNIT_TEST not set]]></skipped>
		</testcase>
		<testcase name="Test/EnvoySuite/TestEnvoyNACK" classname="github.com/cilium/cilium/pkg/envoy" time="0.000">
			<skipped message="Skipped"><![CDATA[    embedded_envoy_test.go:135: skipping envoy unit test; CILIUM_ENABLE_ENVOY_UNIT_TEST not set]]></skipped>
		</testcase>
		<testcase name="Test/ResourcesSuite" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite" classname="github.com/cilium/cilium/pkg/envoy" time="0.030"></testcase>
		<testcase name="Test/ServerSuite/TestGetDirectionNetworkPolicy" classname="github.com/cilium/cilium/pkg/envoy" time="0.010"></testcase>
		<testcase name="Test/ServerSuite/TestGetHTTPRule" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicy" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicyDeny" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicyEgressNotEnforced" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicyIngressNotEnforced" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicyKafka" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicyL7" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicyMySQL" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicyNil" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicyProxylibVisibility" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicyTLSEgress" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicyTLSIngress" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicyWildcard" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetNetworkPolicyWildcardDeny" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/ServerSuite/TestGetPortNetworkPolicyRule" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/SortSuite" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/SortSuite/TestHandleIPUpsert" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/SortSuite/TestSortHeaderMatchers" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/SortSuite/TestSortHttpNetworkPolicyRules" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/SortSuite/TestSortPortNetworkPolicies" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test/SortSuite/TestSortPortNetworkPolicyRules" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test_getPublicListenerAddress" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test_getPublicListenerAddress/IPv4_only" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test_getPublicListenerAddress/IPv6_only" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test_getPublicListenerAddress/IPv4_and_IPv6" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test_getLocalListenerAddresses" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test_getLocalListenerAddresses/IPv4_only" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test_getLocalListenerAddresses/IPv6_only" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<testcase name="Test_getLocalListenerAddresses/IPv4_and_IPv6" classname="github.com/cilium/cilium/pkg/envoy" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/envoy	coverage: 33.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/envoy/xds" tests="23" failures="0" errors="0" id="370" hostname="kind-bpf-next" time="4.532" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/envoy/xds" time="4.520"></testcase>
		<testcase name="Test/AckSuite" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/AckSuite/TestDeleteMultipleNodes" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/AckSuite/TestDeleteSingleNode" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/AckSuite/TestRevertDelete" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/AckSuite/TestRevertInsert" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/AckSuite/TestRevertUpdate" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/AckSuite/TestUpsertMoreRecentVersion" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/AckSuite/TestUpsertMoreRecentVersionNack" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/AckSuite/TestUpsertMultipleNodes" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/AckSuite/TestUpsertSingleNode" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/AckSuite/TestUseCurrent" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/NodeSuite" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/NodeSuite/TestIstioNodeToIP" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.000"></testcase>
		<testcase name="Test/ServerSuite" classname="github.com/cilium/cilium/pkg/envoy/xds" time="4.520"></testcase>
		<testcase name="Test/ServerSuite/TestAck" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.500">
			<system-out><![CDATA[level=info msg="starting xDS stream processing" subsys=xds xdsStreamID=1
level=info msg="xDS stream closed" subsys=xds xdsClientNode="sidecar~10.0.0.0~node0~bar" xdsStreamID=1]]></system-out>
		</testcase>
		<testcase name="Test/ServerSuite/TestNAck" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.750">
			<system-out><![CDATA[level=info msg="starting xDS stream processing" subsys=xds xdsStreamID=1
level=warning msg="NACK received for versions after 1 and up to 2; waiting for a version update before sending again" subsys=xds xdsAckedVersion=1 xdsClientNode="sidecar~10.0.0.0~node0~bar" xdsDetail=FAILFAIL xdsNonce=2 xdsStreamID=1 xdsTypeURL=type.googleapis.com/envoy.config.v3.DummyConfiguration
level=info msg="xDS stream closed" subsys=xds xdsClientNode="sidecar~10.0.0.0~node0~bar" xdsStreamID=1]]></system-out>
		</testcase>
		<testcase name="Test/ServerSuite/TestNAckFromTheStart" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.760">
			<system-out><![CDATA[level=info msg="starting xDS stream processing" subsys=xds xdsStreamID=1
level=warning msg="NACK received for versions after  and up to 1; waiting for a version update before sending again" subsys=xds xdsAckedVersion= xdsClientNode="sidecar~10.0.0.0~node0~bar" xdsDetail= xdsNonce=1 xdsStreamID=1 xdsTypeURL=type.googleapis.com/envoy.config.v3.DummyConfiguration
level=warning msg="NACK received for versions after  and up to 2; waiting for a version update before sending again" subsys=xds xdsAckedVersion= xdsClientNode="sidecar~10.0.0.0~node0~bar" xdsDetail= xdsNonce=2 xdsStreamID=1 xdsTypeURL=type.googleapis.com/envoy.config.v3.DummyConfiguration
level=info msg="xDS stream closed" subsys=xds xdsClientNode="sidecar~10.0.0.0~node0~bar" xdsStreamID=1]]></system-out>
		</testcase>
		<testcase name="Test/ServerSuite/TestRequestAllResources" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.500">
			<system-out><![CDATA[level=info msg="starting xDS stream processing" subsys=xds xdsStreamID=1
level=info msg="xDS stream closed" subsys=xds xdsClientNode="sidecar~10.0.0.0~node0~bar" xdsStreamID=1]]></system-out>
		</testcase>
		<testcase name="Test/ServerSuite/TestRequestHighVersionFromTheStart" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.250">
			<system-out><![CDATA[level=info msg="starting xDS stream processing" subsys=xds xdsStreamID=1
level=info msg="xDS stream closed" subsys=xds xdsClientNode="sidecar~10.0.0.0~node0~bar" xdsStreamID=1]]></system-out>
		</testcase>
		<testcase name="Test/ServerSuite/TestRequestSomeResources" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.750">
			<system-out><![CDATA[level=info msg="starting xDS stream processing" subsys=xds xdsStreamID=1
level=info msg="xDS stream closed" subsys=xds xdsClientNode="sidecar~10.0.0.0~node0~bar" xdsStreamID=1]]></system-out>
		</testcase>
		<testcase name="Test/ServerSuite/TestRequestStaleNonce" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.500">
			<system-out><![CDATA[level=info msg="starting xDS stream processing" subsys=xds xdsStreamID=1
level=info msg="xDS stream closed" subsys=xds xdsClientNode="sidecar~10.0.0.0~node0~bar" xdsStreamID=1]]></system-out>
		</testcase>
		<testcase name="Test/ServerSuite/TestUpdateRequestResources" classname="github.com/cilium/cilium/pkg/envoy/xds" time="0.500">
			<system-out><![CDATA[level=info msg="starting xDS stream processing" subsys=xds xdsStreamID=1
level=info msg="xDS stream closed" subsys=xds xdsClientNode="sidecar~10.0.0.0~node0~bar" xdsStreamID=1]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/envoy/xds	coverage: 81.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/eventqueue" tests="13" failures="0" errors="0" id="371" hostname="kind-bpf-next" time="0.017" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<testcase name="Test/EventQueueSuite" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<testcase name="Test/EventQueueSuite/TestCloseEventQueueMultipleTimes" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<testcase name="Test/EventQueueSuite/TestDrain" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<testcase name="Test/EventQueueSuite/TestDrained" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<testcase name="Test/EventQueueSuite/TestEnqueueTwice" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<testcase name="Test/EventQueueSuite/TestEventCancelAfterQueueClosed" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<testcase name="Test/EventQueueSuite/TestForcefulDraining" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<testcase name="Test/EventQueueSuite/TestNewEvent" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<testcase name="Test/EventQueueSuite/TestNewEventQueue" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<testcase name="Test/EventQueueSuite/TestNewEventQueueBuffered" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<testcase name="Test/EventQueueSuite/TestNilEvent" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<testcase name="Test/EventQueueSuite/TestStopWithoutRun" classname="github.com/cilium/cilium/pkg/eventqueue" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/eventqueue	coverage: 93.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/fqdn" tests="30" failures="0" errors="0" id="372" hostname="kind-bpf-next" time="0.027" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/fqdn" time="0.010"></testcase>
		<testcase name="Test/DNSCacheTestSuite" classname="github.com/cilium/cilium/pkg/fqdn" time="0.010"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestCacheToZombiesGCCascade" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestCountIPs" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestDelete" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestGCOverlimitAfterTTLCleanup" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestJSONMarshal" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestMapIPsToSelectors" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestOverlimitAfterDeleteForwardEntry" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestOverlimitEntriesWithValidLimit" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestOverlimitEntriesWithoutLimit" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestOverlimitPreferNewerEntries" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000">
			<system-out><![CDATA[level=warning msg="Evicting expired DNS cache entries that may be in-use due to per-host limits. This may cause recently created connections to be disconnected. Raise tofqdns-endpoint-max-ip-per-hostname to mitigate this." subsys=fqdn]]></system-out>
		</testcase>
		<testcase name="Test/DNSCacheTestSuite/TestReverseUpdateLookup" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestTTLCleanupEntries" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestTTLCleanupWithoutForward" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestTTLInsertWithMinValue" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestTTLInsertWithZeroValue" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestUpdateLookup" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestZombiesDumpAlive" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestZombiesForceExpire" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestZombiesGC" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/TestZombiesGCDeferredDeletes" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000">
			<system-out><![CDATA[level=warning msg="Evicting expired DNS cache entries that may be in-use. This may cause recently created connections to be disconnected. Raise --tofqdns-max-deferred-connection-deletes to mitigate this." subsys=fqdn]]></system-out>
		</testcase>
		<testcase name="Test/DNSCacheTestSuite/TestZombiesGCOverLimit" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000">
			<system-out><![CDATA[level=warning msg="Evicting expired DNS cache entries that may be in-use due to per-host limits. This may cause recently created connections to be disconnected. Raise tofqdns-endpoint-max-ip-per-hostname to mitigate this." subsys=fqdn]]></system-out>
		</testcase>
		<testcase name="Test/DNSCacheTestSuite/TestZombiesGCOverLimitWithCTGC" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000">
			<system-out><![CDATA[level=warning msg="Evicting expired DNS cache entries that may be in-use due to per-host limits. This may cause recently created connections to be disconnected. Raise tofqdns-endpoint-max-ip-per-hostname to mitigate this." subsys=fqdn]]></system-out>
		</testcase>
		<testcase name="Test/DNSCacheTestSuite/TestZombiesSiblingsGC" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/DNSCacheTestSuite/Test_forceExpiredByNames" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/FQDNTestSuite" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/FQDNTestSuite/TestNameManagerCIDRGeneration" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<testcase name="Test/FQDNTestSuite/TestNameManagerMultiIPUpdate" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000">
			<system-out><![CDATA[level=warning msg="FQDNSelector was already registered for updates, returning without any identities" fqdnSelector="{github.com }" subsys=fqdn]]></system-out>
		</testcase>
		<testcase name="FuzzMapSelectorsToIPsLocked" classname="github.com/cilium/cilium/pkg/fqdn" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/fqdn	coverage: 82.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/fqdn/dns" tests="2" failures="0" errors="0" id="373" hostname="kind-bpf-next" time="0.003" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestIsFQDN" classname="github.com/cilium/cilium/pkg/fqdn/dns" time="0.000"></testcase>
		<testcase name="TestFQDN" classname="github.com/cilium/cilium/pkg/fqdn/dns" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/fqdn/dns	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/fqdn/dnsproxy" tests="40" failures="0" errors="0" id="374" hostname="kind-bpf-next" time="0.081" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestNonPrivileged" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.020"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyHelperTestSuite" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyHelperTestSuite/TestGeneratePattern" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyHelperTestSuite/TestGeneratePatternTrailingDot" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyHelperTestSuite/TestSetPortRulesForID" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyHelperTestSuite/TestSetPortRulesForIDFromUnifiedFormat" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.020"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestAcceptFromMatchingEndpoint" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestAcceptNonRegex" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestCheckAllowedTwiceRemovedOnce" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestCheckNoRules" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestFullPathDependence" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestProxyRequestContext_IsTimeout" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestRejectFromDifferentEndpoint" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestRejectNonMatchingRefusedResponseWithNameError" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestRejectNonMatchingRefusedResponseWithRefused" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestRejectNonRegex" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestRespondMixedCaseInRequestResponse" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestRespondViaCorrectProtocol" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="TestNonPrivileged/DNSProxyTestSuite/TestRestoredEndpoint" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.010">
			<system-out><![CDATA[level=info msg="Disregarding restored DNS rule due to failure in compiling regex. Traffic to the FQDN may be disrupted." endpointID=111 rule="invalid-re-pattern((*" subsys=fqdn/dnsproxy]]></system-out>
		</testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.030"></testcase>
		<testcase name="Test/DNSProxyHelperTestSuite" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyHelperTestSuite/TestGeneratePattern" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyHelperTestSuite/TestGeneratePatternTrailingDot" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyHelperTestSuite/TestSetPortRulesForID" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyHelperTestSuite/TestSetPortRulesForIDFromUnifiedFormat" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyTestSuite" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.030"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestAcceptFromMatchingEndpoint" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestAcceptNonRegex" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestCheckAllowedTwiceRemovedOnce" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestCheckNoRules" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestFullPathDependence" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestProxyRequestContext_IsTimeout" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestRejectFromDifferentEndpoint" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestRejectNonMatchingRefusedResponseWithNameError" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestRejectNonMatchingRefusedResponseWithRefused" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestRejectNonRegex" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestRespondMixedCaseInRequestResponse" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.010"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestRespondViaCorrectProtocol" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000"></testcase>
		<testcase name="Test/DNSProxyTestSuite/TestRestoredEndpoint" classname="github.com/cilium/cilium/pkg/fqdn/dnsproxy" time="0.000">
			<system-out><![CDATA[level=info msg="Disregarding restored DNS rule due to failure in compiling regex. Traffic to the FQDN may be disrupted." endpointID=111 rule="invalid-re-pattern((*" subsys=fqdn/dnsproxy]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/fqdn/dnsproxy	coverage: 62.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/fqdn/matchpattern" tests="6" failures="0" errors="0" id="375" hostname="kind-bpf-next" time="0.014" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/fqdn/matchpattern" time="0.000"></testcase>
		<testcase name="Test/MatchPatternTestSuite" classname="github.com/cilium/cilium/pkg/fqdn/matchpattern" time="0.000"></testcase>
		<testcase name="Test/MatchPatternTestSuite/TestAnchoredMatchPatternMatching" classname="github.com/cilium/cilium/pkg/fqdn/matchpattern" time="0.000"></testcase>
		<testcase name="Test/MatchPatternTestSuite/TestAnchoredMatchPatternREConversion" classname="github.com/cilium/cilium/pkg/fqdn/matchpattern" time="0.000"></testcase>
		<testcase name="Test/MatchPatternTestSuite/TestMatchPatternSanitize" classname="github.com/cilium/cilium/pkg/fqdn/matchpattern" time="0.000"></testcase>
		<testcase name="Test/MatchPatternTestSuite/TestUnAnchoredMatchPatternREConversion" classname="github.com/cilium/cilium/pkg/fqdn/matchpattern" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/fqdn/matchpattern	coverage: 62.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/fswatcher" tests="4" failures="0" errors="0" id="376" hostname="kind-bpf-next" time="0.006" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/fswatcher" time="0.000"></testcase>
		<testcase name="Test/FsWatcherTestSuite" classname="github.com/cilium/cilium/pkg/fswatcher" time="0.000"></testcase>
		<testcase name="Test/FsWatcherTestSuite/TestWatcher" classname="github.com/cilium/cilium/pkg/fswatcher" time="0.000"></testcase>
		<testcase name="Test/FsWatcherTestSuite/Test_hasParent" classname="github.com/cilium/cilium/pkg/fswatcher" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/fswatcher	coverage: 89.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/health/client" tests="9" failures="0" errors="0" id="377" hostname="kind-bpf-next" time="0.044" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/health/client" time="0.030"></testcase>
		<testcase name="Test/ClientTestSuite" classname="github.com/cilium/cilium/pkg/health/client" time="0.030"></testcase>
		<testcase name="Test/ClientTestSuite/TestConnectivityStatusType" classname="github.com/cilium/cilium/pkg/health/client" time="0.000"></testcase>
		<testcase name="Test/ClientTestSuite/TestFormatNodeStatus" classname="github.com/cilium/cilium/pkg/health/client" time="0.030"></testcase>
		<testcase name="Test/ClientTestSuite/TestGetAllEndpointAddresses" classname="github.com/cilium/cilium/pkg/health/client" time="0.000"></testcase>
		<testcase name="Test/ClientTestSuite/TestGetConnectivityStatusType" classname="github.com/cilium/cilium/pkg/health/client" time="0.000"></testcase>
		<testcase name="Test/ClientTestSuite/TestGetHostPrimaryAddress" classname="github.com/cilium/cilium/pkg/health/client" time="0.000"></testcase>
		<testcase name="Test/ClientTestSuite/TestGetPathConnectivityStatusType" classname="github.com/cilium/cilium/pkg/health/client" time="0.000"></testcase>
		<testcase name="Test/ClientTestSuite/TestGetPrimaryAddressIP" classname="github.com/cilium/cilium/pkg/health/client" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/health/client	coverage: 54.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/health/server" tests="6" failures="0" errors="0" id="378" hostname="kind-bpf-next" time="0.022" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/health/server" time="0.000"></testcase>
		<testcase name="Test/HealthServerTestSuite" classname="github.com/cilium/cilium/pkg/health/server" time="0.000"></testcase>
		<testcase name="Test/HealthServerTestSuite/TestProbersetNodes" classname="github.com/cilium/cilium/pkg/health/server" time="0.000"></testcase>
		<testcase name="Test/ServerTestSuite" classname="github.com/cilium/cilium/pkg/health/server" time="0.000"></testcase>
		<testcase name="Test/ServerTestSuite/Test_server_collectNodeConnectivityMetrics" classname="github.com/cilium/cilium/pkg/health/server" time="0.000"></testcase>
		<testcase name="Test/ServerTestSuite/Test_server_getClusterNodeName" classname="github.com/cilium/cilium/pkg/health/server" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/health/server	coverage: 27.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hive" tests="11" failures="0" errors="0" id="379" hostname="kind-bpf-next" time="0.015" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestHiveGoodConfig" classname="github.com/cilium/cilium/pkg/hive" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="61.876µs" function="hive_test.TestHiveGoodConfig.func1 (hive_test.go:35)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg=Stopping subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestHiveBadConfig" classname="github.com/cilium/cilium/pkg/hive" time="0.000">
			<system-out><![CDATA[level=error msg="Invoke failed" ="hive_test.TestHiveBadConfig.func1 (hive_test.go:74)" error="could not build arguments for function \"github.com/cilium/cilium/pkg/hive_test\".TestHiveBadConfig.func1 (/host/pkg/hive/hive_test.go:74): failed to build hive_test.BadConfig: received non-nil error from function \"github.com/cilium/cilium/pkg/hive/cell\".(*config[...]).Apply.func2 (/host/pkg/hive/cell/config.go:132): failed to unmarshal config struct hive_test.BadConfig: 2 error(s) decoding:\n\n* '' has invalid keys: foo\n* '' has unset fields: Bar.\nHint: field 'FooBar' matches flag 'foo-bar', or use tag `mapstructure:\"flag-name\"` to match field with flag" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestHiveConfigOverride" classname="github.com/cilium/cilium/pkg/hive" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="44.844µs" function="hive_test.TestHiveConfigOverride.func1 (hive_test.go:88)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg=Stopping subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestProvideInvoke" classname="github.com/cilium/cilium/pkg/hive" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="11.421µs" function="hive_test.TestProvideInvoke.func2 (hive_test.go:125)" subsys=hive
level=info msg=Invoked duration="9.287µs" function="hive_test.glob..func1 (hive_test.go:316)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="2.474µs" function="hive_test.glob..func1.1 (hive_test.go:318)" subsys=hive
level=info msg=Stopping subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestGroup" classname="github.com/cilium/cilium/pkg/hive" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="38.632µs" function="hive_test.TestGroup.func3 (hive_test.go:146)" subsys=hive
level=info msg=Invoked duration="4.198µs" function="hive_test.glob..func1 (hive_test.go:316)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=732ns function="hive_test.glob..func1.1 (hive_test.go:318)" subsys=hive
level=info msg=Stopping subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestProvidePrivate" classname="github.com/cilium/cilium/pkg/hive" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="6.812µs" function="hive_test.TestProvidePrivate.func2 (hive_test.go:160)" subsys=hive
level=info msg=Invoked duration="6.392µs" function="hive_test.glob..func1 (hive_test.go:316)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=751ns function="hive_test.glob..func1.1 (hive_test.go:318)" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg=Invoked duration="7.674µs" function="hive_test.TestProvidePrivate.func2 (hive_test.go:160)" subsys=hive
level=error msg="Invoke failed" ="hive_test.TestProvidePrivate.func3 (hive_test.go:177)" error="missing dependencies for function \"github.com/cilium/cilium/pkg/hive_test\".TestProvidePrivate.func3 (/host/pkg/hive/hive_test.go:177): missing type: *hive_test.SomeObject" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestDecorate" classname="github.com/cilium/cilium/pkg/hive" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="20.587µs" function="hive_test.TestDecorate.func4 (hive_test.go:205)" subsys=hive
level=info msg=Invoked duration="11.462µs" function="hive_test.TestDecorate.func2 (hive_test.go:192)" subsys=hive
level=info msg=Invoked duration="32.56µs" function="hive_test.glob..func1 (hive_test.go:316)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=832ns function="hive_test.glob..func1.1 (hive_test.go:318)" subsys=hive
level=info msg=Stopping subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestShutdown" classname="github.com/cilium/cilium/pkg/hive" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="5.891µs" function="hive_test.TestShutdown.func1 (hive_test.go:228)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=822ns function="hive_test.TestShutdown.func1.1 (hive_test.go:230)" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg=Invoked duration="3.016µs" function="hive_test.TestShutdown.func2 (hive_test.go:240)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.313µs" function="hive_test.TestShutdown.func2.1 (hive_test.go:242)" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg=Invoked duration="4.789µs" function="hive_test.TestShutdown.func3 (hive_test.go:252)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg=Stopping subsys=hive
level=info msg=Invoked duration="4.849µs" function="hive_test.TestShutdown.func4 (hive_test.go:266)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=922ns function="hive_test.TestShutdown.func4.1 (hive_test.go:268)" subsys=hive
level=info msg=Stopping subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestRunRollback" classname="github.com/cilium/cilium/pkg/hive" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="6.802µs" function="hive_test.TestRunRollback.func1 (hive_test.go:280)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration=331ns function="hive_test.TestRunRollback.func1.1 (hive_test.go:282)" subsys=hive
level=error msg="Start hook failed" error="context deadline exceeded" function="hive_test.TestRunRollback.func1.3 (hive_test.go:292)" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration=360ns function="hive_test.TestRunRollback.func1.2 (hive_test.go:286)" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestLifecycle" classname="github.com/cilium/cilium/pkg/hive" time="0.000">
			<system-out><![CDATA[level=info msg="Start hook executed" duration=290ns function="hive_test.glob..func2 (lifecycle_test.go:23)" subsys=hive
level=info msg="Start hook executed" duration=251ns function="hive_test.glob..func2 (lifecycle_test.go:23)" subsys=hive
level=info msg="Start hook executed" duration=90ns function="hive_test.glob..func2 (lifecycle_test.go:23)" subsys=hive
level=info msg="Stop hook executed" duration=90ns function="hive_test.glob..func3 (lifecycle_test.go:27)" subsys=hive
level=info msg="Stop hook executed" duration=40ns function="hive_test.glob..func3 (lifecycle_test.go:27)" subsys=hive
level=info msg="Stop hook executed" duration=69ns function="hive_test.glob..func3 (lifecycle_test.go:27)" subsys=hive
level=info msg="Start hook executed" duration=81ns function="hive_test.glob..func2 (lifecycle_test.go:23)" subsys=hive
level=info msg="Start hook executed" duration=50ns function="hive_test.glob..func2 (lifecycle_test.go:23)" subsys=hive
level=error msg="Start hook failed" error=nope function="hive_test.glob..func4 (lifecycle_test.go:34)" subsys=hive
level=info msg="Start hook executed" duration=70ns function="hive_test.glob..func2 (lifecycle_test.go:23)" subsys=hive
level=info msg="Start hook executed" duration=50ns function="hive_test.glob..func2 (lifecycle_test.go:23)" subsys=hive
level=info msg="Start hook executed" duration=60ns function="hive_test.glob..func5 (lifecycle_test.go:40)" subsys=hive
level=error msg="Stop hook failed" error=nope function="hive_test.glob..func6 (lifecycle_test.go:44)" subsys=hive
level=info msg="Stop hook executed" duration=60ns function="hive_test.glob..func3 (lifecycle_test.go:27)" subsys=hive
level=info msg="Stop hook executed" duration=50ns function="hive_test.glob..func3 (lifecycle_test.go:27)" subsys=hive
level=info msg="Stop hook executed" duration=80ns function="hive_test.TestLifecycle.func1 (lifecycle_test.go:112)" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestLifecycleCancel" classname="github.com/cilium/cilium/pkg/hive" time="0.000">
			<system-out><![CDATA[level=error msg="Start hook failed" error="context canceled" function="hive_test.TestLifecycleCancel.func1 (lifecycle_test.go:131)" subsys=hive
level=error msg="Stop hook failed" error="stop cancelled" function="hive_test.TestLifecycleCancel.func2 (lifecycle_test.go:145)" subsys=hive]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hive	coverage: 70.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hive/job" tests="19" failures="0" errors="0" id="380" hostname="kind-bpf-next" time="16.913" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestOneShot_ShortRun" classname="github.com/cilium/cilium/pkg/hive/job" time="0.000">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestOneShot_ShortRun.func1 (job_test.go:45)" subsys=hive
level=info msg=Invoked duration="19.066µs" function="job.TestOneShot_ShortRun.func1 (job_test.go:45)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="3.497µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_ShortRun.func1.1 (job_test.go:49)" name=short subsys=hive
level=debug msg="one-shot job finished" func="job.TestOneShot_ShortRun.func1.1 (job_test.go:49)" name=short subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="4.248µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestOneShot_LongRun" classname="github.com/cilium/cilium/pkg/hive/job" time="0.000">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestOneShot_LongRun.func1 (job_test.go:69)" subsys=hive
level=info msg=Invoked duration="19.396µs" function="job.TestOneShot_LongRun.func1 (job_test.go:69)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="2.313µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_LongRun.func1.1 (job_test.go:73)" name=long subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=debug msg="one-shot job finished" func="job.TestOneShot_LongRun.func1.1 (job_test.go:73)" name=long subsys=hive
level=info msg="Stop hook executed" duration="11.071µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestOneShot_RetryFail" classname="github.com/cilium/cilium/pkg/hive/job" time="0.040">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestOneShot_RetryFail.func1 (job_test.go:100)" subsys=hive
level=info msg=Invoked duration="14.467µs" function="job.TestOneShot_RetryFail.func1 (job_test.go:100)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="1.873µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_RetryFail.func1.1 (job_test.go:104)" name=retry-fail subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.TestOneShot_RetryFail.func1.1 (job_test.go:104)" name=retry-fail subsys=hive
level=debug msg="Delaying retry attempt" backoff=5ms func="job.TestOneShot_RetryFail.func1.1 (job_test.go:104)" name=retry-fail retry-count=1 subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_RetryFail.func1.1 (job_test.go:104)" name=retry-fail subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.TestOneShot_RetryFail.func1.1 (job_test.go:104)" name=retry-fail subsys=hive
level=debug msg="Delaying retry attempt" backoff=10ms func="job.TestOneShot_RetryFail.func1.1 (job_test.go:104)" name=retry-fail retry-count=2 subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_RetryFail.func1.1 (job_test.go:104)" name=retry-fail subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.TestOneShot_RetryFail.func1.1 (job_test.go:104)" name=retry-fail subsys=hive
level=debug msg="Delaying retry attempt" backoff=20ms func="job.TestOneShot_RetryFail.func1.1 (job_test.go:104)" name=retry-fail retry-count=3 subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_RetryFail.func1.1 (job_test.go:104)" name=retry-fail subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.TestOneShot_RetryFail.func1.1 (job_test.go:104)" name=retry-fail subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="4.909µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestOneShot_RetryBackoff" classname="github.com/cilium/cilium/pkg/hive/job" time="15.770">
			<system-out><![CDATA[level=debug msg=Invoking function="job.testOneShot_RetryBackoff.func1 (job_test.go:161)" subsys=hive
level=info msg=Invoked duration="13.796µs" function="job.testOneShot_RetryBackoff.func1 (job_test.go:161)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="4.227µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=50ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=1 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=100ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=2 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=200ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=3 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=400ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=4 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=800ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=5 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=1.6s func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=6 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="7.684µs" function="*job.group.Stop" subsys=hive
level=debug msg=Invoking function="job.testOneShot_RetryBackoff.func1 (job_test.go:161)" subsys=hive
level=info msg=Invoked duration="12.564µs" function="job.testOneShot_RetryBackoff.func1 (job_test.go:161)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="3.727µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=50ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=1 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=100ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=2 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=200ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=3 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=400ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=4 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=800ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=5 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=1.6s func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=6 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="7.423µs" function="*job.group.Stop" subsys=hive
level=debug msg=Invoking function="job.testOneShot_RetryBackoff.func1 (job_test.go:161)" subsys=hive
level=info msg=Invoked duration="19.377µs" function="job.testOneShot_RetryBackoff.func1 (job_test.go:161)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="4.799µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=50ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=1 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=100ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=2 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=200ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=3 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=400ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=4 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=800ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=5 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=1.6s func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=6 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="18.585µs" function="*job.group.Stop" subsys=hive
level=debug msg=Invoking function="job.testOneShot_RetryBackoff.func1 (job_test.go:161)" subsys=hive
level=info msg=Invoked duration="21.45µs" function="job.testOneShot_RetryBackoff.func1 (job_test.go:161)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="5.88µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=50ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=1 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=100ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=2 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=200ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=3 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=400ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=4 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=800ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=5 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=1.6s func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=6 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="9.057µs" function="*job.group.Stop" subsys=hive
level=debug msg=Invoking function="job.testOneShot_RetryBackoff.func1 (job_test.go:161)" subsys=hive
level=info msg=Invoked duration="17.152µs" function="job.testOneShot_RetryBackoff.func1 (job_test.go:161)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="10.87µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=50ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=1 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=100ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=2 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=200ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=3 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=400ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=4 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=800ms func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=5 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=debug msg="Delaying retry attempt" backoff=1.6s func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff retry-count=6 subsys=hive
level=debug msg="Starting one-shot job" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.testOneShot_RetryBackoff.func1.1 (job_test.go:165)" name=retry-backoff subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="7.463µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestOneShot_RetryRecover" classname="github.com/cilium/cilium/pkg/hive/job" time="0.010">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestOneShot_RetryRecover.func1 (job_test.go:212)" subsys=hive
level=info msg=Invoked duration="17.182µs" function="job.TestOneShot_RetryRecover.func1 (job_test.go:212)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="3.868µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_RetryRecover.func1.1 (job_test.go:216)" name=retry-recover subsys=hive
level=error msg="one-shot job errored" error="Sometimes error" func="job.TestOneShot_RetryRecover.func1.1 (job_test.go:216)" name=retry-recover subsys=hive
level=debug msg="Delaying retry attempt" backoff=5ms func="job.TestOneShot_RetryRecover.func1.1 (job_test.go:216)" name=retry-recover retry-count=1 subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_RetryRecover.func1.1 (job_test.go:216)" name=retry-recover subsys=hive
level=debug msg="one-shot job finished" func="job.TestOneShot_RetryRecover.func1.1 (job_test.go:216)" name=retry-recover subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="4.228µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestOneShot_Shutdown" classname="github.com/cilium/cilium/pkg/hive/job" time="0.000">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestOneShot_Shutdown.func1 (job_test.go:248)" subsys=hive
level=info msg=Invoked duration="10.73µs" function="job.TestOneShot_Shutdown.func1 (job_test.go:248)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="4.118µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_Shutdown.func1.1 (job_test.go:252)" name=shutdown subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.TestOneShot_Shutdown.func1.1 (job_test.go:252)" name=shutdown subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.587µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestOneShot_RetryFailShutdown" classname="github.com/cilium/cilium/pkg/hive/job" time="0.040">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestOneShot_RetryFailShutdown.func1 (job_test.go:274)" subsys=hive
level=info msg=Invoked duration="24.966µs" function="job.TestOneShot_RetryFailShutdown.func1 (job_test.go:274)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="5.231µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_RetryFailShutdown.func1.1 (job_test.go:278)" name=retry-fail-shutdown subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.TestOneShot_RetryFailShutdown.func1.1 (job_test.go:278)" name=retry-fail-shutdown subsys=hive
level=debug msg="Delaying retry attempt" backoff=5ms func="job.TestOneShot_RetryFailShutdown.func1.1 (job_test.go:278)" name=retry-fail-shutdown retry-count=1 subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_RetryFailShutdown.func1.1 (job_test.go:278)" name=retry-fail-shutdown subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.TestOneShot_RetryFailShutdown.func1.1 (job_test.go:278)" name=retry-fail-shutdown subsys=hive
level=debug msg="Delaying retry attempt" backoff=10ms func="job.TestOneShot_RetryFailShutdown.func1.1 (job_test.go:278)" name=retry-fail-shutdown retry-count=2 subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_RetryFailShutdown.func1.1 (job_test.go:278)" name=retry-fail-shutdown subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.TestOneShot_RetryFailShutdown.func1.1 (job_test.go:278)" name=retry-fail-shutdown subsys=hive
level=debug msg="Delaying retry attempt" backoff=20ms func="job.TestOneShot_RetryFailShutdown.func1.1 (job_test.go:278)" name=retry-fail-shutdown retry-count=3 subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_RetryFailShutdown.func1.1 (job_test.go:278)" name=retry-fail-shutdown subsys=hive
level=error msg="one-shot job errored" error="Always error" func="job.TestOneShot_RetryFailShutdown.func1.1 (job_test.go:278)" name=retry-fail-shutdown subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="6.351µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestOneShot_RetryRecoverNoShutdown" classname="github.com/cilium/cilium/pkg/hive/job" time="0.010">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestOneShot_RetryRecoverNoShutdown.func1 (job_test.go:309)" subsys=hive
level=info msg=Invoked duration="13.626µs" function="job.TestOneShot_RetryRecoverNoShutdown.func1 (job_test.go:309)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="3.216µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_RetryRecoverNoShutdown.func1.1 (job_test.go:313)" name=retry-recover-no-shutdown subsys=hive
level=error msg="one-shot job errored" error="First try error" func="job.TestOneShot_RetryRecoverNoShutdown.func1.1 (job_test.go:313)" name=retry-recover-no-shutdown subsys=hive
level=debug msg="Delaying retry attempt" backoff=5ms func="job.TestOneShot_RetryRecoverNoShutdown.func1.1 (job_test.go:313)" name=retry-recover-no-shutdown retry-count=1 subsys=hive
level=debug msg="Starting one-shot job" func="job.TestOneShot_RetryRecoverNoShutdown.func1.1 (job_test.go:313)" name=retry-recover-no-shutdown subsys=hive
level=debug msg="one-shot job finished" func="job.TestOneShot_RetryRecoverNoShutdown.func1.1 (job_test.go:313)" name=retry-recover-no-shutdown subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.397µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestTimer_OnInterval" classname="github.com/cilium/cilium/pkg/hive/job" time="0.500">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestTimer_OnInterval.func1 (job_test.go:358)" subsys=hive
level=info msg=Invoked duration="10.559µs" function="job.TestTimer_OnInterval.func1 (job_test.go:358)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="2.315µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting timer job" func="job.TestTimer_OnInterval.func1.1 (job_test.go:362)" name=on-interval subsys=hive
level=debug msg="Timer job triggered" func="job.TestTimer_OnInterval.func1.1 (job_test.go:362)" name=on-interval subsys=hive
level=debug msg="Timer job finished" func="job.TestTimer_OnInterval.func1.1 (job_test.go:362)" name=on-interval subsys=hive
level=debug msg="Timer job triggered" func="job.TestTimer_OnInterval.func1.1 (job_test.go:362)" name=on-interval subsys=hive
level=debug msg="Timer job finished" func="job.TestTimer_OnInterval.func1.1 (job_test.go:362)" name=on-interval subsys=hive
level=debug msg="Timer job triggered" func="job.TestTimer_OnInterval.func1.1 (job_test.go:362)" name=on-interval subsys=hive
level=debug msg="Timer job finished" func="job.TestTimer_OnInterval.func1.1 (job_test.go:362)" name=on-interval subsys=hive
level=debug msg="Timer job triggered" func="job.TestTimer_OnInterval.func1.1 (job_test.go:362)" name=on-interval subsys=hive
level=debug msg="Timer job finished" func="job.TestTimer_OnInterval.func1.1 (job_test.go:362)" name=on-interval subsys=hive
level=debug msg="Timer job triggered" func="job.TestTimer_OnInterval.func1.1 (job_test.go:362)" name=on-interval subsys=hive
level=debug msg="Timer job finished" func="job.TestTimer_OnInterval.func1.1 (job_test.go:362)" name=on-interval subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="9.909µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestTimer_Trigger" classname="github.com/cilium/cilium/pkg/hive/job" time="0.000">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestTimer_Trigger.func1 (job_test.go:394)" subsys=hive
level=info msg=Invoked duration="16.231µs" function="job.TestTimer_Trigger.func1 (job_test.go:394)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="3.156µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting timer job" func="job.TestTimer_Trigger.func1.1 (job_test.go:398)" name=on-interval subsys=hive
level=debug msg="Timer job triggered" func="job.TestTimer_Trigger.func1.1 (job_test.go:398)" name=on-interval subsys=hive
level=debug msg="Timer job finished" func="job.TestTimer_Trigger.func1.1 (job_test.go:398)" name=on-interval subsys=hive
level=debug msg="Timer job triggered" func="job.TestTimer_Trigger.func1.1 (job_test.go:398)" name=on-interval subsys=hive
level=debug msg="Timer job finished" func="job.TestTimer_Trigger.func1.1 (job_test.go:398)" name=on-interval subsys=hive
level=debug msg="Timer job triggered" func="job.TestTimer_Trigger.func1.1 (job_test.go:398)" name=on-interval subsys=hive
level=debug msg="Timer job finished" func="job.TestTimer_Trigger.func1.1 (job_test.go:398)" name=on-interval subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="5.22µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestTimer_DoubleTrigger" classname="github.com/cilium/cilium/pkg/hive/job" time="0.100">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestTimer_DoubleTrigger.func1 (job_test.go:440)" subsys=hive
level=info msg=Invoked duration="10.078µs" function="job.TestTimer_DoubleTrigger.func1 (job_test.go:440)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="2.044µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting timer job" func="job.TestTimer_DoubleTrigger.func1.1 (job_test.go:444)" name=on-interval subsys=hive
level=debug msg="Timer job triggered" func="job.TestTimer_DoubleTrigger.func1.1 (job_test.go:444)" name=on-interval subsys=hive
level=debug msg="Timer job finished" func="job.TestTimer_DoubleTrigger.func1.1 (job_test.go:444)" name=on-interval subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="10.891µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestTimer_ExitOnClose" classname="github.com/cilium/cilium/pkg/hive/job" time="0.000">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestTimer_ExitOnClose.func1 (job_test.go:479)" subsys=hive
level=info msg=Invoked duration="12.793µs" function="job.TestTimer_ExitOnClose.func1 (job_test.go:479)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="3.797µs" function="*job.group.Start" subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=debug msg="Starting timer job" func="job.TestTimer_ExitOnClose.func1.1 (job_test.go:483)" name=on-interval subsys=hive
level=info msg="Stop hook executed" duration="44.854µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestTimer_ExitOnCloseFnCtx" classname="github.com/cilium/cilium/pkg/hive/job" time="0.000">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestTimer_ExitOnCloseFnCtx.func1 (job_test.go:510)" subsys=hive
level=info msg=Invoked duration="11.772µs" function="job.TestTimer_ExitOnCloseFnCtx.func1 (job_test.go:510)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="2.004µs" function="*job.group.Start" subsys=hive
level=debug msg="Starting timer job" func="job.TestTimer_ExitOnCloseFnCtx.func1.1 (job_test.go:514)" name=on-interval subsys=hive
level=debug msg="Timer job triggered" func="job.TestTimer_ExitOnCloseFnCtx.func1.1 (job_test.go:514)" name=on-interval subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=debug msg="Timer job finished" func="job.TestTimer_ExitOnCloseFnCtx.func1.1 (job_test.go:514)" name=on-interval subsys=hive
level=info msg="Stop hook executed" duration="14.116µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestObserver_ShortStream" classname="github.com/cilium/cilium/pkg/hive/job" time="0.000">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestObserver_ShortStream.func1 (job_test.go:552)" subsys=hive
level=info msg=Invoked duration="13.015µs" function="job.TestObserver_ShortStream.func1 (job_test.go:552)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="2.034µs" function="*job.group.Start" subsys=hive
level=debug msg="Observer job started" func="job.TestObserver_ShortStream.func1.1 (job_test.go:556)" name=retry-fail subsys=hive
level=debug msg="Observer job stopped" error="<nil>" func="job.TestObserver_ShortStream.func1.1 (job_test.go:556)" name=retry-fail subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.114µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestObserver_LongStream" classname="github.com/cilium/cilium/pkg/hive/job" time="0.000">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestObserver_LongStream.func1 (job_test.go:591)" subsys=hive
level=info msg=Invoked duration="20.137µs" function="job.TestObserver_LongStream.func1 (job_test.go:591)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="2.014µs" function="*job.group.Start" subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=debug msg="Observer job started" func="job.TestObserver_LongStream.func1.1 (job_test.go:595)" name=retry-fail subsys=hive
level=error msg="Observer job stopped with an error" error="context canceled" func="job.TestObserver_LongStream.func1.1 (job_test.go:595)" name=retry-fail subsys=hive
level=info msg="Stop hook executed" duration="52.838µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestObserver_CtxClose" classname="github.com/cilium/cilium/pkg/hive/job" time="0.000">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestObserver_CtxClose.func1 (job_test.go:624)" subsys=hive
level=info msg=Invoked duration="10.94µs" function="job.TestObserver_CtxClose.func1 (job_test.go:624)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="1.483µs" function="*job.group.Start" subsys=hive
level=debug msg="Observer job started" func="job.TestObserver_CtxClose.func1.1 (job_test.go:628)" name=retry-fail subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=debug msg="Observer job stopped" error="<nil>" func="job.TestObserver_CtxClose.func1.1 (job_test.go:628)" name=retry-fail subsys=hive
level=info msg="Stop hook executed" duration="11.131µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestRegistry" classname="github.com/cilium/cilium/pkg/hive/job" time="0.000">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestRegistry.func1 (job_test.go:660)" subsys=hive
level=info msg=Invoked duration="8.807µs" function="job.TestRegistry.func1 (job_test.go:660)" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestGroup_JobQueue" classname="github.com/cilium/cilium/pkg/hive/job" time="0.000">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestGroup_JobQueue.func1 (job_test.go:677)" subsys=hive
level=info msg=Invoked duration="11.251µs" function="job.TestGroup_JobQueue.func1 (job_test.go:677)" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestGroup_JobRuntime" classname="github.com/cilium/cilium/pkg/hive/job" time="0.000">
			<system-out><![CDATA[level=debug msg=Invoking function="job.TestGroup_JobRuntime.func1 (job_test.go:703)" subsys=hive
level=info msg=Invoked duration="10.589µs" function="job.TestGroup_JobRuntime.func1 (job_test.go:703)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration=451ns function="*job.group.Start" subsys=hive
level=info msg=Stopping subsys=hive
level=debug msg="Executing stop hook" function="*job.group.Stop" subsys=hive
level=debug msg="Starting one-shot job" func="job.TestGroup_JobRuntime.func2 (job_test.go:711)" name=runtime subsys=hive
level=debug msg="one-shot job finished" func="job.TestGroup_JobRuntime.func2 (job_test.go:711)" name=runtime subsys=hive
level=info msg="Stop hook executed" duration="31.408µs" function="*job.group.Stop" subsys=hive]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hive/job	coverage: 85.2% of statements
goleak: Errors on successful test run: found unexpected goroutines:
[Goroutine 44 in state sleep, with time.Sleep on top of the stack:
goroutine 44 [sleep]:
time.Sleep(0x12a05f200)
	/usr/local/go/src/runtime/time.go:195 +0x135
github.com/cilium/cilium/pkg/hive.(*Hive).fatalOnTimeout.func1()
	/host/pkg/hive/hive.go:306 +0x7c
created by github.com/cilium/cilium/pkg/hive.(*Hive).fatalOnTimeout
	/host/pkg/hive/hive.go:295 +0xaa
]]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/build" tests="5" failures="0" errors="0" id="381" hostname="kind-bpf-next" time="0.005" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestVersion" classname="github.com/cilium/cilium/pkg/hubble/build" time="0.000"></testcase>
		<testcase name="TestVersion/hubble-relay_v1.9.0+g63aa1b8" classname="github.com/cilium/cilium/pkg/hubble/build" time="0.000"></testcase>
		<testcase name="TestVersion/hubble-relay_v1.9.0-rc3+g9907232" classname="github.com/cilium/cilium/pkg/hubble/build" time="0.000"></testcase>
		<testcase name="TestVersion/hubble-relay_v1.9.0" classname="github.com/cilium/cilium/pkg/hubble/build" time="0.000"></testcase>
		<testcase name="TestVersion/hubble-relay" classname="github.com/cilium/cilium/pkg/hubble/build" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/build	coverage: 92.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/container" tests="101" failures="0" errors="0" id="382" hostname="kind-bpf-next" time="0.251" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestRingReader_Previous" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_Previous/read_1,_start_at_position_13" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_Previous/read_2,_start_at_position_13" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_Previous/read_5,_start_at_position_5" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_Previous/read_1,_start_at_position_0" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_Previous/read_1,_start_at_position_0#01" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_Previous/read_1,_start_at_position_14" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_PreviousLost" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_Next" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_Next/read_1,_start_at_position_0" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_Next/read_2,_start_at_position_0" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_Next/read_5,_start_at_position_5" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_Next/read_1,_start_at_position_13" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_Next/read_1,_start_at_position_14" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_NextLost" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_NextFollow" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.100"></testcase>
		<testcase name="TestRingReader_NextFollow/read_1,_start_at_position_0,_expect_timeout=false" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_NextFollow/read_2,_start_at_position_0,_expect_timeout=false" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_NextFollow/read_5,_start_at_position_5,_expect_timeout=false" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_NextFollow/read_1,_start_at_position_13,_expect_timeout=false" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingReader_NextFollow/read_1,_start_at_position_14,_expect_timeout=true" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.100"></testcase>
		<testcase name="TestRingReader_NextFollow_WithEmptyRing" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.100"></testcase>
		<testcase name="TestNewCapacity" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=1" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=3" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=7" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=15" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=31" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=63" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=127" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=255" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=511" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=1023" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=2047" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=4095" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=8191" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=16383" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=32767" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=65535" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity1" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity3" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity7" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity15" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity31" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity63" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity127" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity255" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity511" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity1023" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity2047" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity4095" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity8191" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity16383" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity32767" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=Capacity65535" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=-127" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=-10" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=0" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=2" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=128" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewCapacity/n=131071" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.020"></testcase>
		<testcase name="TestNewRing/n=1" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=3" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=7" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=15" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=31" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=63" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=127" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=255" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=511" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=1023" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=2047" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=4095" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=8191" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=16383" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=32767" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestNewRing/n=65535" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.010"></testcase>
		<testcase name="TestRing_Read" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_Read/normal_read_for_the_index_7" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_Read/we_can&#39;t_read_index_0_since_we_just_wrote_into_it" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_Read/we_can&#39;t_read_index_0x7_since_we_are_one_writing_cycle_ahead" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_Read/we_can_read_index_0x8_since_it&#39;s_the_last_entry_that_we_can_read_in_this_cycle" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_Read/we_overflow_write_and_we_are_trying_to_read_the_previous_writes,_that_we_can&#39;t" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_Read/we_overflow_write_and_we_are_trying_to_read_the_previous_writes,_that_we_can" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_Read/we_overflow_write_and_we_are_trying_to_read_the_2_previously_cycles" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_Write" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_Write/normal_write" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_Write/overflow_write" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_LastWriteParallel" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_LastWriteParallel/#00" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_LastWriteParallel/#01" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_LastWrite" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_LastWrite/#00" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_LastWrite/#01" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingOldestWrite" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingFunctionalityInParallel" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRingFunctionalitySerialized" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_ReadFrom_Test_1" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_ReadFrom_Test_2" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<testcase name="TestRing_ReadFrom_Test_3" classname="github.com/cilium/cilium/pkg/hubble/container" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/container	coverage: 93.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/exporter" tests="2" failures="0" errors="0" id="383" hostname="kind-bpf-next" time="0.026" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestExporter" classname="github.com/cilium/cilium/pkg/hubble/exporter" time="0.000"></testcase>
		<testcase name="TestEventToExportEvent" classname="github.com/cilium/cilium/pkg/hubble/exporter" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/exporter	coverage: 50.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/filters" tests="222" failures="0" errors="0" id="384" hostname="kind-bpf-next" time="0.033" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestEventTypeFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestEventTypeFilter/drop_without_subtype" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestEventTypeFilter/drop_with_subtype" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestEventTypeFilter/agent_event_without_subtype" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestEventTypeFilter/agent_event_with_subtype" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestApply" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestApply/#00" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestApply/#01" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestApply/#02" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestApply/#03" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestApply/#04" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestApply/#05" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestApply/#06" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestApply/#07" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestApply/#08" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestMatch" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestOnBuildFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFQDNFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFQDNFilter/source_fqdn" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFQDNFilter/destination_fqdn" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFQDNFilter/source_and_destination_fqdn" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFQDNFilter/source_or_destination_fqdn" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFQDNFilter/invalid_data" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFQDNFilter/invalid_source_fqdn_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFQDNFilter/invalid_destination_fqdn_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFQDNFilter/wildcard_filters" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByDNSQuery" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByDNSQuery/not-dns" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByDNSQuery/invalid-regex" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByDNSQuery/positive" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByDNSQuery/positive#01" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByDNSQuery/negative" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/status_code_full" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/status_code_prefix" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/invalid_data" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/invalid_empty_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/invalid_catch-all_prefix" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/invalid_status_code" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/invalid_status_code_text" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/invalid_status_code_prefix" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/invalid_status_code_prefix#01" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/empty_event_type_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/compatible_event_type_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/basic_http_method_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/http_method_wrong_type" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/http_method_wrong_type#01" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/path_full" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/invalid_uri" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestHTTPFilters/invalid_path_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIdentityFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIdentityFilter/source-nil" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIdentityFilter/destination-nil" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIdentityFilter/source-positive" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIdentityFilter/source-negative" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIdentityFilter/destination-negative" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPFilter/source_ip" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPFilter/destination_ip" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPFilter/source_and_destination_ip" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPFilter/source_or_destination_ip" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPFilter/invalid_data" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPFilter/invalid_source_ip_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPFilter/invalid_destination_ip_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPFilter/source_cidr" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPFilter/destination_cidr" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPFilter/invalid_source_cidr_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPFilter/invalid_destination_cidr_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPVersionFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPVersionFilter/ipv4_test" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPVersionFilter/ipv6_test" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPVersionFilter/unknown_network_protocol_test" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPVersionFilter/both_ipv4_and_ipv6_allow_test" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPVersionFilter/all_ipv4,ipv6,unknown_allow_test" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPVersionFilter/test_with_non-flow_event" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestIPVersionFilter/test_with_non-flow_event_and_IP_NOT_USED" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPodFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPodFilter/source_pod" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPodFilter/destination_pod" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPodFilter/source_and_destination_pod" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPodFilter/source_or_destination_pod" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPodFilter/namespace_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPodFilter/prefix_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPodFilter/invalid_data" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPodFilter/invalid_source_pod_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPodFilter/invalid_destination_pod_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestServiceFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestServiceFilter/source_service" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestServiceFilter/destination_service" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestLabelSelectorFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestLabelSelectorFilter/label_filter_without_value" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestLabelSelectorFilter/label_filter_with_value" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestLabelSelectorFilter/complex_label_label_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestLabelSelectorFilter/source_and_destination_label_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestLabelSelectorFilter/matchall_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestLabelSelectorFilter/cilium_fixed_prefix_filters" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestLabelSelectorFilter/cilium_any_prefix_filters" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestLabelSelectorFilter/invalid_source_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestLabelSelectorFilter/invalid_destination_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_parseSelector" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_parseSelector/simple_labels" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_parseSelector/complex_labels" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_parseSelector/too_many_colons" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter/no_filter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter/everything" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter/literal_cluster_pattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter/literal_node_pattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter/literal_node_patterns" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter/node_wildcard_pattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter/cluster_wildcard_pattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter/cluster_pattern_and_node_wildcard_pattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter/invalid_empty_pattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter/invalid_cluster_pattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter/invalid_node_pattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestNodeFilter/too_many_slashes" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern/empty" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern/simple" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern/multiple" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern/star" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern/trailing_dot" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern/spaces" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern/upper_case" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern/spaces_trailing_dot_upper_case" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern/underscores" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern/empty_after_trim" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern/invalid_rune" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileFQDNPattern/multiple_trailing_dots" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileNodeNamePatterns" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileNodeNamePatterns/all" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileNodeNamePatterns/node_pattern_only" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileNodeNamePatterns/cluster_pattern_only" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileNodeNamePatterns/wildcard_node_pattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileNodeNamePatterns/multiple_patterns" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileNodeNamePatterns/empty_pattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileNodeNamePatterns/invalid_rune_in_node_pattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileNodeNamePatterns/invalid_rune_in_cluster_pattern" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestCompileNodeNamePatterns/too_many_slashes" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPortFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPortFilter/udp" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPortFilter/tcp" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPortFilter/wrong_direction" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPortFilter/no_port" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestPortFilter/invalid_port" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowProtocolFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowProtocolFilter/udp" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowProtocolFilter/http" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowProtocolFilter/icmp_(v4)" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowProtocolFilter/icmp_(v6)" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowProtocolFilter/multiple_protocols" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowProtocolFilter/invalid_protocols" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByReplyField" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByReplyField/nil_flow" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByReplyField/empty-param" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByReplyField/empty-param-2" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByReplyField/no-reply" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByReplyField/trace-event-from-endpoint" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByReplyField/trace-event-to-endpoint" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByReplyField/reply" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByReplyField/drop_implies_reply=false" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByReplyField/no-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="Test_filterByReplyField/no-match-2" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterSYNACK___eventSYNACK" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterSYNACK___eventSYN" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterSYN_OR_filterACK___eventSYN" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterSYN_OR_filterACK___eventSYNACK" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterSYN_OR_filterACK___eventPSHACK" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterSYN___eventSYN" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterSYN___eventSYNACK" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterFIN___eventRST" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterSYN___eventPSH" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterURG___eventPSH" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterRST___eventRST" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterFIN___eventFIN" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/filterPSH___eventPSHACK" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestFlowTCPFilter/TCP_flow_without_flags" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestTraceIDFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestTraceIDFilter/match_example_trace_ID" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestTraceIDFilter/match_example_trace_ID_with_multiple_input_filters" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestTraceIDFilter/empty_trace_ID_filter_on_flow_without_trace_ID" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestTraceIDFilter/empty_trace_ID_filter_on_flow_with_trace_ID" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestTraceIDFilter/don&#39;t_match_example_trace_ID" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestTraceIDFilter/no_trace_ID_in_flow" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestTrafficDirectionFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestTrafficDirectionFilter/nil" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestTrafficDirectionFilter/match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestTrafficDirectionFilter/no-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestUUIDFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestUUIDFilter/nil" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestUUIDFilter/match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestUUIDFilter/no-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestVerdictFilter" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/source-nil" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/destination-nil" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/source-filter-nil" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/destination-filter-nil" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/source-filter-empty" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/destination-filter-empty" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/source-both-kind-and-name-empty-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/destination-both-kind-and-name-empty-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/source-both-kind-and-name-empty-mismatch" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/destination-both-kind-and-name-empty-mismatch" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/source-kind-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/destination-kind-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/source-name-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/destination-name-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/source-kind-and-name-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/destination-kind-and-name-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/source-kind-mismatch" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/destination-kind-mismatch" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/source-name-mismatch" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/destination-name-mismatch" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/source-kind-and-name-mismatch" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterInclude/destination-kind-and-name-mismatch" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterExclude" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterExclude/source-both-kind-and-name-empty-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterExclude/destination-both-kind-and-name-empty-match" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterExclude/source-both-kind-and-name-empty-mismatch" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<testcase name="TestWorkloadFilterExclude/destination-both-kind-and-name-empty-mismatch" classname="github.com/cilium/cilium/pkg/hubble/filters" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/filters	coverage: 93.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/k8s" tests="5" failures="0" errors="0" id="385" hostname="kind-bpf-next" time="0.004" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestParseNamespaceName" classname="github.com/cilium/cilium/pkg/hubble/k8s" time="0.000"></testcase>
		<testcase name="TestParseNamespaceName/#00" classname="github.com/cilium/cilium/pkg/hubble/k8s" time="0.000"></testcase>
		<testcase name="TestParseNamespaceName/#01" classname="github.com/cilium/cilium/pkg/hubble/k8s" time="0.000"></testcase>
		<testcase name="TestParseNamespaceName/#02" classname="github.com/cilium/cilium/pkg/hubble/k8s" time="0.000"></testcase>
		<testcase name="TestParseNamespaceName/#03" classname="github.com/cilium/cilium/pkg/hubble/k8s" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/k8s	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/math" tests="11" failures="0" errors="0" id="386" hostname="kind-bpf-next" time="0.003" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test_msb" classname="github.com/cilium/cilium/pkg/hubble/math" time="0.000"></testcase>
		<testcase name="Test_msb/#00" classname="github.com/cilium/cilium/pkg/hubble/math" time="0.000"></testcase>
		<testcase name="Test_msb/#01" classname="github.com/cilium/cilium/pkg/hubble/math" time="0.000"></testcase>
		<testcase name="Test_msb/#02" classname="github.com/cilium/cilium/pkg/hubble/math" time="0.000"></testcase>
		<testcase name="Test_msb/#03" classname="github.com/cilium/cilium/pkg/hubble/math" time="0.000"></testcase>
		<testcase name="Test_getmask" classname="github.com/cilium/cilium/pkg/hubble/math" time="0.000"></testcase>
		<testcase name="Test_getmask/#00" classname="github.com/cilium/cilium/pkg/hubble/math" time="0.000"></testcase>
		<testcase name="Test_getmask/#01" classname="github.com/cilium/cilium/pkg/hubble/math" time="0.000"></testcase>
		<testcase name="Test_getmask/#02" classname="github.com/cilium/cilium/pkg/hubble/math" time="0.000"></testcase>
		<testcase name="Test_getmask/#03" classname="github.com/cilium/cilium/pkg/hubble/math" time="0.000"></testcase>
		<testcase name="Test_getmask/#04" classname="github.com/cilium/cilium/pkg/hubble/math" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/math	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/metrics" tests="3" failures="0" errors="0" id="387" hostname="kind-bpf-next" time="0.035" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestUninitializedMetrics" classname="github.com/cilium/cilium/pkg/hubble/metrics" time="0.000"></testcase>
		<testcase name="TestInitializedMetrics" classname="github.com/cilium/cilium/pkg/hubble/metrics" time="0.010"></testcase>
		<testcase name="TestInitializedMetrics/Should_send_pod_removal_to_delayed_delivery_queue" classname="github.com/cilium/cilium/pkg/hubble/metrics" time="0.010"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/metrics	coverage: 13.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/metrics/api" tests="24" failures="0" errors="0" id="388" hostname="kind-bpf-next" time="0.011" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestDefaultRegistry" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="TestParseMetricOptions" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="TestParseContextOptions" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="TestParseGetLabelValues" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="TestShortenPodName" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="Test_reservedIdentityContext" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="Test_workloadNameContext" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="Test_appContext" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="Test_labelsSetString" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="Test_labelsSetString/nil" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="Test_labelsSetString/empty" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="Test_labelsSetString/invalid" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="Test_labelsSetString/single" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="Test_labelsSetString/duplicated" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="Test_labelsSetString/two" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="Test_labelsSetString/three" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="TestParseOptions" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="TestRegister" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="TestRegister/Should_not_register_handler" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="TestRegister/Should_register_handler" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="TestRegister/Should_remove_metrics_series_with_ContextPod" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="TestRegister/Should_not_remove_metrics_series_with_ContextPodShort" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="TestRegister/Should_remove_metrics_series_with_LabelsContext" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<testcase name="TestRegister/Should_not_remove_metrics_series_with_LabelsContext_without_namespace" classname="github.com/cilium/cilium/pkg/hubble/metrics/api" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/metrics/api	coverage: 92.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/metrics/drop" tests="5" failures="0" errors="0" id="389" hostname="kind-bpf-next" time="0.014" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestDropHandler" classname="github.com/cilium/cilium/pkg/hubble/metrics/drop" time="0.000"></testcase>
		<testcase name="TestDropHandler/Init" classname="github.com/cilium/cilium/pkg/hubble/metrics/drop" time="0.000"></testcase>
		<testcase name="TestDropHandler/Status" classname="github.com/cilium/cilium/pkg/hubble/metrics/drop" time="0.000"></testcase>
		<testcase name="TestDropHandler/ProcessFlow_ShouldReportNothingForForwardedFlow" classname="github.com/cilium/cilium/pkg/hubble/metrics/drop" time="0.000"></testcase>
		<testcase name="TestDropHandler/ProcessFlow_ShouldReportDroppedFlow" classname="github.com/cilium/cilium/pkg/hubble/metrics/drop" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/metrics/drop	coverage: 73.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/metrics/flow" tests="4" failures="0" errors="0" id="390" hostname="kind-bpf-next" time="0.015" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestFlowHandler" classname="github.com/cilium/cilium/pkg/hubble/metrics/flow" time="0.000"></testcase>
		<testcase name="TestFlowHandler/Init" classname="github.com/cilium/cilium/pkg/hubble/metrics/flow" time="0.000"></testcase>
		<testcase name="TestFlowHandler/Status" classname="github.com/cilium/cilium/pkg/hubble/metrics/flow" time="0.000"></testcase>
		<testcase name="TestFlowHandler/ProcessFlow" classname="github.com/cilium/cilium/pkg/hubble/metrics/flow" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/metrics/flow	coverage: 68.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/metrics/flows-to-world" tests="6" failures="0" errors="0" id="391" hostname="kind-bpf-next" time="0.018" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestFlowsToWorldHandler_MatchingFlow" classname="github.com/cilium/cilium/pkg/hubble/metrics/flows-to-world" time="0.000"></testcase>
		<testcase name="TestFlowsToWorldHandler_NonMatchingFlows" classname="github.com/cilium/cilium/pkg/hubble/metrics/flows-to-world" time="0.000"></testcase>
		<testcase name="TestFlowsToWorldHandler_AnyDrop" classname="github.com/cilium/cilium/pkg/hubble/metrics/flows-to-world" time="0.000"></testcase>
		<testcase name="TestFlowsToWorldHandler_IncludePort" classname="github.com/cilium/cilium/pkg/hubble/metrics/flows-to-world" time="0.000"></testcase>
		<testcase name="TestFlowsToWorldHandler_SynOnly" classname="github.com/cilium/cilium/pkg/hubble/metrics/flows-to-world" time="0.000"></testcase>
		<testcase name="Test_flowsToWorldHandler_Status" classname="github.com/cilium/cilium/pkg/hubble/metrics/flows-to-world" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/metrics/flows-to-world	coverage: 90.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/metrics/http" tests="6" failures="0" errors="0" id="392" hostname="kind-bpf-next" time="0.009" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test_httpHandler_Status" classname="github.com/cilium/cilium/pkg/hubble/metrics/http" time="0.000"></testcase>
		<testcase name="Test_httpHandler_ProcessFlow" classname="github.com/cilium/cilium/pkg/hubble/metrics/http" time="0.000"></testcase>
		<testcase name="Test_httpHandlerV2_ProcessFlow" classname="github.com/cilium/cilium/pkg/hubble/metrics/http" time="0.000"></testcase>
		<testcase name="Test_httpHandler_ListMetricVec" classname="github.com/cilium/cilium/pkg/hubble/metrics/http" time="0.000"></testcase>
		<testcase name="Test_httpV2Handler_ListMetricVec" classname="github.com/cilium/cilium/pkg/hubble/metrics/http" time="0.000"></testcase>
		<testcase name="Test_httpPlugin_HelpText" classname="github.com/cilium/cilium/pkg/hubble/metrics/http" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/metrics/http	coverage: 86.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/metrics/policy" tests="1" failures="0" errors="0" id="393" hostname="kind-bpf-next" time="0.017" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestPolicyHandler" classname="github.com/cilium/cilium/pkg/hubble/metrics/policy" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/metrics/policy	coverage: 77.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/metrics/port-distribution" tests="7" failures="0" errors="0" id="394" hostname="kind-bpf-next" time="0.009" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestPortDistributionHandler" classname="github.com/cilium/cilium/pkg/hubble/metrics/port-distribution" time="0.000"></testcase>
		<testcase name="TestPortDistributionHandler/Init" classname="github.com/cilium/cilium/pkg/hubble/metrics/port-distribution" time="0.000"></testcase>
		<testcase name="TestPortDistributionHandler/Status" classname="github.com/cilium/cilium/pkg/hubble/metrics/port-distribution" time="0.000"></testcase>
		<testcase name="TestPortDistributionHandler/ProcessFlow_SkipReply" classname="github.com/cilium/cilium/pkg/hubble/metrics/port-distribution" time="0.000"></testcase>
		<testcase name="TestPortDistributionHandler/ProcessFlow_SkipDropped" classname="github.com/cilium/cilium/pkg/hubble/metrics/port-distribution" time="0.000"></testcase>
		<testcase name="TestPortDistributionHandler/ProcessFlow" classname="github.com/cilium/cilium/pkg/hubble/metrics/port-distribution" time="0.000"></testcase>
		<testcase name="TestPortDistributionHandler/ProcessFlow_MultiplePorts" classname="github.com/cilium/cilium/pkg/hubble/metrics/port-distribution" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/metrics/port-distribution	coverage: 62.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/metrics/tcp" tests="16" failures="0" errors="0" id="395" hostname="kind-bpf-next" time="0.007" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestTcpHandler_Init" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler_Init/Init" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler_Init/Status" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler/ProcessSupportedFlagsFlow_SYN" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler/ProcessSupportedFlagsFlow_SYN_ACK" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler/ProcessSupportedFlagsFlow_FIN" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler/ProcessSupportedFlagsFlow_FIN_ACK" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler/ProcessSupportedFlagsFlow_RST" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler/ProcessUnsupportedFlagsFlow_empty" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler/ProcessUnsupportedFlagsFlow_PSH" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler/ProcessUnsupportedFlagsFlow_ACK" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler/ProcessUnsupportedFlagsFlow_URG" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler/ProcessUnsupportedFlagsFlow_ECE" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler/ProcessUnsupportedFlagsFlow_CWR" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<testcase name="TestTcpHandler/ProcessUnsupportedFlagsFlow_NS" classname="github.com/cilium/cilium/pkg/hubble/metrics/tcp" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/metrics/tcp	coverage: 78.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/monitor" tests="22" failures="0" errors="0" id="396" hostname="kind-bpf-next" time="0.035" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestHubbleConsumer" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000">
			<system-out><![CDATA[time="2023-05-31T11:42:39Z" level=warning msg="hubble events queue is full: dropping messages; consider increasing the queue size (hubble-event-queue-size) or provisioning more CPU" related-metric=hubble_lost_events_total]]></system-out>
		</testcase>
		<testcase name="TestNewMonitorFilter" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="TestNewMonitorFilter/unknown_filter" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="TestNewMonitorFilter/valid_filters" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/nil_payload" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/unknown_event_type" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/unknown_observerTypes.AgentEvent" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/monitorAPI.MessageTypeAgent" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/monitorAPI.MessageTypeAccessLog" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/empty_observerTypes.PerfEvent" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/monitorAPI.MessageTypeDrop" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/monitorAPI.MessageTypeDebug" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/monitorAPI.MessageTypeCapture" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/monitorAPI.MessageTypeTrace" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/monitorAPI.MessageTypePolicyVerdict" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/monitorAPI.MessageTypeRecCapture" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/monitorAPI.MessageTypeTraceSock" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/composite_filter_with_debug,trace" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/composite_filter_with_debug,policy-verdict" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/composite_filter_with_capture,trace" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<testcase name="Test_OnMonitorEvent/monitorAPI.MessageTypeNamePolicyVerdict_should_drop_everything_else_except_monitorAPI.MessageTypePolicyVerdict" classname="github.com/cilium/cilium/pkg/hubble/monitor" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/monitor	coverage: 98.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/observer" tests="8" failures="0" errors="0" id="397" hostname="kind-bpf-next" time="0.050" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestNewLocalServer" classname="github.com/cilium/cilium/pkg/hubble/observer" time="0.000"></testcase>
		<testcase name="TestLocalObserverServer_ServerStatus" classname="github.com/cilium/cilium/pkg/hubble/observer" time="0.000"></testcase>
		<testcase name="TestLocalObserverServer_GetFlows" classname="github.com/cilium/cilium/pkg/hubble/observer" time="0.000"></testcase>
		<testcase name="TestLocalObserverServer_GetAgentEvents" classname="github.com/cilium/cilium/pkg/hubble/observer" time="0.000"></testcase>
		<testcase name="TestLocalObserverServer_GetFlows_Follow_Since" classname="github.com/cilium/cilium/pkg/hubble/observer" time="0.000"></testcase>
		<testcase name="TestHooks" classname="github.com/cilium/cilium/pkg/hubble/observer" time="0.000"></testcase>
		<testcase name="TestLocalObserverServer_OnFlowDelivery" classname="github.com/cilium/cilium/pkg/hubble/observer" time="0.000"></testcase>
		<testcase name="TestLocalObserverServer_OnGetFlows" classname="github.com/cilium/cilium/pkg/hubble/observer" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/observer	coverage: 70.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/parser" tests="3" failures="0" errors="0" id="398" hostname="kind-bpf-next" time="0.034" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test_InvalidPayloads" classname="github.com/cilium/cilium/pkg/hubble/parser" time="0.000"></testcase>
		<testcase name="Test_ParserDispatch" classname="github.com/cilium/cilium/pkg/hubble/parser" time="0.000"></testcase>
		<testcase name="Test_EventType_RecordLost" classname="github.com/cilium/cilium/pkg/hubble/parser" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/parser	coverage: 63.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/parser/agent" tests="16" failures="0" errors="0" id="399" hostname="kind-bpf-next" time="0.006" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestDecodeAgentEvent" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/empty" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/unspecified" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/type_and_notification_type_mismatch" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/StartMessage" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/PolicyUpdateMessage" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/PolicyDeleteMessage" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/EndpointRegenMessage_success" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/EndpointRegenMessage_failure" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/EndpointCreateMessage" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/EndpointDeleteMessage" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/IPCacheUpsertedMessage_(insert)" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/IPCacheUpsertedMessage_(update)" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/IPCacheDeletedMessage" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/ServiceUpsertMessage" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<testcase name="TestDecodeAgentEvent/ServiceDeleteMessage" classname="github.com/cilium/cilium/pkg/hubble/parser/agent" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/parser/agent	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/parser/debug" tests="7" failures="0" errors="0" id="400" hostname="kind-bpf-next" time="0.039" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestDecodeDebugEvent" classname="github.com/cilium/cilium/pkg/hubble/parser/debug" time="0.000"></testcase>
		<testcase name="TestDecodeDebugEvent/Generic_event" classname="github.com/cilium/cilium/pkg/hubble/parser/debug" time="0.000"></testcase>
		<testcase name="TestDecodeDebugEvent/IPv4_Mapping" classname="github.com/cilium/cilium/pkg/hubble/parser/debug" time="0.000"></testcase>
		<testcase name="TestDecodeDebugEvent/ICMP6_Handle" classname="github.com/cilium/cilium/pkg/hubble/parser/debug" time="0.000"></testcase>
		<testcase name="TestDecodeDebugEvent/Unknown_event" classname="github.com/cilium/cilium/pkg/hubble/parser/debug" time="0.000"></testcase>
		<testcase name="TestDecodeDebugEvent/No_data" classname="github.com/cilium/cilium/pkg/hubble/parser/debug" time="0.000"></testcase>
		<testcase name="TestDecodeDebugEvent/Invalid_data" classname="github.com/cilium/cilium/pkg/hubble/parser/debug" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/parser/debug	coverage: 94.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/parser/seven" tests="22" failures="0" errors="0" id="401" hostname="kind-bpf-next" time="0.032" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestDecodeL7DNSRecord" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="TestDecodeL7HTTPRequest" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="TestDecodeL7HTTPRecordResponse" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="TestDecodeL7HTTPResponseTime" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="TestGetL7HTTPResponseTraceID" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="Test_decodeKafka" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="Test_decodeKafka/request" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="Test_decodeKafka/response" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="Test_decodeKafka/empty-topic" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="Test_kafkaSummary" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="Test_kafkaSummary/request" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="Test_kafkaSummary/response" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="Test_kafkaSummary/nil" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="Test_decodeVerdict" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="TestExtractTraceContext" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="TestExtractTraceContext/nil_log_record" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="TestExtractTraceContext/http_log_record_without_trace" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="TestExtractTraceContext/http_log_record_with_trace" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="TestTraceIDFromHTTPHeader" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="TestTraceIDFromHTTPHeader/no_trace" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="TestTraceIDFromHTTPHeader/example_traceparent" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<testcase name="TestTraceIDFromHTTPHeader/invalid_trace" classname="github.com/cilium/cilium/pkg/hubble/parser/seven" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/parser/seven	coverage: 86.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/parser/sock" tests="9" failures="0" errors="0" id="402" hostname="kind-bpf-next" time="0.032" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestDecodeSockEvent" classname="github.com/cilium/cilium/pkg/hubble/parser/sock" time="0.000"></testcase>
		<testcase name="TestDecodeSockEvent/empty_buffer" classname="github.com/cilium/cilium/pkg/hubble/parser/sock" time="0.000"></testcase>
		<testcase name="TestDecodeSockEvent/invalid_buffer" classname="github.com/cilium/cilium/pkg/hubble/parser/sock" time="0.000"></testcase>
		<testcase name="TestDecodeSockEvent/empty_event" classname="github.com/cilium/cilium/pkg/hubble/parser/sock" time="0.000"></testcase>
		<testcase name="TestDecodeSockEvent/invalid_cgroup_id" classname="github.com/cilium/cilium/pkg/hubble/parser/sock" time="0.000"></testcase>
		<testcase name="TestDecodeSockEvent/minimal" classname="github.com/cilium/cilium/pkg/hubble/parser/sock" time="0.000"></testcase>
		<testcase name="TestDecodeSockEvent/pre-translate_v4_xwing_to_service_ip" classname="github.com/cilium/cilium/pkg/hubble/parser/sock" time="0.000"></testcase>
		<testcase name="TestDecodeSockEvent/post-translate_v4_xwing_to_remote_pod_ip" classname="github.com/cilium/cilium/pkg/hubble/parser/sock" time="0.000"></testcase>
		<testcase name="TestDecodeSockEvent/post-translate_rev_v6_xwing_from_service_ip" classname="github.com/cilium/cilium/pkg/hubble/parser/sock" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/parser/sock	coverage: 90.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/parser/threefour" tests="21" failures="0" errors="0" id="403" hostname="kind-bpf-next" time="0.033" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestL34DecodeEmpty" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestL34Decode" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestDecodeTraceNotify" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestDecodeDropNotify" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestDecodePolicyVerdictNotify" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestDecodeDropReason" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestDecodeLocalIdentity" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestDecodeTrafficDirection" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestDecodeIsReply" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="Test_filterCIDRLabels" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="Test_filterCIDRLabels/mixed" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="Test_filterCIDRLabels/mixed,_IPv6" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="Test_filterCIDRLabels/no-cidr" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="Test_filterCIDRLabels/cidr-only" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="Test_filterCIDRLabels/cidr-only,_IPv6" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="Test_filterCIDRLabels/empty" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestTraceNotifyOriginalIP" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestICMP" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestTraceNotifyLocalEndpoint" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestDebugCapture" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<testcase name="TestTraceNotifyProxyPort" classname="github.com/cilium/cilium/pkg/hubble/parser/threefour" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/parser/threefour	coverage: 90.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/peer" tests="43" failures="0" errors="0" id="404" hostname="kind-bpf-next" time="0.143" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestBufferPush" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestBufferPop" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestBufferPopWithClosedStopChan" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeAdd" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeAdd/node_with_just_a_name" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeAdd/node_with_just_a_name_and_cluster" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeAdd/node_with_name,_cluster_and_one_internal_IP_address" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeAdd/node_with_name,_cluster_and_one_external_IP_address" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeAdd/node_with_name,_cluster_and_mixed_IP_addresses_preferring_IPv4" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeAdd/node_with_name,_cluster_and_mixed_IP_addresses_preferring_IPv6" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeAdd/node_with_a_name_and_withTLS_is_set" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeAdd/node_with_name,_cluster_and_withTLS_is_set" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeAdd/node_name_with_dots" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeAdd/node_name_with_dots_in_the_cluster_name_section" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeUpdate" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeUpdate/a_node_is_renamed" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeUpdate/a_node_within_a_named_cluster_is_renamed" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeUpdate/a_node_with_name,_cluster_and_one_internal_IP_address,_the_latter_is_updated" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeUpdate/node_with_name,_cluster_and_one_external_IP_address,_the_latter_is_updated" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeUpdate/node_with_name,_cluster_and_mixed_IP_addresses_preferring_IPv4,_the_latter_is_updated" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeUpdate/node_with_name,_cluster_and_mixed_IP_addresses_preferring_IPv6,_the_latter_is_updated" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeUpdate/node_with_name,_cluster_and_one_external_IP_address,_no_name_or_address_change" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeUpdate/a_node_is_renamed_and_withTLS_is_set" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeUpdate/a_node_within_a_named_cluster_is_renamed_and_withTLS_is_set" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeDelete" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeDelete/node_with_just_a_name" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeDelete/node_with_just_a_name_and_cluster" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeDelete/node_with_name,_cluster_and_one_internal_IP_address" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeDelete/node_with_name,_cluster_and_one_external_IP_address" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeDelete/node_with_name,_cluster_and_mixed_IP_addresses_preferring_IPv4" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeDelete/node_with_name,_cluster_and_mixed_IP_addresses_preferring_IPv6" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeDelete/node_with_a_name_and_withTLS_is_set" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestNodeDelete/node_with_a_name_and_cluster_and_withTLS_is_set" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestService_Notify" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestService_Notify/add_4_nodes_with_TLS_info_disabled" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestService_Notify/delete_3_nodes_with_TLS_info_disabled" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestService_Notify/update_2_nodes_with_TLS_info_disabled" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestService_Notify/rename_2_nodes_with_TLS_info_disabled" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestService_Notify/add_4_nodes" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestService_Notify/delete_3_nodes" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestService_Notify/update_2_nodes" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestService_Notify/rename_2_nodes" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.000"></testcase>
		<testcase name="TestService_NotifyWithBlockedSend" classname="github.com/cilium/cilium/pkg/hubble/peer" time="0.100"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/peer	coverage: 94.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/peer/types" tests="13" failures="0" errors="0" id="405" hostname="kind-bpf-next" time="0.024" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestFromChangeNotification" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<testcase name="TestFromChangeNotification/nil" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<testcase name="TestFromChangeNotification/without_address" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<testcase name="TestFromChangeNotification/with_IPv4_address_but_without_port" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<testcase name="TestFromChangeNotification/with_an_IPv4_address_and_port" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<testcase name="TestFromChangeNotification/with_an_IPv4_address_and_a_bad_port" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<testcase name="TestFromChangeNotification/with_an_IPv6_address_but_without_port" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<testcase name="TestFromChangeNotification/with_an_IPv6_address_and_port" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<testcase name="TestFromChangeNotification/with_an_IPv6_address_and_a_bad_port" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<testcase name="TestFromChangeNotification/with_a_unix_domain_socket" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<testcase name="TestFromChangeNotification/with_a_unix_domain_socket_without_prefix" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<testcase name="TestFromChangeNotification/with_an_IPv4_address_and_port_and_TLS_enabled" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<testcase name="TestFromChangeNotification/with_an_IPv4_address_and_port_and_TLS_enabled_with_server_name" classname="github.com/cilium/cilium/pkg/hubble/peer/types" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/peer/types	coverage: 51.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/relay/observer" tests="15" failures="0" errors="0" id="406" hostname="kind-bpf-next" time="0.032" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestGetFlows" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestGetFlows/Observe_0_flows_from_1_peer_without_address" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestGetFlows/Observe_4_flows_from_2_online_peers" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestGetFlows/Observe_2_flows_from_1_online_peer_and_none_from_1_unavailable_peer" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestGetNodes" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestGetNodes/1_peer_without_address" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestGetNodes/2_connected_peers" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestGetNodes/2_connected_peers_with_TLS" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestGetNodes/1_connected_peer,_1_unreachable_peer" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestGetNodes/1_connected_peer,_1_unreachable_peer,_1_peer_with_error" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestServerStatus" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestServerStatus/1_peer_without_address" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestServerStatus/2_connected_peers" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestServerStatus/1_connected_peer,_1_unreachable_peer" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<testcase name="TestServerStatus/2_unreachable_peers" classname="github.com/cilium/cilium/pkg/hubble/relay/observer" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/relay/observer	coverage: 70.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/relay/pool" tests="14" failures="0" errors="0" id="407" hostname="kind-bpf-next" time="3.042" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestPeerManager" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="3.000"></testcase>
		<testcase name="TestPeerManager/empty_pool" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="0.000"></testcase>
		<testcase name="TestPeerManager/1_peer_without_IP_address" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="0.000"></testcase>
		<testcase name="TestPeerManager/1_unreachable_peer" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="0.000"></testcase>
		<testcase name="TestPeerManager/1_reachable_peer" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="1.000"></testcase>
		<testcase name="TestPeerManager/1_peer_is_deleted" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="0.000"></testcase>
		<testcase name="TestPeerManager/1_peer_in_transient_failure" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="1.000"></testcase>
		<testcase name="TestPeerManager/1_peer_added_then_modified" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="1.000"></testcase>
		<testcase name="TestPeerManager/2_peers_added,_1_deleted" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="0.000"></testcase>
		<testcase name="TestPeerManager/2_peers_added,_1_deleted,_TLS_enabled" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="0.000"></testcase>
		<testcase name="TestPeerManager/PeerClientBuilder_errors_out" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="0.000"></testcase>
		<testcase name="TestPeerManager/ClientConnBuilder_errors_out" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="0.000"></testcase>
		<testcase name="TestPeerManager/peer_notify_errors_out" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="0.000"></testcase>
		<testcase name="TestPeerManager/peer_recv_errors_out" classname="github.com/cilium/cilium/pkg/hubble/relay/pool" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/relay/pool	coverage: 76.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/relay/queue" tests="9" failures="0" errors="0" id="408" hostname="kind-bpf-next" time="0.006" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestPriorityQueue" classname="github.com/cilium/cilium/pkg/hubble/relay/queue" time="0.000"></testcase>
		<testcase name="TestPriorityQueue_WithobjectsInTheSameSecond" classname="github.com/cilium/cilium/pkg/hubble/relay/queue" time="0.000"></testcase>
		<testcase name="TestPriorityQueue_WithInitialCapacity0" classname="github.com/cilium/cilium/pkg/hubble/relay/queue" time="0.000"></testcase>
		<testcase name="TestPriorityQueue_GrowingOverInitialCapacity" classname="github.com/cilium/cilium/pkg/hubble/relay/queue" time="0.000"></testcase>
		<testcase name="TestPriorityQueue_PopOlderThan" classname="github.com/cilium/cilium/pkg/hubble/relay/queue" time="0.000"></testcase>
		<testcase name="TestPriorityQueue_PopOlderThan/some_older,_some_newer" classname="github.com/cilium/cilium/pkg/hubble/relay/queue" time="0.000"></testcase>
		<testcase name="TestPriorityQueue_PopOlderThan/all_olders" classname="github.com/cilium/cilium/pkg/hubble/relay/queue" time="0.000"></testcase>
		<testcase name="TestPriorityQueue_PopOlderThan/all_more_recent" classname="github.com/cilium/cilium/pkg/hubble/relay/queue" time="0.000"></testcase>
		<testcase name="TestPriorityQueue_PopOlderThan/empty_queue" classname="github.com/cilium/cilium/pkg/hubble/relay/queue" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/hubble/relay/queue	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/hubble/testutils" tests="7" failures="0" errors="0" id="409" hostname="kind-bpf-next" time="0.056" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestCreateL3L4Payload" classname="github.com/cilium/cilium/pkg/hubble/testutils" time="0.000"></testcase>
		<testcase name="TestCreateL3L4Payload/ICMPv4_Echo_Reply" classname="github.com/cilium/cilium/pkg/hubble/testutils" time="0.000"></testcase>
		<testcase name="TestCreateL3L4Payload/ICMPv6_Neighbor_Solicitation" classname="github.com/cilium/cilium/pkg/hubble/testutils" time="0.000"></testcase>
		<testcase name="TestCreateL3L4Payload/ICMPv4_Echo_Reply_Reversed" classname="github.com/cilium/cilium/pkg/hubble/testutils" time="0.000"></testcase>
		<testcase name="TestCreateL3L4Payload/ICMPv6_Neighbor_Solicitation_Reversed" classname="github.com/cilium/cilium/pkg/hubble/testutils" time="0.000"></testcase>
		<testcase name="TestCreateL3L4Payload/802.11q_ICMPv4_Echo_Reply" classname="github.com/cilium/cilium/pkg/hubble/testutils" time="0.000"></testcase>
		<testcase name="TestCreateL3L4Payload/802.11q_ICMPv6_Neighbor_Solicitation" classname="github.com/cilium/cilium/pkg/hubble/testutils" time="0.000"></testcase>
		<system-out><![CDATA[testing: warning: no tests to run
	github.com/cilium/cilium/pkg/hubble/relay/server	coverage: 1.7% of statements
ok  	github.com/cilium/cilium/pkg/hubble/relay/server	0.039s	coverage: 1.7% of statements [no tests to run]
	github.com/cilium/cilium/pkg/hubble/testutils	coverage: 4.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/iana" tests="3" failures="0" errors="0" id="410" hostname="kind-bpf-next" time="0.005" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/iana" time="0.000"></testcase>
		<testcase name="Test/IANATestSuite" classname="github.com/cilium/cilium/pkg/iana" time="0.000"></testcase>
		<testcase name="Test/IANATestSuite/TestIsSvcName" classname="github.com/cilium/cilium/pkg/iana" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/iana	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/identity" tests="29" failures="0" errors="0" id="411" hostname="kind-bpf-next" time="0.018" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="Test/IdentityTestSuite" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="Test/IdentityTestSuite/TestClusterID" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="Test/IdentityTestSuite/TestIsReservedIdentity" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="Test/IdentityTestSuite/TestLocalIdentity" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="Test/IdentityTestSuite/TestNewIdentityFromLabelArray" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="Test/IdentityTestSuite/TestRequiresGlobalIdentity" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="Test/IdentityTestSuite/TestReservedID" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels/nil" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels/host" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels/non-reserved" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels/non-reserved-2" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels/health" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels/remote-node" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels/kube-apiserver" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels/kube-apiserver-and-host" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels/host-and-kube-apiserver" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels/kube-apiserver-and-remote-node" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels/remote-node-and-kube-apiserver" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestLookupReservedIdentityByLabels/ingress" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestIPIdentityPair_PrefixString" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestIPIdentityPair_PrefixString/IPv4_with_mask" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestIPIdentityPair_PrefixString/IPv4_without_mask" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestIPIdentityPair_PrefixString/IPv4_encoded_as_IPv6_with_mask" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestIPIdentityPair_PrefixString/IPv4_encoded_as_IPv6_without_mask" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestIPIdentityPair_PrefixString/IPv6_local_with_mask" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestIPIdentityPair_PrefixString/IPv6_local_without_mask" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<testcase name="TestGetAllReservedIdentities" classname="github.com/cilium/cilium/pkg/identity" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/identity	coverage: 40.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/identity/cache" tests="4" failures="0" errors="0" id="412" hostname="kind-bpf-next" skipped="3" time="0.031" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/identity/cache" time="0.000"></testcase>
		<testcase name="Test/IdentityAllocatorEtcdSuite" classname="github.com/cilium/cilium/pkg/identity/cache" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/IdentityAllocatorConsulSuite" classname="github.com/cilium/cilium/pkg/identity/cache" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/IdentityCacheTestSuite" classname="github.com/cilium/cilium/pkg/identity/cache" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/identity/cache	coverage: 0.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/identity/identitymanager" tests="6" failures="0" errors="0" id="413" hostname="kind-bpf-next" time="0.021" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/identity/identitymanager" time="0.000"></testcase>
		<testcase name="Test/IdentityManagerTestSuite" classname="github.com/cilium/cilium/pkg/identity/identitymanager" time="0.000"></testcase>
		<testcase name="Test/IdentityManagerTestSuite/TestHostIdentityLifecycle" classname="github.com/cilium/cilium/pkg/identity/identitymanager" time="0.000"></testcase>
		<testcase name="Test/IdentityManagerTestSuite/TestIdentityManagerLifecycle" classname="github.com/cilium/cilium/pkg/identity/identitymanager" time="0.000"></testcase>
		<testcase name="Test/IdentityManagerTestSuite/TestLocalEndpointIdentityAdded" classname="github.com/cilium/cilium/pkg/identity/identitymanager" time="0.000"></testcase>
		<testcase name="Test/IdentityManagerTestSuite/TestLocalEndpointIdentityRemoved" classname="github.com/cilium/cilium/pkg/identity/identitymanager" time="0.000">
			<system-out><![CDATA[level=error msg="removing identity not added to the identity manager!" identity=12345 subsys=identitymanager]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/identity/identitymanager	coverage: 68.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/idpool" tests="10" failures="0" errors="0" id="414" hostname="kind-bpf-next" time="0.676" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/idpool" time="0.670"></testcase>
		<testcase name="Test/IDPoolTestSuite" classname="github.com/cilium/cilium/pkg/idpool" time="0.670"></testcase>
		<testcase name="Test/IDPoolTestSuite/TestAllocateID" classname="github.com/cilium/cilium/pkg/idpool" time="0.670"></testcase>
		<testcase name="Test/IDPoolTestSuite/TestInsertIDs" classname="github.com/cilium/cilium/pkg/idpool" time="0.000"></testcase>
		<testcase name="Test/IDPoolTestSuite/TestInsertRemoveIDs" classname="github.com/cilium/cilium/pkg/idpool" time="0.000"></testcase>
		<testcase name="Test/IDPoolTestSuite/TestLeaseAvailableID" classname="github.com/cilium/cilium/pkg/idpool" time="0.000"></testcase>
		<testcase name="Test/IDPoolTestSuite/TestOperationsOnAvailableIDs" classname="github.com/cilium/cilium/pkg/idpool" time="0.000"></testcase>
		<testcase name="Test/IDPoolTestSuite/TestOperationsOnLeasedIDs" classname="github.com/cilium/cilium/pkg/idpool" time="0.000"></testcase>
		<testcase name="Test/IDPoolTestSuite/TestOperationsOnUnavailableIDs" classname="github.com/cilium/cilium/pkg/idpool" time="0.000"></testcase>
		<testcase name="Test/IDPoolTestSuite/TestReleaseID" classname="github.com/cilium/cilium/pkg/idpool" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/idpool	coverage: 96.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/inctimer" tests="2" failures="0" errors="0" id="415" hostname="kind-bpf-next" time="0.210" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestTimerAfter" classname="github.com/cilium/cilium/pkg/inctimer" time="0.100"></testcase>
		<testcase name="TestTimerHardReset" classname="github.com/cilium/cilium/pkg/inctimer" time="0.110"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/inctimer	coverage: 88.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ip" tests="41" failures="0" errors="0" id="416" hostname="kind-bpf-next" time="0.022" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestPrefixToIPNet" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestAddrToIPNet" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestIPToNetPrefix" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestNetsContainsAny" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestNetsContainsAny/a_contains_b" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestNetsContainsAny/b_contains_a" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestNetsContainsAny/a_equals_b" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/ip" time="0.020"></testcase>
		<testcase name="Test/IPTestSuite" classname="github.com/cilium/cilium/pkg/ip" time="0.020"></testcase>
		<testcase name="Test/IPTestSuite/TestAddrFromIP" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestByteFunctions" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestCoalesceCIDRs" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestCountIPs" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestCreateSpanningCIDR" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestFirstIP" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestGetIPAtIndex" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestGetIPFromListByFamily" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestIPListEquals" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestIPNetToRange" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestIPVersion" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestKeepUniqueIPs" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestMustAddrsFromIPs" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestNetsByRange" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestNextIP" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestPartitionCIDR" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestPreviousIP" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestRangeToCIDRs" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestRemoveCIDRs" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestRemoveCIDRsEdgeCases" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestRemoveRedundant" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="Test/IPTestSuite/TestRemoveSameCIDR" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestKeepUniqueAddrs" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestKeepUniqueAddrs/nil_slice" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestKeepUniqueAddrs/empty_slice" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestKeepUniqueAddrs/one_element_slice" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestKeepUniqueAddrs/IPv4_all_duplicates" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestKeepUniqueAddrs/IPv4_all_unique" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestKeepUniqueAddrs/IPv4_mixed" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestKeepUniqueAddrs/IPv6_all_duplicates" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestKeepUniqueAddrs/Mixed_IPv4_&amp;_IPv6" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<testcase name="TestKeepUniqueAddrs/With_IPv6-in-IPv6" classname="github.com/cilium/cilium/pkg/ip" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ip	coverage: 80.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam" tests="54" failures="0" errors="0" id="417" hostname="kind-bpf-next" time="13.449" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test_cleanupUnreachableRoutes" classname="github.com/cilium/cilium/pkg/ipam" time="0.010"></testcase>
		<testcase name="TestPodCIDRPool" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="TestPodCIDRPool/ipv4" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=warning msg="Removed last pod CIDR allocator" subsys=ipam]]></system-out>
		</testcase>
		<testcase name="TestPodCIDRPool/ipv6" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=warning msg="Removed last pod CIDR allocator" subsys=ipam]]></system-out>
		</testcase>
		<testcase name="TestPodCIDRPoolTwoPools" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="TestPodCIDRPoolTwoPools/ipv4" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="TestPodCIDRPoolTwoPools/ipv6" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="TestPodCIDRPoolRemoveInUse" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="TestPodCIDRPoolRemoveInUse/remove_first_unused" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=warning msg="Removed last pod CIDR allocator" subsys=ipam]]></system-out>
		</testcase>
		<testcase name="TestPodCIDRPoolRemoveInUse/remove_first_in_use" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=error msg="in-use pod CIDR was removed from spec" cidr=192.168.0.0/27 subsys=ipam]]></system-out>
		</testcase>
		<testcase name="TestPodCIDRPoolRemoveInUse/remove_second_unused" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="TestPodCIDRPoolRemoveInUse/remove_second_in_use" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=error msg="in-use pod CIDR was removed from spec" cidr=192.168.1.0/27 subsys=ipam]]></system-out>
		</testcase>
		<testcase name="TestPodCIDRPoolRemoveInUseWithRelease" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=error msg="in-use pod CIDR was removed from spec" cidr=192.168.1.0/27 subsys=ipam]]></system-out>
		</testcase>
		<testcase name="TestPodCIDRPoolDuplicatePodCIDRs" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=error msg="ignoring duplicate pod CIDR" cidr=192.168.0.0/27 subsys=ipam]]></system-out>
		</testcase>
		<testcase name="TestPodCIDRPoolSmallAlloc" classname="github.com/cilium/cilium/pkg/ipam" time="0.010"></testcase>
		<testcase name="TestPodCIDRPoolTooSmallAlloc" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=error msg="skipping too-small pod CIDR" cidr=192.168.0.0/32 subsys=ipam
level=error msg="skipping too-small pod CIDR" cidr=192.168.0.0/31 subsys=ipam]]></system-out>
		</testcase>
		<testcase name="TestNewCRDWatcher" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="TestNewCRDWatcher/ipv4" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="TestNewCRDWatcher/ipv6" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="TestNewCRDWatcher_restoreFinished" classname="github.com/cilium/cilium/pkg/ipam" time="0.010"></testcase>
		<testcase name="TestIPNotAvailableInPoolError" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/ipam" time="13.370"></testcase>
		<testcase name="Test/IPAMSuite" classname="github.com/cilium/cilium/pkg/ipam" time="13.370"></testcase>
		<testcase name="Test/IPAMSuite/TestAllocateNextWithExpiration" classname="github.com/cilium/cilium/pkg/ipam" time="2.000">
			<system-out><![CDATA[level=info msg="Initializing cluster-pool IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=warning msg="Released IP after expiration" ip=1.1.1.131 subsys=ipam uuid=45473b01-3bd9-4c99-8e3f-81ae3597e7e4
level=warning msg="Released IP after expiration" ip="cafe::ef01" subsys=ipam uuid=72a92cdb-7140-456f-af3c-3746b1093bbd
level=warning msg="Released IP after expiration" ip="cafe::ee08" subsys=ipam uuid=d12ca49a-0b4f-4bd0-bf64-b8696a8525e7]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestAllocatedIPDump" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=info msg="Initializing cluster-pool IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestCalculateExcessIPs" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test/IPAMSuite/TestCalculateNeededIPs" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test/IPAMSuite/TestDeriveFamily" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test/IPAMSuite/TestExcludeIP" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=info msg="Initializing cluster-pool IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestExpirationTimer" classname="github.com/cilium/cilium/pkg/ipam" time="0.200">
			<system-out><![CDATA[level=info msg="Initializing cluster-pool IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=warning msg="Released IP after expiration" ip=1.1.1.1 subsys=ipam uuid=96cfebfc-11f7-4b01-9cf6-cd4fd0879f20]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestGetNodeNames" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node2 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node1 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2048 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=8 name=node2 neededIPs=8 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2048 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=8 name=node1 neededIPs=8 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestLock" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=info msg="Initializing cluster-pool IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestMarkForReleaseNoAllocate" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=info msg="Initializing CRD-based IPAM" subsys=ipam]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestNodeManagerAbortRelease" classname="github.com/cilium/cilium/pkg/ipam" time="5.020">
			<system-out><![CDATA[level=info msg="Discovered new CiliumNode custom resource" name=node3 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node3 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2048 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=3 name=node3 neededIPs=3 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node3 subsys=ipam
level=info msg="Resolving IP deficit of node" available=3 availableForAllocation=2045 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=1 name=node3 neededIPs=1 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=3]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestNodeManagerDefaultAllocation" classname="github.com/cilium/cilium/pkg/ipam" time="0.020">
			<system-out><![CDATA[level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2048 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=8 name=node1 neededIPs=8 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=8 availableForAllocation=2040 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=7 name=node1 neededIPs=7 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=7]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestNodeManagerGet" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestNodeManagerManyNodes" classname="github.com/cilium/cilium/pkg/ipam" time="0.050">
			<system-out><![CDATA[level=info msg="Discovered new CiliumNode custom resource" name=node0 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node1 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node2 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node3 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node4 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node5 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node6 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node7 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node8 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node9 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node10 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node11 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node12 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node13 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node14 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node15 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node16 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node17 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node18 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node19 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node20 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node21 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node22 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node23 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node24 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node25 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node26 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node27 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node28 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node29 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node30 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node31 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node32 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node33 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node34 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node35 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node36 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node37 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node38 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node39 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node40 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node41 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node42 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node43 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node44 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node45 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node46 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node47 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node48 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node49 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node50 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node51 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node52 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node53 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node54 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node55 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node56 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node57 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node58 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node59 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node60 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node61 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node62 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node63 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node64 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node65 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node66 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node67 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node68 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node69 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node70 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node71 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node72 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node73 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node74 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node75 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node76 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node77 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node78 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node79 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node80 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node81 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node82 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node83 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node84 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node85 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node86 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node87 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node88 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node89 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node90 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node91 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node92 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node93 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node94 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node95 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node96 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node97 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node98 subsys=ipam
level=info msg="Discovered new CiliumNode custom resource" name=node99 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node41 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2048 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node41 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node42 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2038 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node42 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node43 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2028 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node43 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node44 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2018 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node44 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node45 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2008 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node45 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node46 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node46 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2048 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=8 name=node1 neededIPs=8 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node18 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node18 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node19 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node19 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node20 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node20 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node21 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node21 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node22 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node22 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node23 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node71 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node71 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node23 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node24 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node24 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node25 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node25 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node26 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node26 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node27 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node27 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node28 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node28 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node29 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node29 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node30 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node30 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node31 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node31 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node32 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node32 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node33 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node33 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node34 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node34 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node35 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node35 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node36 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node36 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node37 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node37 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node38 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node38 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node39 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node39 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node40 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node40 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node47 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node47 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node48 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node48 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node49 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node49 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node50 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node50 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node51 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node51 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node52 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node52 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node53 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node53 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node54 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node54 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node55 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node55 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node56 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node56 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node57 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node57 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node58 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node58 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node59 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node59 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node60 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node60 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node61 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node61 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node62 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node62 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node63 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node63 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node64 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node64 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node65 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node65 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node66 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node66 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node67 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node67 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node68 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node68 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node69 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node69 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node70 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node70 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node82 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node82 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node72 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node72 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node0 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node0 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node1 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node1 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node90 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node9 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node9 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node92 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node92 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node8 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node8 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node94 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node94 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node17 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node17 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node96 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node96 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node14 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node14 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node11 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node11 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node90 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node84 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node85 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node12 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node85 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node98 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node98 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node15 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node15 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node84 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node12 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node93 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node93 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node76 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node76 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node78 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node78 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node87 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node87 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node80 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node80 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node88 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node88 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node7 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node7 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node91 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node91 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node95 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node95 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node16 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node16 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node13 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node13 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node2 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node3 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node3 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node97 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node97 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node89 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node89 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node77 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node77 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node5 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node5 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node6 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node6 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node10 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node10 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node86 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node86 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node83 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node83 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node79 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node79 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node81 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node81 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node74 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node75 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node4 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node73 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node99 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node99 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node74 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node75 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node4 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=1998 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node73 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestNodeManagerMinAllocate20" classname="github.com/cilium/cilium/pkg/ipam" time="0.020">
			<system-out><![CDATA[level=info msg="Discovered new CiliumNode custom resource" name=node2 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2048 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node2 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=10 availableForAllocation=2038 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node2 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=8]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestNodeManagerMinAllocateAndPreallocate" classname="github.com/cilium/cilium/pkg/ipam" time="0.020">
			<system-out><![CDATA[level=info msg="Discovered new CiliumNode custom resource" name=node2 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2048 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=10 name=node2 neededIPs=10 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node2 subsys=ipam
level=info msg="Resolving IP deficit of node" available=10 availableForAllocation=2038 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=1 name=node2 neededIPs=1 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=10]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestNodeManagerReleaseAddress" classname="github.com/cilium/cilium/pkg/ipam" time="6.030">
			<system-out><![CDATA[level=info msg="Discovered new CiliumNode custom resource" name=node3 subsys=ipam
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node3 subsys=ipam
level=info msg="Resolving IP deficit of node" available=0 availableForAllocation=2048 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=19 name=node3 neededIPs=15 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=0
level=warning msg="Unable to compute pending pods, will not surge-allocate" error="pod store uninitialized" instanceID= name=node3 subsys=ipam
level=info msg="Resolving IP deficit of node" available=19 availableForAllocation=2029 emptyInterfaceSlots=0 instanceID= maxIPsToAllocate=8 name=node3 neededIPs=4 remainingInterfaces=0 selectedInterface= selectedPoolID=global subsys=ipam used=19
level=info msg="Releasing excess IPs from node" available=27 excess=9 excessIps="[16 15 11 19 17 14 13 12 18]" instanceID= name=node3 releasing="[16 15 11 19 17 14 13 12 18]" selectedInterface= selectedPoolID=global subsys=ipam used=10]]></system-out>
		</testcase>
		<testcase name="Test/IPAMSuite/TestOwnerRelease" classname="github.com/cilium/cilium/pkg/ipam" time="0.000">
			<system-out><![CDATA[level=info msg="Initializing cluster-pool IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"]]></system-out>
		</testcase>
		<testcase name="Test_MultiPoolManager" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test_neededIPCeil" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test_neededIPCeil/numIP=0_preAlloc=0" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test_neededIPCeil/numIP=1_preAlloc=0" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test_neededIPCeil/numIP=3_preAlloc=0" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test_neededIPCeil/numIP=0_preAlloc=1" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test_neededIPCeil/numIP=1_preAlloc=1" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test_neededIPCeil/numIP=3_preAlloc=1" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test_neededIPCeil/numIP=0_preAlloc=16" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test_neededIPCeil/numIP=1_preAlloc=16" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test_neededIPCeil/numIP=15_preAlloc=16" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test_neededIPCeil/numIP=16_preAlloc=16" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<testcase name="Test_neededIPCeil/numIP=17_preAlloc=16" classname="github.com/cilium/cilium/pkg/ipam" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ipam	coverage: 62.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/allocator" tests="12" failures="0" errors="0" id="418" hostname="kind-bpf-next" time="0.055" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/ipam/allocator" time="0.000"></testcase>
		<testcase name="Test/AllocatorSuite" classname="github.com/cilium/cilium/pkg/ipam/allocator" time="0.000"></testcase>
		<testcase name="Test/AllocatorSuite/TestNoOpAllocator" classname="github.com/cilium/cilium/pkg/ipam/allocator" time="0.000"></testcase>
		<testcase name="Test/AllocatorSuite/TestPoolAllocator" classname="github.com/cilium/cilium/pkg/ipam/allocator" time="0.000"></testcase>
		<testcase name="Test/AllocatorSuite/TestPoolAllocatorLimit" classname="github.com/cilium/cilium/pkg/ipam/allocator" time="0.000"></testcase>
		<testcase name="Test/AllocatorSuite/TestPoolAllocatorRelease" classname="github.com/cilium/cilium/pkg/ipam/allocator" time="0.000"></testcase>
		<testcase name="Test/AllocatorSuite/TestPoolGroupAllocator" classname="github.com/cilium/cilium/pkg/ipam/allocator" time="0.000"></testcase>
		<testcase name="Test/AllocatorSuite/TestPoolGroupAllocatorAllocate" classname="github.com/cilium/cilium/pkg/ipam/allocator" time="0.000"></testcase>
		<testcase name="Test/AllocatorSuite/TestPoolGroupAllocatorAllocateWithPoolSearch" classname="github.com/cilium/cilium/pkg/ipam/allocator" time="0.000">
			<system-out><![CDATA[level=warning msg="Unable to allocate IP in internal allocator" error="provided IP is not in the valid range. The range of valid IPs is 10.10.0.0/24" instance=i-1 interface=1 ip=1.1.1.1 subsys=ipam-allocator]]></system-out>
		</testcase>
		<testcase name="Test/AllocatorSuite/TestPoolGroupAllocatorLimit" classname="github.com/cilium/cilium/pkg/ipam/allocator" time="0.000"></testcase>
		<testcase name="Test/AllocatorSuite/TestPoolGroupAllocatorReserve" classname="github.com/cilium/cilium/pkg/ipam/allocator" time="0.000">
			<system-out><![CDATA[level=warning msg="Unable to allocate IP in internal allocator" error="provided IP is not in the valid range. The range of valid IPs is 10.10.0.0/24" instance=i-1 interface=1 ip=1.1.1.1 subsys=ipam-allocator]]></system-out>
		</testcase>
		<testcase name="Test/AllocatorSuite/TestPoolID" classname="github.com/cilium/cilium/pkg/ipam/allocator" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ipam/allocator	coverage: 91.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/allocator/clusterpool/cidralloc" tests="7" failures="0" errors="0" id="419" hostname="kind-bpf-next" time="0.004" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestNewCIDRSets" classname="github.com/cilium/cilium/pkg/ipam/allocator/clusterpool/cidralloc" time="0.000"></testcase>
		<testcase name="TestNewCIDRSets/test-1" classname="github.com/cilium/cilium/pkg/ipam/allocator/clusterpool/cidralloc" time="0.000"></testcase>
		<testcase name="TestNewCIDRSets/test-2_-_CIDRs_collide" classname="github.com/cilium/cilium/pkg/ipam/allocator/clusterpool/cidralloc" time="0.000"></testcase>
		<testcase name="TestNewCIDRSets/test-2_-_CIDRs_collide#01" classname="github.com/cilium/cilium/pkg/ipam/allocator/clusterpool/cidralloc" time="0.000"></testcase>
		<testcase name="TestNewCIDRSets/test-4_-_CIDRs_collide" classname="github.com/cilium/cilium/pkg/ipam/allocator/clusterpool/cidralloc" time="0.000"></testcase>
		<testcase name="TestNewCIDRSets/test-5_-_CIDRs_do_not_collide" classname="github.com/cilium/cilium/pkg/ipam/allocator/clusterpool/cidralloc" time="0.000"></testcase>
		<testcase name="TestNewCIDRSets/test-6_-_CIDR_does_not_collide" classname="github.com/cilium/cilium/pkg/ipam/allocator/clusterpool/cidralloc" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ipam/allocator/clusterpool/cidralloc	coverage: 71.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/allocator/multipool" tests="9" failures="0" errors="0" id="420" hostname="kind-bpf-next" time="0.029" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestPoolAllocator" classname="github.com/cilium/cilium/pkg/ipam/allocator/multipool" time="0.000"></testcase>
		<testcase name="TestPoolAllocator_PoolErrors" classname="github.com/cilium/cilium/pkg/ipam/allocator/multipool" time="0.000"></testcase>
		<testcase name="Test_addrsInPrefix" classname="github.com/cilium/cilium/pkg/ipam/allocator/multipool" time="0.000"></testcase>
		<testcase name="Test_addrsInPrefix/ipv4" classname="github.com/cilium/cilium/pkg/ipam/allocator/multipool" time="0.000"></testcase>
		<testcase name="Test_addrsInPrefix/ipv6" classname="github.com/cilium/cilium/pkg/ipam/allocator/multipool" time="0.000"></testcase>
		<testcase name="Test_addrsInPrefix/zero" classname="github.com/cilium/cilium/pkg/ipam/allocator/multipool" time="0.000"></testcase>
		<testcase name="Test_addrsInPrefix/two" classname="github.com/cilium/cilium/pkg/ipam/allocator/multipool" time="0.000"></testcase>
		<testcase name="Test_addrsInPrefix/underflow_/31" classname="github.com/cilium/cilium/pkg/ipam/allocator/multipool" time="0.000"></testcase>
		<testcase name="Test_addrsInPrefix/underflow_/32" classname="github.com/cilium/cilium/pkg/ipam/allocator/multipool" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ipam/allocator/multipool	coverage: 59.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" tests="13" failures="0" errors="0" id="421" hostname="kind-bpf-next" time="4.078" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="4.030"></testcase>
		<testcase name="Test/PodCIDRSuite" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="4.030"></testcase>
		<testcase name="Test/PodCIDRSuite/TestNewNodesPodCIDRManager" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="4.000"></testcase>
		<testcase name="Test/PodCIDRSuite/TestNodesPodCIDRManager_Create" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="0.020"></testcase>
		<testcase name="Test/PodCIDRSuite/TestNodesPodCIDRManager_Delete" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="0.000">
			<system-out><![CDATA[level=info msg="node released cidrs" cidr=10.10.0.0/24 node-name=node-1 subsys=pod-cidr
level=info msg="node released cidrs" cidr=10.10.0.0/24 node-name=node-1 subsys=pod-cidr]]></system-out>
		</testcase>
		<testcase name="Test/PodCIDRSuite/TestNodesPodCIDRManager_Resync" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="0.000"></testcase>
		<testcase name="Test/PodCIDRSuite/TestNodesPodCIDRManager_Update" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="0.000"></testcase>
		<testcase name="Test/PodCIDRSuite/TestNodesPodCIDRManager_allocateIPNets" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="0.000"></testcase>
		<testcase name="Test/PodCIDRSuite/TestNodesPodCIDRManager_allocateNext" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="0.000"></testcase>
		<testcase name="Test/PodCIDRSuite/TestNodesPodCIDRManager_allocateNodeV2" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="0.000">
			<system-out><![CDATA[level=info msg="node released cidrs" cidr=10.10.1.0/24 node-name=node-1 subsys=pod-cidr]]></system-out>
		</testcase>
		<testcase name="Test/PodCIDRSuite/TestNodesPodCIDRManager_releaseIPNets" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="0.000">
			<system-out><![CDATA[level=info msg="node released cidrs" cidr=10.0.0.0/16 node-name=node-1 subsys=pod-cidr
level=info msg="node released cidrs" cidr="fd00::/80" node-name=node-1 subsys=pod-cidr]]></system-out>
		</testcase>
		<testcase name="Test/PodCIDRSuite/Test_parsePodCIDRs" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="0.000"></testcase>
		<testcase name="Test/PodCIDRSuite/Test_syncToK8s" classname="github.com/cilium/cilium/pkg/ipam/allocator/podcidr" time="0.000">
			<system-out><![CDATA[level=warning msg="Received a CiliumNode delete event, but the resource may not have been deleted (see error)." cidr="fd00::/80" error="Timeout: " node-name=node-1 subsys=pod-cidr]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ipam/allocator/podcidr	coverage: 86.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/cidrset" tests="15" failures="0" errors="0" id="422" hostname="kind-bpf-next" time="0.010" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestCIDRSetFullyAllocated" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestIndexToCIDRBlock" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestCIDRSet_RandomishAllocation" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestCIDRSet_AllocationOccupied" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestGetBitforCIDR" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestOccupy" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestCIDRSetv6" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestCIDRSetv6/Max_cluster_subnet_size_with_IPv4" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestCIDRSetv6/Max_cluster_subnet_size_with_IPv6" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestCIDRSetv6/Allocate_a_few_IPv6" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestInvalidSubNetMaskSize" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestInvalidSubNetMaskSize/Check_valid_subnet_mask_size_with_IPv4" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestInvalidSubNetMaskSize/Check_valid_subnet_mask_size_with_IPv6" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestInvalidSubNetMaskSize/Check_invalid_subnet_mask_size_with_IPv4" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<testcase name="TestInvalidSubNetMaskSize/Check_invalid_subnet_mask_size_with_IPv6" classname="github.com/cilium/cilium/pkg/ipam/cidrset" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ipam/cidrset	coverage: 83.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/metrics/mock" tests="3" failures="0" errors="0" id="423" hostname="kind-bpf-next" time="0.005" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/ipam/metrics/mock" time="0.000"></testcase>
		<testcase name="Test/MockSuite" classname="github.com/cilium/cilium/pkg/ipam/metrics/mock" time="0.000"></testcase>
		<testcase name="Test/MockSuite/TestMock" classname="github.com/cilium/cilium/pkg/ipam/metrics/mock" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ipam/metrics/mock	coverage: 65.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/service/allocator" tests="1" failures="0" errors="0" id="424" hostname="kind-bpf-next" time="0.003" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestCountBits" classname="github.com/cilium/cilium/pkg/ipam/service/allocator" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ipam/service/allocator	coverage: 5.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipam/types" tests="7" failures="0" errors="0" id="425" hostname="kind-bpf-next" time="0.006" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/ipam/types" time="0.000"></testcase>
		<testcase name="Test/TypesSuite" classname="github.com/cilium/cilium/pkg/ipam/types" time="0.000"></testcase>
		<testcase name="Test/TypesSuite/TestFirstSubnetWithAvailableAddresses" classname="github.com/cilium/cilium/pkg/ipam/types" time="0.000"></testcase>
		<testcase name="Test/TypesSuite/TestForeachAddresses" classname="github.com/cilium/cilium/pkg/ipam/types" time="0.000"></testcase>
		<testcase name="Test/TypesSuite/TestGetInterface" classname="github.com/cilium/cilium/pkg/ipam/types" time="0.000"></testcase>
		<testcase name="Test/TypesSuite/TestInstanceMapNumInstances" classname="github.com/cilium/cilium/pkg/ipam/types" time="0.000"></testcase>
		<testcase name="Test/TypesSuite/TestTagsMatch" classname="github.com/cilium/cilium/pkg/ipam/types" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ipam/types	coverage: 10.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipcache" tests="14" failures="0" errors="0" id="426" hostname="kind-bpf-next" time="0.031" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestDeferRelease" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000"></testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000"></testcase>
		<testcase name="Test/IPCacheTestSuite" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000"></testcase>
		<testcase name="Test/IPCacheTestSuite/TestIPCache" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000">
			<system-out><![CDATA[level=warning msg="Attempt to remove non-existing IP from ipcache layer" ipAddr=10.0.0.15 subsys=ipcache
level=warning msg="Attempt to remove non-existing IP from ipcache layer" ipAddr=27.2.2.2 subsys=ipcache
level=warning msg="Attempt to remove non-existing IP from ipcache layer" ipAddr=127.0.0.1 subsys=ipcache
level=warning msg="Attempt to remove non-existing IP from ipcache layer" ipAddr=127.0.0.1 subsys=ipcache
level=warning msg="Attempt to remove non-existing IP from ipcache layer" ipAddr=10.1.1.250 subsys=ipcache]]></system-out>
		</testcase>
		<testcase name="Test/IPCacheTestSuite/TestIPCacheNamedPorts" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000">
			<system-out><![CDATA[level=warning msg="Attempt to remove non-existing IP from ipcache layer" ipAddr=10.0.0.15 subsys=ipcache
level=info msg="Named ports after Delete 0: &{{{{{0 0} 0 0 {{} 0} {{} 0}}}} map[dns:map[{53 0}:1] http2:map[{8080 6}:1] https:map[{443 6}:1]]}" subsys=ipcache
level=info msg="Named ports after Delete 1: &{{{{{0 0} 0 0 {{} 0} {{} 0}}}} map[dns:map[{53 0}:1] https:map[{443 6}:1]]}" subsys=ipcache
level=warning msg="Attempt to remove non-existing IP from ipcache layer" ipAddr=27.2.2.2 subsys=ipcache
level=info msg="Named ports after Delete 2: &{{{{{0 0} 0 0 {{} 0} {{} 0}}}} map[dns:map[{53 0}:1] https:map[{443 6}:1]]}" subsys=ipcache
level=warning msg="Attempt to remove non-existing IP from ipcache layer" ipAddr=127.0.0.1 subsys=ipcache
level=info msg="Named ports after Delete 3: &{{{{{0 0} 0 0 {{} 0} {{} 0}}}} map[dns:map[{53 0}:1] https:map[{443 6}:1]]}" subsys=ipcache
level=warning msg="Attempt to remove non-existing IP from ipcache layer" ipAddr=127.0.0.1 subsys=ipcache
level=info msg="Named ports after Delete 4: &{{{{{0 0} 0 0 {{} 0} {{} 0}}}} map[dns:map[{53 0}:1] https:map[{443 6}:1]]}" subsys=ipcache]]></system-out>
		</testcase>
		<testcase name="Test/IPCacheTestSuite/TestIPCacheShadowing" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000"></testcase>
		<testcase name="Test/IPCacheTestSuite/TestKeyToIPNet" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000"></testcase>
		<testcase name="TestInjectLabels" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000"></testcase>
		<testcase name="TestInjectExisting" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000"></testcase>
		<testcase name="TestFilterMetadataByLabels" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000"></testcase>
		<testcase name="TestRemoveLabelsFromIPs" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000"></testcase>
		<testcase name="TestOverrideIdentity" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000"></testcase>
		<testcase name="TestUpsertMetadataTunnelPeerAndEncryptKey" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000">
			<system-out><![CDATA[level=warning msg="Detected conflicting tunnel peer for prefix. This may cause connectivity issues for this address." cidr=10.0.0.4/32 conflictingResource=generated-uid conflictingTunnelPeer=192.168.1.101 resource=node-uid subsys=ipcache tunnelPeer=192.168.1.100
level=warning msg="Detected conflicting encryption key index for prefix. This may cause connectivity issues for this address." cidr=10.0.0.4/32 conflictingKey=6 conflictingResource=generated-uid key=7 resource=node-uid subsys=ipcache
level=warning msg="Detected conflicting tunnel peer for prefix. This may cause connectivity issues for this address." cidr=10.0.0.4/32 conflictingResource=generated-uid conflictingTunnelPeer=192.168.1.101 resource=node-uid subsys=ipcache tunnelPeer=192.168.1.100
level=warning msg="Detected conflicting tunnel peer for prefix. This may cause connectivity issues for this address." cidr=10.0.0.4/32 conflictingResource=generated-uid conflictingTunnelPeer=192.168.1.101 resource=node-uid subsys=ipcache tunnelPeer=192.168.1.101]]></system-out>
		</testcase>
		<testcase name="Test_sortedByResourceIDsAndSource" classname="github.com/cilium/cilium/pkg/ipcache" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ipcache	coverage: 61.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipcache/fake" tests="3" failures="0" errors="0" id="427" hostname="kind-bpf-next" time="0.031" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/ipcache/fake" time="0.000"></testcase>
		<testcase name="Test/fakeIPCacheSuite" classname="github.com/cilium/cilium/pkg/ipcache/fake" time="0.000"></testcase>
		<testcase name="Test/fakeIPCacheSuite/TestFakeIPCache" classname="github.com/cilium/cilium/pkg/ipcache/fake" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ipcache/fake	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/ipmasq" tests="4" failures="0" errors="0" id="428" hostname="kind-bpf-next" time="1.826" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/ipmasq" time="1.810"></testcase>
		<testcase name="Test/IPMasqTestSuite" classname="github.com/cilium/cilium/pkg/ipmasq" time="1.810"></testcase>
		<testcase name="Test/IPMasqTestSuite/TestRestore" classname="github.com/cilium/cilium/pkg/ipmasq" time="0.310">
			<system-out><![CDATA[level=info msg="Adding CIDR" cidr=169.254.0.0/16 subsys=ipmasq
level=info msg="Adding CIDR" cidr=203.0.113.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=198.18.0.0/15 subsys=ipmasq
level=info msg="Adding CIDR" cidr=10.0.0.0/8 subsys=ipmasq
level=info msg="Adding CIDR" cidr=172.16.0.0/12 subsys=ipmasq
level=info msg="Adding CIDR" cidr=100.64.0.0/10 subsys=ipmasq
level=info msg="Adding CIDR" cidr=192.0.0.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=192.88.99.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=192.0.2.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=240.0.0.0/4 subsys=ipmasq
level=info msg="Adding CIDR" cidr=198.51.100.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=192.168.0.0/16 subsys=ipmasq
level=info msg="Stopping ip-masq-agent" subsys=ipmasq
level=info msg="Removing CIDR" cidr=192.88.99.0/24 subsys=ipmasq
level=info msg="Removing CIDR" cidr=192.0.2.0/24 subsys=ipmasq
level=info msg="Removing CIDR" cidr=192.168.0.0/16 subsys=ipmasq
level=info msg="Removing CIDR" cidr=10.0.0.0/8 subsys=ipmasq
level=info msg="Removing CIDR" cidr=172.16.0.0/12 subsys=ipmasq
level=info msg="Removing CIDR" cidr=3.3.3.0/24 subsys=ipmasq
level=info msg="Removing CIDR" cidr=203.0.113.0/24 subsys=ipmasq
level=info msg="Removing CIDR" cidr=198.18.0.0/15 subsys=ipmasq
level=info msg="Removing CIDR" cidr=198.51.100.0/24 subsys=ipmasq
level=info msg="Removing CIDR" cidr=240.0.0.0/4 subsys=ipmasq
level=info msg="Removing CIDR" cidr=100.64.0.0/10 subsys=ipmasq
level=info msg="Removing CIDR" cidr=192.0.0.0/24 subsys=ipmasq
level=info msg="Stopping ip-masq-agent" subsys=ipmasq
level=info msg="Adding CIDR" cidr=3.3.0.0/16 subsys=ipmasq
level=info msg="Stopping ip-masq-agent" subsys=ipmasq]]></system-out>
		</testcase>
		<testcase name="Test/IPMasqTestSuite/TestUpdate" classname="github.com/cilium/cilium/pkg/ipmasq" time="1.500">
			<system-out><![CDATA[level=info msg="Adding CIDR" cidr=100.64.0.0/10 subsys=ipmasq
level=info msg="Adding CIDR" cidr=192.88.99.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=198.51.100.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=172.16.0.0/12 subsys=ipmasq
level=info msg="Adding CIDR" cidr=240.0.0.0/4 subsys=ipmasq
level=info msg="Adding CIDR" cidr=192.0.2.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=192.168.0.0/16 subsys=ipmasq
level=info msg="Adding CIDR" cidr=198.18.0.0/15 subsys=ipmasq
level=info msg="Adding CIDR" cidr=10.0.0.0/8 subsys=ipmasq
level=info msg="Adding CIDR" cidr=203.0.113.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=169.254.0.0/16 subsys=ipmasq
level=info msg="Adding CIDR" cidr=192.0.0.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=1.1.1.1/32 subsys=ipmasq
level=info msg="Adding CIDR" cidr=2.2.0.0/16 subsys=ipmasq
level=info msg="Removing CIDR" cidr=192.0.0.0/24 subsys=ipmasq
level=info msg="Removing CIDR" cidr=192.88.99.0/24 subsys=ipmasq
level=info msg="Removing CIDR" cidr=172.16.0.0/12 subsys=ipmasq
level=info msg="Removing CIDR" cidr=192.168.0.0/16 subsys=ipmasq
level=info msg="Removing CIDR" cidr=10.0.0.0/8 subsys=ipmasq
level=info msg="Removing CIDR" cidr=203.0.113.0/24 subsys=ipmasq
level=info msg="Removing CIDR" cidr=240.0.0.0/4 subsys=ipmasq
level=info msg="Removing CIDR" cidr=100.64.0.0/10 subsys=ipmasq
level=info msg="Removing CIDR" cidr=192.0.2.0/24 subsys=ipmasq
level=info msg="Removing CIDR" cidr=198.18.0.0/15 subsys=ipmasq
level=info msg="Removing CIDR" cidr=198.51.100.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=8.8.0.0/16 subsys=ipmasq
level=info msg="Removing CIDR" cidr=1.1.1.1/32 subsys=ipmasq
level=info msg="Removing CIDR" cidr=8.8.0.0/16 subsys=ipmasq
level=info msg="Removing CIDR" cidr=2.2.0.0/16 subsys=ipmasq
level=info msg="Adding CIDR" cidr=1.1.0.0/16 subsys=ipmasq
level=info msg="Adding CIDR" cidr=8.8.0.0/16 subsys=ipmasq
level=info msg="Removing CIDR" cidr=169.254.0.0/16 subsys=ipmasq
level=info msg="Config file not found" file-path=/tmp/ipmasq-test2565432682 subsys=ipmasq
level=info msg="Adding CIDR" cidr=240.0.0.0/4 subsys=ipmasq
level=info msg="Adding CIDR" cidr=198.51.100.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=10.0.0.0/8 subsys=ipmasq
level=info msg="Adding CIDR" cidr=172.16.0.0/12 subsys=ipmasq
level=info msg="Adding CIDR" cidr=100.64.0.0/10 subsys=ipmasq
level=info msg="Adding CIDR" cidr=192.0.0.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=169.254.0.0/16 subsys=ipmasq
level=info msg="Adding CIDR" cidr=203.0.113.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=198.18.0.0/15 subsys=ipmasq
level=info msg="Adding CIDR" cidr=192.168.0.0/16 subsys=ipmasq
level=info msg="Adding CIDR" cidr=192.88.99.0/24 subsys=ipmasq
level=info msg="Adding CIDR" cidr=192.0.2.0/24 subsys=ipmasq
level=info msg="Removing CIDR" cidr=1.1.0.0/16 subsys=ipmasq
level=info msg="Removing CIDR" cidr=8.8.0.0/16 subsys=ipmasq
level=info msg="Stopping ip-masq-agent" subsys=ipmasq]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/ipmasq	coverage: 81.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s" tests="146" failures="0" errors="0" id="429" hostname="kind-bpf-next" time="2.575" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestEndpoints_DeepEqual" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestEndpoints_DeepEqual/both_equal" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestEndpoints_DeepEqual/different_BE_IPs" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestEndpoints_DeepEqual/ports_different_name" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestEndpoints_DeepEqual/ports_different_content" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestEndpoints_DeepEqual/ports_different_one_is_bigger" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestEndpoints_DeepEqual/backend_different_one_is_nil" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestEndpoints_DeepEqual/node_name_different" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestEndpoints_DeepEqual/both_nil" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseEndpointPortV1Beta1" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseEndpointPortV1Beta1/tcp-port" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseEndpointPortV1Beta1/udp-port" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseEndpointPortV1Beta1/sctp-port" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseEndpointPortV1Beta1/unset-protocol-should-have-tcp-port" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseEndpointPortV1Beta1/unset-port-number-should-fail" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseEndpointPortV1" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseEndpointPortV1/tcp-port" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseEndpointPortV1/udp-port" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseEndpointPortV1/sctp-port" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseEndpointPortV1/unset-protocol-should-have-tcp-port" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseEndpointPortV1/unset-port-number-should-fail" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestGetPodMetadata" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestGetPodMetadata/normal_scenario" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestGetPodMetadata/pod_labels_contains_cilium_owned_label" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestGetPodMetadata/pod_labels_contains_cilium_owned_label/override_namespace_labels" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestGetPodMetadata/pod_labels_contains_cilium_owned_label/add_one_more_namespace_labels" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestGetPodMetadata/istio_sidecar_label" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestGetPodMetadata/istio_sidecar_label/with_istio_sidecar_label" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestGetPodMetadata/istio_sidecar_label/with_istio_sidecar_injection" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_filterPodLabels" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_filterPodLabels/normal_scenario" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_filterPodLabels/having_cilium_owned_namespace_labels" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_filterPodLabels/having_cilium_owned_policy_labels" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/k8s" time="2.520"></testcase>
		<testcase name="Test/K8sSuite" classname="github.com/cilium/cilium/pkg/k8s" time="2.520"></testcase>
		<testcase name="Test/K8sSuite/TestCIDRPolicyExamples" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestCacheActionString" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestClusterServiceMerging" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestDontDeleteUserRules" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestEndpointsString" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestExternalServiceDeletion" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestExternalServiceMerging" classname="github.com/cilium/cilium/pkg/k8s" time="0.200"></testcase>
		<testcase name="Test/K8sSuite/TestGenerateToCIDRFromEndpoint" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestGetAnnotationIncludeExternal" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestGetAnnotationServiceAffinity" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestGetAnnotationShared" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestGetPolicyLabelsv1" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestGetUniqueServiceFrontends" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestIPBlockToCIDRRule" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestIsK8ServiceExternal" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestK8sErrorLogTimeout" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestNetworkPolicyExamples" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestNewClusterService" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestNonSharedService" classname="github.com/cilium/cilium/pkg/k8s" time="0.100"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyDenyAll" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyEgress" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyEgressAllowAll" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyEgressL4AllowAll" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyEmptyFrom" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyEmptyPort" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyIngress" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyIngressAllowAll" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyIngressL4AllowAll" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyMultipleSelectors" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyNamedPort" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyNoIngress" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyNoSelectors" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNetworkPolicyUnknownProto" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseNode" classname="github.com/cilium/cilium/pkg/k8s" time="0.000">
			<system-out><![CDATA[level=warning msg="Detected multiple IPs of the same address type and family, Cilium will only consider the first IP in the Node resource" k8sNodeID= node=node2 nodeName=node2 subsys=k8s type=InternalIP
level=warning msg="Detected multiple IPs of the same address type and family, Cilium will only consider the first IP in the Node resource" k8sNodeID= node=node2 nodeName=node2 subsys=k8s type=InternalIP
level=warning msg="Detected multiple IPs of the same address type and family, Cilium will only consider the first IP in the Node resource" k8sNodeID= node=node2 nodeName=node2 subsys=k8s type=ExternalIP
level=warning msg="Detected multiple IPs of the same address type and family, Cilium will only consider the first IP in the Node resource" k8sNodeID= node=node2 nodeName=node2 subsys=k8s type=ExternalIP]]></system-out>
		</testcase>
		<testcase name="Test/K8sSuite/TestParseNodeWithoutAnnotations" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParsePorts" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseService" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestParseServiceID" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestPrepareRemoveNodeAnnotationsPayload" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestPreprocessRules" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestRemovalOfNodeAnnotations" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestServiceCacheEndpointSlice" classname="github.com/cilium/cilium/pkg/k8s" time="0.300"></testcase>
		<testcase name="Test/K8sSuite/TestServiceCacheEndpoints" classname="github.com/cilium/cilium/pkg/k8s" time="0.300"></testcase>
		<testcase name="Test/K8sSuite/TestServiceCacheWith2EndpointSlice" classname="github.com/cilium/cilium/pkg/k8s" time="0.300"></testcase>
		<testcase name="Test/K8sSuite/TestServiceCacheWith2EndpointSliceSameAddress" classname="github.com/cilium/cilium/pkg/k8s" time="0.300"></testcase>
		<testcase name="Test/K8sSuite/TestServiceEndpointFiltering" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestServiceMatches" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestServiceMutators" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestServiceString" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestServiceUniquePorts" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestTranslatorDirect" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestTranslatorLabels" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/TestUseNodeCIDR" classname="github.com/cilium/cilium/pkg/k8s" time="1.000">
			<system-out><![CDATA[level=warning msg="Unable to patch node resource with annotation" error="failing on purpose" key=0 nodeName=node2 subsys=k8s v4CiliumHostIP.IPv4=10.254.0.1 v4IngressIP.IPv4="<nil>" v4Prefix=10.254.0.0/16 v4healthIP.IPv4="<nil>" v6CiliumHostIP.IPv6="<nil>" v6IngressIP.IPv6="<nil>" v6Prefix="aaaa:aaaa:aaaa:aaaa:beef:beef::/96" v6healthIP.IPv6="<nil>"]]></system-out>
		</testcase>
		<testcase name="Test/K8sSuite/Test_AnnotationsEqual" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_ConvertToCCNP" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_ConvertToCNP" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_ConvertToCiliumEndpoint" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_ConvertToCiliumNode" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_ConvertToK8sService" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_ConvertToK8sV1LoadBalancerIngress" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_ConvertToK8sV1ServicePorts" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_ConvertToK8sV1SessionAffinityConfig" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_ConvertToNetworkV1IngressLoadBalancerIngress" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_ConvertToNode" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_ConvertToSlimIngressLoadBalancerStatus" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_EqualV1Endpoints" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_EqualV1Namespace" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_EqualV1Node" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_EqualV1Pod" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_EqualV1Service" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_EqualV2CNP" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_parseK8sEPSlicev1" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_parseK8sEPSlicev1Beta1" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test/K8sSuite/Test_parseK8sEPv1" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseNetworkPolicyPeer" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseNetworkPolicyPeer/peer-with-pod-selector" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseNetworkPolicyPeer/peer-nil" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseNetworkPolicyPeer/peer-with-pod-selector-and-ns-selector" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseNetworkPolicyPeer/peer-with-ns-selector" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_parseNetworkPolicyPeer/peer-with-allow-all-ns-selector" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_ParseNodeAddressType" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_ParseNodeAddressType/NodeExternalDNS" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_ParseNodeAddressType/NodeExternalIP" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_ParseNodeAddressType/NodeHostName" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_ParseNodeAddressType/NodeInternalIP" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_ParseNodeAddressType/NodeInternalDNS" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="Test_ParseNodeAddressType/invalid" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/both_equal" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/different_labels" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/different_shared" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/different_include_external" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/different_selector" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/ports_different_name" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/ports_different_content" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/ports_different_one_is_bigger" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/ports_different_one_is_nil" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/nodeports_different" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/external-ip_was_added" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/session_affinity_was_added" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/session_affinity_timeout_changed" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestService_Equals/both_nil" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestParseServiceIDFrom" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestParseServiceIDFrom/cilium-etcd-client.kube-system.svc" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestParseServiceIDFrom/1.kube-system" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestParseServiceIDFrom/.kube-system" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestParseServiceIDFrom/..kube-system" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestParseServiceIDFrom/2-..kube-system" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestParseServiceIDFrom/#00" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestParseServiceIDFrom/cilium-etcd-client.kube-system" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<testcase name="TestParseServiceIDFrom/cilium-etcd-client" classname="github.com/cilium/cilium/pkg/k8s" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/k8s	coverage: 50.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/client" tests="9" failures="0" errors="0" id="430" hostname="kind-bpf-next" time="0.029" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/client" time="0.000"></testcase>
		<testcase name="Test/CiliumV2RegisterSuite" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/client" time="0.000"></testcase>
		<testcase name="Test/CiliumV2RegisterSuite/TestCreateUpdateCRD" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/client" time="0.000">
			<system-out><![CDATA[level=info msg="Creating CRD (CustomResourceDefinition)..." name=foo-v1 subsys=k8s
level=info msg="CRD (CustomResourceDefinition) is installed and up-to-date" name=foo-v1 subsys=k8s
level=info msg="Creating CRD (CustomResourceDefinition)..." name=foo-v1 subsys=k8s
level=info msg="CRD (CustomResourceDefinition) is installed and up-to-date" name=foo-v1 subsys=k8s
level=info msg="Creating CRD (CustomResourceDefinition)..." name=foo-v1 subsys=k8s
level=info msg="CRD (CustomResourceDefinition) is installed and up-to-date" name=foo-v1 subsys=k8s
level=info msg="CRD (CustomResourceDefinition) is installed and up-to-date" name=foo-v1beta1 subsys=k8s]]></system-out>
		</testcase>
		<testcase name="Test/CiliumV2RegisterSuite/TestFQDNNameRegex" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/client" time="0.000"></testcase>
		<testcase name="Test/CiliumV2RegisterSuite/TestNeedsUpdateCorruptedVersion" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/client" time="0.000"></testcase>
		<testcase name="Test/CiliumV2RegisterSuite/TestNeedsUpdateNoLabels" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/client" time="0.000"></testcase>
		<testcase name="Test/CiliumV2RegisterSuite/TestNeedsUpdateNoValidation" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/client" time="0.000"></testcase>
		<testcase name="Test/CiliumV2RegisterSuite/TestNeedsUpdateNoVersionLabel" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/client" time="0.000"></testcase>
		<testcase name="Test/CiliumV2RegisterSuite/TestNeedsUpdateOlderVersion" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/client" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/k8s/apis/cilium.io/client	coverage: 25.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils" tests="10" failures="0" errors="0" id="431" hostname="kind-bpf-next" time="0.018" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils" time="0.000"></testcase>
		<testcase name="Test/CiliumUtilsSuite" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils" time="0.000"></testcase>
		<testcase name="Test/CiliumUtilsSuite/TestParseToCiliumLabels" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils" time="0.000"></testcase>
		<testcase name="Test/CiliumUtilsSuite/Test_namespacesAreValid" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils" time="0.000"></testcase>
		<testcase name="Test_ParseToCiliumRule" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils" time="0.000"></testcase>
		<testcase name="Test_ParseToCiliumRule/parse-in-namespace" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils" time="0.000"></testcase>
		<testcase name="Test_ParseToCiliumRule/parse-in-namespace-with-ns-selector" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils" time="0.000">
			<system-out><![CDATA[level=warning msg="CiliumNetworkPolicy contains illegal namespace match in EndpointSelector. EndpointSelector always applies in namespace of the policy resource, removing illegal namespace match'." ciliumNetworkPolicyName=parse-in-namespace-with-ns-selector k8sNamespace=default k8sNamespace.illegal="[foo]" subsys=k8s]]></system-out>
		</testcase>
		<testcase name="Test_ParseToCiliumRule/parse-init-policy" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils" time="0.000"></testcase>
		<testcase name="Test_ParseToCiliumRule/set-any-source-for-namespace" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils" time="0.000"></testcase>
		<testcase name="Test_ParseToCiliumRule/wildcard-to-from-endpoints-with-ccnp" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils	coverage: 45.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2" tests="9" failures="0" errors="0" id="432" hostname="kind-bpf-next" time="0.036" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2" time="0.010"></testcase>
		<testcase name="Test/CiliumV2Suite" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2" time="0.010"></testcase>
		<testcase name="Test/CiliumV2Suite/TestCiliumNodeInstanceID" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2" time="0.000"></testcase>
		<testcase name="Test/CiliumV2Suite/TestParseEnvoySpec" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2" time="0.010"></testcase>
		<testcase name="Test/CiliumV2Suite/TestParseRules" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2" time="0.000"></testcase>
		<testcase name="Test/CiliumV2Suite/TestParseSpec" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2" time="0.000"></testcase>
		<testcase name="Test/CiliumV2Suite/TestParseWithNodeSelector" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2" time="0.000"></testcase>
		<testcase name="FuzzCiliumNetworkPolicyParse" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2" time="0.000"></testcase>
		<testcase name="FuzzCiliumClusterwideNetworkPolicyParse" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2	coverage: 3.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2/validator" tests="9" failures="0" errors="0" id="433" hostname="kind-bpf-next" time="1.145" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2/validator" time="1.100"></testcase>
		<testcase name="Test/CNPValidationSuite" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2/validator" time="1.100"></testcase>
		<testcase name="Test/CNPValidationSuite/Test_BadCCNP" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2/validator" time="0.090"></testcase>
		<testcase name="Test/CNPValidationSuite/Test_BadMatchLabels" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2/validator" time="0.060"></testcase>
		<testcase name="Test/CNPValidationSuite/Test_GH10643" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2/validator" time="0.080"></testcase>
		<testcase name="Test/CNPValidationSuite/Test_GoodCCNP" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2/validator" time="0.100"></testcase>
		<testcase name="Test/CNPValidationSuite/Test_GoodCNP" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2/validator" time="0.080"></testcase>
		<testcase name="Test/CNPValidationSuite/Test_UnknownFieldDetection" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2/validator" time="0.700">
			<system-out><![CDATA[level=warning msg="It seems you have a policy with a top-level description. This field is no longer supported. Please migrate your policy's description field under `spec` or `specs`." ciliumNetworkPolicyName=cnp-test-1 subsys=validator
level=warning msg="It seems you have a policy with a top-level description. This field is no longer supported. Please migrate your policy's description field under `spec` or `specs`." ciliumClusterwideNetworkPolicyName=ccnp-test-1 subsys=validator
level=warning msg="It seems you have a policy with extra unknown fields. Consider removing these fields, as they have no effect. The presence of these fields may have introduced a false sense security, so please check whether your policy is actually behaving as you expect." ciliumNetworkPolicyName=ccnp-test-1 subsys=validator
level=warning msg="It seems you have a policy with extra unknown fields. Consider removing these fields, as they have no effect. The presence of these fields may have introduced a false sense security, so please check whether your policy is actually behaving as you expect." ciliumClusterwideNetworkPolicyName=ccnp-test-1 subsys=validator]]></system-out>
		</testcase>
		<testcase name="Test/CNPValidationSuite/Test_getFields" classname="github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2/validator" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2/validator	coverage: 72.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/client" tests="4" failures="0" errors="0" id="434" hostname="kind-bpf-next" time="0.213" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/k8s/client" time="0.170"></testcase>
		<testcase name="Test/K8sClientSuite" classname="github.com/cilium/cilium/pkg/k8s/client" time="0.170"></testcase>
		<testcase name="Test/K8sClientSuite/Test_client" classname="github.com/cilium/cilium/pkg/k8s/client" time="0.010">
			<system-out><![CDATA[level=info msg=Invoked duration="501.294µs" function="client.(*K8sClientSuite).Test_client.func3 (client_test.go:232)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Establishing connection to apiserver" host="http://127.0.0.1:42807" subsys=k8s-client
level=info msg="Connected to apiserver" subsys=k8s-client
level=info msg="Start hook executed" duration=1.939574ms function="client.(*compositeClientset).onStart" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="9.788µs" function="client.(*compositeClientset).onStop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="Test/K8sClientSuite/Test_runHeartbeat" classname="github.com/cilium/cilium/pkg/k8s/client" time="0.150">
			<system-out><![CDATA[level=warning msg="Heartbeat timed out, restarting client connections"
level=warning msg="Heartbeat timed out, restarting client connections"
level=warning msg="Network status error received, restarting client connections" error=]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/k8s/client	coverage: 63.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/identitybackend" tests="10" failures="0" errors="0" id="435" hostname="kind-bpf-next" time="0.644" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/k8s/identitybackend" time="0.000"></testcase>
		<testcase name="Test/K8sIdentityBackendSuite" classname="github.com/cilium/cilium/pkg/k8s/identitybackend" time="0.000"></testcase>
		<testcase name="Test/K8sIdentityBackendSuite/TestSanitizeK8sLabels" classname="github.com/cilium/cilium/pkg/k8s/identitybackend" time="0.000"></testcase>
		<testcase name="TestGetIdentity" classname="github.com/cilium/cilium/pkg/k8s/identitybackend" time="0.610"></testcase>
		<testcase name="TestGetIdentity/Simple_case" classname="github.com/cilium/cilium/pkg/k8s/identitybackend" time="0.030"></testcase>
		<testcase name="TestGetIdentity/Multiple_identities" classname="github.com/cilium/cilium/pkg/k8s/identitybackend" time="0.030"></testcase>
		<testcase name="TestGetIdentity/Duplicated_identity" classname="github.com/cilium/cilium/pkg/k8s/identitybackend" time="0.030"></testcase>
		<testcase name="TestGetIdentity/Duplicated_key" classname="github.com/cilium/cilium/pkg/k8s/identitybackend" time="0.030"></testcase>
		<testcase name="TestGetIdentity/No_identities" classname="github.com/cilium/cilium/pkg/k8s/identitybackend" time="0.250"></testcase>
		<testcase name="TestGetIdentity/Identity_not_found" classname="github.com/cilium/cilium/pkg/k8s/identitybackend" time="0.250"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/k8s/identitybackend	coverage: 36.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/metrics" tests="5" failures="0" errors="0" id="436" hostname="kind-bpf-next" time="0.005" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/k8s/metrics" time="0.000"></testcase>
		<testcase name="Test/K8sIntegrationSuite" classname="github.com/cilium/cilium/pkg/k8s/metrics" time="0.000"></testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/k8s/metrics" time="0.000"></testcase>
		<testcase name="Test/MetricsSuite" classname="github.com/cilium/cilium/pkg/k8s/metrics" time="0.000"></testcase>
		<testcase name="Test/MetricsSuite/TestLastInteraction" classname="github.com/cilium/cilium/pkg/k8s/metrics" time="0.000"></testcase>
		<system-out><![CDATA[coverage: [no statements]
ok  	github.com/cilium/cilium/pkg/k8s/informer/benchmarks	0.030s	coverage: [no statements]
	github.com/cilium/cilium/pkg/k8s/metrics	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/resource" tests="4" failures="0" errors="0" id="437" hostname="kind-bpf-next" skipped="1" time="0.580" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestResource_WithFakeClient" classname="github.com/cilium/cilium/pkg/k8s/resource" time="0.100">
			<system-out><![CDATA[level=info msg=Invoked duration="79.92µs" function="resource_test.TestResource_WithFakeClient.func2 (resource_test.go:114)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="20.268µs" function="*resource.resource[*k8s.io/api/core/v1.Node].Start" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="58.853µs" function="*resource.resource[*k8s.io/api/core/v1.Node].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestResource_CompletionOnStop" classname="github.com/cilium/cilium/pkg/k8s/resource" time="0.100">
			<system-out><![CDATA[level=info msg=Invoked duration="28.353µs" function="resource_test.TestResource_CompletionOnStop.func1 (resource_test.go:251)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="2.675µs" function="*resource.resource[*k8s.io/api/core/v1.Node].Start" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="55.731µs" function="*resource.resource[*k8s.io/api/core/v1.Node].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestResource_Retries" classname="github.com/cilium/cilium/pkg/k8s/resource" time="0.340">
			<system-out><![CDATA[level=info msg=Invoked duration="20.528µs" function="resource_test.TestResource_Retries.func4 (resource_test.go:315)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="1.673µs" function="*resource.resource[*k8s.io/api/core/v1.Node].Start" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="48.751µs" function="*resource.resource[*k8s.io/api/core/v1.Node].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestResource_SkippedDonePanics" classname="github.com/cilium/cilium/pkg/k8s/resource" time="0.000">
			<skipped message="Skipped"><![CDATA[    resource_test.go:511: This test can be only done manually as it tests finalizer panicing]]></skipped>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/k8s/resource	coverage: 86.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/synced" tests="6" failures="0" errors="0" id="438" hostname="kind-bpf-next" time="0.746" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestWaitForCacheSyncWithTimeout" classname="github.com/cilium/cilium/pkg/k8s/synced" time="0.000"></testcase>
		<testcase name="TestWaitForCacheSyncWithTimeout/Should_timeout_due_to_watched_resource_exceeding_timeout" classname="github.com/cilium/cilium/pkg/k8s/synced" time="0.200"></testcase>
		<testcase name="TestWaitForCacheSyncWithTimeout/Any_one_timeout_should_cause_error" classname="github.com/cilium/cilium/pkg/k8s/synced" time="0.600"></testcase>
		<testcase name="TestWaitForCacheSyncWithTimeout/Waiting_for_no_resources_should_always_sync" classname="github.com/cilium/cilium/pkg/k8s/synced" time="0.000"></testcase>
		<testcase name="TestWaitForCacheSyncWithTimeout/Not_invoking_BlockWaitGroupToSyncResources_should_cause_wait_to_succeed_immediately" classname="github.com/cilium/cilium/pkg/k8s/synced" time="0.000"></testcase>
		<testcase name="TestWaitForCacheSyncWithTimeout/Should_complete_due_to_event_causing_timeout_to_be_extended_past_initial_timeout" classname="github.com/cilium/cilium/pkg/k8s/synced" time="0.700"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/k8s/synced	coverage: 35.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/utils" tests="31" failures="0" errors="0" id="439" hostname="kind-bpf-next" time="0.021" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestServiceProxyName" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestValidIPs" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestValidIPs/podip_is_nil" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestValidIPs/one_pod_ip" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestValidIPs/duplicate_ip" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestValidIPs/multiple_pod_ip" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestValidIPs/have_empty_pod_ip" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestIsPodRunning" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestIsPodRunning/Pod_is_not_Running" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestIsPodRunning/Pod_is_Running" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestGetLatestPodReadiness" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestGetLatestPodReadiness/conditions_are_podReadyconditiontrue,_podReadyconditionfalse_and_podReadyconditionUnknown" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestGetLatestPodReadiness/conditions_are_podReadyconditionfalse_and_podScheduled" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestGetLatestPodReadiness/conditions_are_podReadyconditionUnknown_and_podReadyconditiontrue" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestGetLatestPodReadiness/conditions_are_podScheduled_and_podReadyconditiontrue" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestGetLatestPodReadiness/conditions_is_podScheduled" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestDeploymentMetadata" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestDeploymentMetadata/deployment-name-deploy" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestDeploymentMetadata/deployment-name-deploy2" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestDeploymentMetadata/non-deployment" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestDeploymentMetadata/bare-pod" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestCronJobMetadata" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestCronJobMetadata/cron-job-name-sec" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestCronJobMetadata/cron-job-name-min" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestCronJobMetadata/non-cron-job-name" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestDeploymentConfigMetadata" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestDeploymentConfigMetadata/deployconfig-name-deploy" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestDeploymentConfigMetadata/deployconfig-name-deploy2" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestDeploymentConfigMetadata/non-deployconfig-label" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestStatefulSetMetadata" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<testcase name="TestStatefulSetMetadata/statefulset-name-foo" classname="github.com/cilium/cilium/pkg/k8s/utils" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/k8s/utils	coverage: 64.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/k8s/watchers" tests="58" failures="0" errors="0" id="440" hostname="kind-bpf-next" time="0.068" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestHasCIDRGroupRef" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestHasCIDRGroupRef/nil_Spec" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestHasCIDRGroupRef/nil_Ingress" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestHasCIDRGroupRef/nil_FromCidrSet_rule" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestHasCIDRGroupRef/missing_CIDRGroup" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestHasCIDRGroupRef/CIDRGroupRef_in_Spec" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestHasCIDRGroupRef/CIDR_in_Spec" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestHasCIDRGroupRef/CIDRGroupRef_in_Specs" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsGet" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsGet/nil_Spec" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsGet/nil_Spec_with_non-nil_Specs" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsGet/nil_Ingress" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsGet/nil_FromCidrSet_rule" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsGet/single_FromCidrSet_rule" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsGet/single_FromCidrSet_rule_with_only_CIDR" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsGet/multiple_FromCidrSet_rules" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsToCIDRsSets" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsToCIDRsSets/nil_refs" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsToCIDRsSets/with_refs" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsTranslate" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsTranslate/nil_Spec" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsTranslate/nil_Ingress" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsTranslate/nil_FromCidrSet_rule" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsTranslate/with_FromCidrSet_rules" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsTranslate/with_mixed_FromCidrSet_rules" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCIDRGroupRefsTranslate/with_CIDRGroupRef_and_ExceptCIDRs_rules" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCESSubscriber_CEPTransferOnStartup" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000">
			<system-out><![CDATA[level=info msg="CEP deleted, other CEP exists, calling endpointUpdated" CEPName=ns1/cep1 CESName=old-ces subsys=k8s-watcher]]></system-out>
		</testcase>
		<testcase name="TestCESSubscriber_CEPTransferViaUpdate" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000">
			<system-out><![CDATA[level=info msg="CEP deleted, calling endpointDeleted" CEPName=ns1/cep1 CESName=old-ces subsys=k8s-watcher]]></system-out>
		</testcase>
		<testcase name="TestCESSubscriber_deleteCEPfromCES" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCESSubscriber_deleteCEPfromCES/delete_CEP_triggers_deletion" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000">
			<system-out><![CDATA[level=info msg="CEP deleted, calling endpointDeleted" CEPName=ns1/cep1 CESName=ces1 subsys=k8s-watcher]]></system-out>
		</testcase>
		<testcase name="TestCESSubscriber_deleteCEPfromCES/delete_CEP_triggers_update" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000">
			<system-out><![CDATA[level=info msg="CEP deleted, other CEP exists, calling endpointUpdated" CEPName=ns1/cep1 CESName=ces1 subsys=k8s-watcher]]></system-out>
		</testcase>
		<testcase name="TestCESSubscriber_deleteCEPfromCES/delete_CEP_triggers_no_update_or_deletion" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCEPToCESmap_insertCEP" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCEPToCESmap_insertCEP/add_new_cep" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCEPToCESmap_insertCEP/update_cep_object" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCEPToCESmap_insertCEP/add_new_ces_for_existing_cep" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCEPToCESmap_deleteCEP" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCEPToCESmap_deleteCEP/missing_ces_does_not_delete_any_entries" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCEPToCESmap_deleteCEP/missing_cep_does_not_delete_any_entries" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCEPToCESmap_deleteCEP/last_ces_entry" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="TestCEPToCESmap_deleteCEP/multiple_ces_entries" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test_updateCEPUID" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test_updateCEPUID/no_net_status" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test_updateCEPUID/CiliumEndpoint_not_local" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test_updateCEPUID/CiliumEndpoint_not_local,_but_already_owned" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test_updateCEPUID/ciliumendpoint_already_exists" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test_updateCEPUID/take_ownership_of_cep_due_to_empty_CiliumEndpointUID_ref" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test_k8sToEnvoySecret" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.010"></testcase>
		<testcase name="Test/K8sWatcherSuite" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.010"></testcase>
		<testcase name="Test/K8sWatcherSuite/TestChangeSVCPort" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000">
			<system-out><![CDATA[level=warning msg="service not found" k8sNamespace=bar k8sSvcName=foo obj="{AddrCluster:172.0.20.1 L4Addr:{Protocol:UDP Port:80} Scope:0}" subsys=k8s-watcher]]></system-out>
		</testcase>
		<testcase name="Test/K8sWatcherSuite/TestParseEnvoySpec" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test/K8sWatcherSuite/TestUpdateToServiceEndpointsGH9525" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test/K8sWatcherSuite/Test_addK8sSVCs_ClusterIP" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test/K8sWatcherSuite/Test_addK8sSVCs_ExternalIPs" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test/K8sWatcherSuite/Test_addK8sSVCs_GH9576_1" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test/K8sWatcherSuite/Test_addK8sSVCs_GH9576_2" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<testcase name="Test/K8sWatcherSuite/Test_addK8sSVCs_NodePort" classname="github.com/cilium/cilium/pkg/k8s/watchers" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/k8s/watchers	coverage: 12.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/kafka" tests="4" failures="0" errors="0" id="441" hostname="kind-bpf-next" time="0.022" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/kafka" time="0.000"></testcase>
		<testcase name="Test/kafkaTestSuite" classname="github.com/cilium/cilium/pkg/kafka" time="0.000"></testcase>
		<testcase name="Test/kafkaTestSuite/TestProduceRequest" classname="github.com/cilium/cilium/pkg/kafka" time="0.000"></testcase>
		<testcase name="Test/kafkaTestSuite/TestUnknownRequest" classname="github.com/cilium/cilium/pkg/kafka" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/kafka	coverage: 16.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/kvstore" tests="16" failures="0" errors="0" id="442" hostname="kind-bpf-next" skipped="6" time="0.033" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestConsulClientOk" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000">
			<skipped message="Skipped"><![CDATA[    consul_test.go:71: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="TestGetSvcNamespace" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000"></testcase>
		<testcase name="TestGetSvcNamespace/test-1" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000"></testcase>
		<testcase name="TestGetSvcNamespace/test-2" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000"></testcase>
		<testcase name="TestGetSvcNamespace/test-3" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000"></testcase>
		<testcase name="TestGetSvcNamespace/test-4" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000"></testcase>
		<testcase name="TestGetSvcNamespace/test-5" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000"></testcase>
		<testcase name="TestShuffleEndpoints" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000"></testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000"></testcase>
		<testcase name="Test/ConsulSuite" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/EtcdSuite" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/EtcdHelpersSuite" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000"></testcase>
		<testcase name="Test/EtcdHelpersSuite/TestIsEtcdOperator" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000"></testcase>
		<testcase name="Test/EtcdLockedSuite" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/EtcdRateLimiterSuite" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/independentSuite" classname="github.com/cilium/cilium/pkg/kvstore" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/kvstore	coverage: 4.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/kvstore/allocator" tests="3" failures="0" errors="0" id="443" hostname="kind-bpf-next" skipped="2" time="0.020" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/kvstore/allocator" time="0.000"></testcase>
		<testcase name="Test/AllocatorEtcdSuite" classname="github.com/cilium/cilium/pkg/kvstore/allocator" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/AllocatorConsulSuite" classname="github.com/cilium/cilium/pkg/kvstore/allocator" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/kvstore/allocator	coverage: 0.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/kvstore/store" tests="14" failures="0" errors="0" id="444" hostname="kind-bpf-next" skipped="2" time="0.074" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.000"></testcase>
		<testcase name="Test/StoreEtcdSuite" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="Test/StoreConsulSuite" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.000">
			<skipped message="Skipped"><![CDATA[    privileged.go:48: Set INTEGRATION_TESTS to run this test]]></skipped>
		</testcase>
		<testcase name="TestWorkqueueSyncStore" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.010">
			<system-out><![CDATA[level=info msg="Starting workqueue-based sync store" clusterName= prefix=/foo/bar subsys=shared-store workers=1
level=warning msg="Failed upserting key in kvstore. Retrying..." clusterName= error="failing on purpose" key=key1 prefix=/foo/bar subsys=shared-store
level=warning msg="Failed deleting key from kvstore. Retrying..." clusterName= error="failing on purpose" key=key2 prefix=/foo/bar subsys=shared-store
level=info msg="Shutting down workqueue-based sync store" clusterName= prefix=/foo/bar subsys=shared-store]]></system-out>
		</testcase>
		<testcase name="TestWorkqueueSyncStoreWithoutLease" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.000">
			<system-out><![CDATA[level=info msg="Starting workqueue-based sync store" clusterName= prefix=/foo/bar subsys=shared-store workers=1
level=info msg="Shutting down workqueue-based sync store" clusterName= prefix=/foo/bar subsys=shared-store]]></system-out>
		</testcase>
		<testcase name="TestWorkqueueSyncStoreWithRateLimiter" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.000">
			<system-out><![CDATA[level=info msg="Starting workqueue-based sync store" clusterName= prefix=/foo/bar subsys=shared-store workers=1
level=warning msg="Failed upserting key in kvstore. Retrying..." clusterName= error="failing on purpose" key=key1 prefix=/foo/bar subsys=shared-store
level=info msg="Shutting down workqueue-based sync store" clusterName= prefix=/foo/bar subsys=shared-store]]></system-out>
		</testcase>
		<testcase name="TestWorkqueueSyncStoreWithWorkers" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.000">
			<system-out><![CDATA[level=info msg="Starting workqueue-based sync store" clusterName= prefix=/foo/bar subsys=shared-store workers=2
level=info msg="Shutting down workqueue-based sync store" clusterName= prefix=/foo/bar subsys=shared-store]]></system-out>
		</testcase>
		<testcase name="TestWorkqueueSyncStoreSynced" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.030"></testcase>
		<testcase name="TestWorkqueueSyncStoreSynced/standard" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.000">
			<system-out><![CDATA[level=info msg="Starting workqueue-based sync store" clusterName=qux prefix=foo/bar subsys=shared-store workers=1
level=info msg="Initial synchronization from the external source completed" clusterName=qux prefix=foo/bar subsys=shared-store
level=info msg="Shutting down workqueue-based sync store" clusterName=qux prefix=foo/bar subsys=shared-store]]></system-out>
		</testcase>
		<testcase name="TestWorkqueueSyncStoreSynced/key-override" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.000">
			<system-out><![CDATA[level=info msg="Starting workqueue-based sync store" clusterName=qux prefix=foo/bar subsys=shared-store workers=1
level=info msg="Initial synchronization from the external source completed" clusterName=qux prefix=foo/bar subsys=shared-store
level=info msg="Shutting down workqueue-based sync store" clusterName=qux prefix=foo/bar subsys=shared-store]]></system-out>
		</testcase>
		<testcase name="TestWorkqueueSyncStoreSynced/key-upsertion-failure" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.010">
			<system-out><![CDATA[level=info msg="Starting workqueue-based sync store" clusterName=qux prefix=foo/bar subsys=shared-store workers=1
level=warning msg="Failed upserting key in kvstore. Retrying..." clusterName=qux error="failing on purpose" key=key1 prefix=foo/bar subsys=shared-store
level=info msg="Shutting down workqueue-based sync store" clusterName=qux prefix=foo/bar subsys=shared-store
level=info msg="Initial synchronization from the external source completed" clusterName=qux prefix=foo/bar subsys=shared-store]]></system-out>
		</testcase>
		<testcase name="TestWorkqueueSyncStoreSynced/synced-upsertion-failure" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.010">
			<system-out><![CDATA[level=info msg="Starting workqueue-based sync store" clusterName=qux prefix=foo/bar subsys=shared-store workers=1
level=warning msg="Failed upserting synced key in kvstore. Retrying..." clusterName=qux error="failing on purpose" key=cilium/synced/qux/foo/bar prefix=foo/bar subsys=shared-store
level=info msg="Initial synchronization from the external source completed" clusterName=qux prefix=foo/bar subsys=shared-store
level=info msg="Shutting down workqueue-based sync store" clusterName=qux prefix=foo/bar subsys=shared-store]]></system-out>
		</testcase>
		<testcase name="TestWorkqueueSyncStoreSynced/multiple-workers" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.020">
			<system-out><![CDATA[level=info msg="Starting workqueue-based sync store" clusterName=qux prefix=foo/bar subsys=shared-store workers=10
level=warning msg="Failed upserting key in kvstore. Retrying..." clusterName=qux error="failing on purpose" key=key1 prefix=foo/bar subsys=shared-store
level=warning msg="Failed upserting key in kvstore. Retrying..." clusterName=qux error="failing on purpose" key=key1 prefix=foo/bar subsys=shared-store
level=info msg="Shutting down workqueue-based sync store" clusterName=qux prefix=foo/bar subsys=shared-store
level=info msg="Initial synchronization from the external source completed" clusterName=qux prefix=foo/bar subsys=shared-store]]></system-out>
		</testcase>
		<testcase name="TestWorkqueueSyncStoreMetrics" classname="github.com/cilium/cilium/pkg/kvstore/store" time="0.020">
			<system-out><![CDATA[level=info msg="Starting workqueue-based sync store" clusterName=foo prefix=cilium/state/nodes/v1 subsys=shared-store workers=1
level=warning msg="Failed upserting key in kvstore. Retrying..." clusterName=foo error="failing on purpose" key=key3 prefix=cilium/state/nodes/v1 subsys=shared-store
level=info msg="Initial synchronization from the external source completed" clusterName=foo prefix=cilium/state/nodes/v1 subsys=shared-store
level=info msg="Shutting down workqueue-based sync store" clusterName=foo prefix=cilium/state/nodes/v1 subsys=shared-store]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/kvstore/store	coverage: 42.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/labels" tests="51" failures="5" errors="0" id="445" hostname="kind-bpf-next" time="0.015" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/labels" time="0.010">
			<failure message="Failed"></failure>
		</testcase>
		<testcase name="Test/LabelsSuite" classname="github.com/cilium/cilium/pkg/labels" time="0.000">
			<failure message="Failed"></failure>
		</testcase>
		<testcase name="Test/LabelsSuite/TestEquals" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestHas" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestLabel" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestLabelArrayListEquals" classname="github.com/cilium/cilium/pkg/labels" time="0.000">
			<failure message="Failed"><![CDATA[    arraylist_test.go:50: 
        ... obtained bool = false
        ... expected bool = true
        ]]></failure>
		</testcase>
		<testcase name="Test/LabelsSuite/TestLabelArrayListMergeSorted" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestLabelArrayListSort" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestLabelArraySorted" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestLabelCompare" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestLabelParseKey" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestLabelsCompare" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestLabelsK8sStringMap" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestLess" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestMap2Labels" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestMatches" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestMergeLabels" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestOutputConversions" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestParse" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestParseLabel" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestParseSelectLabel" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite/TestSortMap" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01" classname="github.com/cilium/cilium/pkg/labels" time="0.000">
			<failure message="Failed"></failure>
		</testcase>
		<testcase name="Test/LabelsSuite#01/TestEquals" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestHas" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestLabel" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestLabelArrayListEquals" classname="github.com/cilium/cilium/pkg/labels" time="0.000">
			<failure message="Failed"><![CDATA[    arraylist_test.go:50: 
        ... obtained bool = false
        ... expected bool = true
        ]]></failure>
		</testcase>
		<testcase name="Test/LabelsSuite#01/TestLabelArrayListMergeSorted" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestLabelArrayListSort" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestLabelArraySorted" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestLabelCompare" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestLabelParseKey" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestLabelsCompare" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestLabelsK8sStringMap" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestLess" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestMap2Labels" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestMatches" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestMergeLabels" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestOutputConversions" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestParse" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestParseLabel" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestParseSelectLabel" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="Test/LabelsSuite#01/TestSortMap" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="TestLabels_Has" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="TestLabels_Has/empty_labels" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="TestLabels_Has/has_label" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="TestLabels_Has/does_not_have_label" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="TestLabels_GetFromSource" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="TestLabels_GetFromSource/should_contain_label_with_the_given_source" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="TestLabels_GetFromSource/should_return_an_empty_slice_as_there_are_not_labels_for_the_given_source" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<testcase name="TestLabel_String" classname="github.com/cilium/cilium/pkg/labels" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/labels	coverage: 45.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/labels/cidr" tests="5" failures="0" errors="0" id="446" hostname="kind-bpf-next" time="0.008" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/labels/cidr" time="0.000"></testcase>
		<testcase name="Test/CIDRLabelsSuite" classname="github.com/cilium/cilium/pkg/labels/cidr" time="0.000"></testcase>
		<testcase name="Test/CIDRLabelsSuite/TestGetCIDRLabels" classname="github.com/cilium/cilium/pkg/labels/cidr" time="0.000"></testcase>
		<testcase name="Test/CIDRLabelsSuite/TestGetCIDRLabelsInCluster" classname="github.com/cilium/cilium/pkg/labels/cidr" time="0.000"></testcase>
		<testcase name="Test/CIDRLabelsSuite/TestIPStringToLabel" classname="github.com/cilium/cilium/pkg/labels/cidr" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/labels/cidr	coverage: 97.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/labelsfilter" tests="5" failures="0" errors="0" id="447" hostname="kind-bpf-next" time="0.008" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/labelsfilter" time="0.000"></testcase>
		<testcase name="Test/LabelsPrefCfgSuite" classname="github.com/cilium/cilium/pkg/labelsfilter" time="0.000"></testcase>
		<testcase name="Test/LabelsPrefCfgSuite/TestDefaultFilterLabels" classname="github.com/cilium/cilium/pkg/labelsfilter" time="0.000">
			<system-out><![CDATA[level=info msg="Parsing base label prefixes from default label list" subsys=labels-filter
level=info msg="Parsing additional label prefixes from user inputs: []" subsys=labels-filter
level=info msg="Final label prefixes to be used for identity evaluation:" subsys=labels-filter
level=info msg=" - reserved:.*" subsys=labels-filter
level=info msg=" - :io\\.kubernetes\\.pod\\.namespace" subsys=labels-filter
level=info msg=" - :io\\.cilium\\.k8s\\.namespace\\.labels" subsys=labels-filter
level=info msg=" - :app\\.kubernetes\\.io" subsys=labels-filter
level=info msg=" - !:io\\.kubernetes" subsys=labels-filter
level=info msg=" - !:kubernetes\\.io" subsys=labels-filter
level=info msg=" - !:.*beta\\.kubernetes\\.io" subsys=labels-filter
level=info msg=" - !:k8s\\.io" subsys=labels-filter
level=info msg=" - !:pod-template-generation" subsys=labels-filter
level=info msg=" - !:pod-template-hash" subsys=labels-filter
level=info msg=" - !:controller-revision-hash" subsys=labels-filter
level=info msg=" - !:annotation.*" subsys=labels-filter
level=info msg=" - !:etcd_node" subsys=labels-filter]]></system-out>
		</testcase>
		<testcase name="Test/LabelsPrefCfgSuite/TestFilterLabels" classname="github.com/cilium/cilium/pkg/labelsfilter" time="0.000">
			<system-out><![CDATA[level=info msg="Parsing base label prefixes from default label list" subsys=labels-filter
level=info msg="Parsing additional label prefixes from user inputs: [:!ignor[eE] id.* foo]" subsys=labels-filter
level=info msg="Final label prefixes to be used for identity evaluation:" subsys=labels-filter
level=info msg=" - reserved:.*" subsys=labels-filter
level=info msg=" - :io\\.kubernetes\\.pod\\.namespace" subsys=labels-filter
level=info msg=" - :io\\.cilium\\.k8s\\.namespace\\.labels" subsys=labels-filter
level=info msg=" - :app\\.kubernetes\\.io" subsys=labels-filter
level=info msg=" - !:io\\.kubernetes" subsys=labels-filter
level=info msg=" - !:kubernetes\\.io" subsys=labels-filter
level=info msg=" - !:.*beta\\.kubernetes\\.io" subsys=labels-filter
level=info msg=" - !:k8s\\.io" subsys=labels-filter
level=info msg=" - !:pod-template-generation" subsys=labels-filter
level=info msg=" - !:pod-template-hash" subsys=labels-filter
level=info msg=" - !:controller-revision-hash" subsys=labels-filter
level=info msg=" - !:annotation.*" subsys=labels-filter
level=info msg=" - !:etcd_node" subsys=labels-filter
level=info msg=" - !:ignor[eE]" subsys=labels-filter
level=info msg=" - :id.*" subsys=labels-filter
level=info msg=" - :foo" subsys=labels-filter]]></system-out>
		</testcase>
		<testcase name="Test/LabelsPrefCfgSuite/TestFilterLabelsDocExample" classname="github.com/cilium/cilium/pkg/labelsfilter" time="0.000">
			<system-out><![CDATA[level=info msg="Parsing base label prefixes from default label list" subsys=labels-filter
level=info msg="Parsing additional label prefixes from user inputs: [k8s:io.kubernetes.pod.namespace k8s:k8s-app k8s:app k8s:name]" subsys=labels-filter
level=info msg="Final label prefixes to be used for identity evaluation:" subsys=labels-filter
level=info msg=" - reserved:.*" subsys=labels-filter
level=info msg=" - :io\\.kubernetes\\.pod\\.namespace" subsys=labels-filter
level=info msg=" - :io\\.cilium\\.k8s\\.namespace\\.labels" subsys=labels-filter
level=info msg=" - :app\\.kubernetes\\.io" subsys=labels-filter
level=info msg=" - !:io\\.kubernetes" subsys=labels-filter
level=info msg=" - !:kubernetes\\.io" subsys=labels-filter
level=info msg=" - !:.*beta\\.kubernetes\\.io" subsys=labels-filter
level=info msg=" - !:k8s\\.io" subsys=labels-filter
level=info msg=" - !:pod-template-generation" subsys=labels-filter
level=info msg=" - !:pod-template-hash" subsys=labels-filter
level=info msg=" - !:controller-revision-hash" subsys=labels-filter
level=info msg=" - !:annotation.*" subsys=labels-filter
level=info msg=" - !:etcd_node" subsys=labels-filter
level=info msg=" - k8s:io.kubernetes.pod.namespace" subsys=labels-filter
level=info msg=" - k8s:k8s-app" subsys=labels-filter
level=info msg=" - k8s:app" subsys=labels-filter
level=info msg=" - k8s:name" subsys=labels-filter]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/labelsfilter	coverage: 63.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/loadbalancer" tests="42" failures="0" errors="0" id="448" hostname="kind-bpf-next" time="0.024" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="Test/TypesSuite" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestL4Addr_Equals" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestL4Addr_Equals/both_equal" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestL4Addr_Equals/both_different" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestL4Addr_Equals/both_nil" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestL4Addr_Equals/other_nil" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestL3n4AddrID_Equals" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestL3n4AddrID_Equals/both_equal" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestL3n4AddrID_Equals/IDs_different" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestL3n4AddrID_Equals/IPs_different" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestL3n4AddrID_Equals/Ports_different" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestL3n4AddrID_Equals/both_nil" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#00" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#01" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#02" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#03" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#04" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#05" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#06" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#07" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#08" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#09" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#10" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#11" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#12" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#13" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#14" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#15" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestNewSvcFlag/#16" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestServiceFlags_String" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestServiceFlags_String/Test-1" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestServiceFlags_String/Test-2" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestServiceFlags_String/Test-3" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestServiceFlags_String/Test-4" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestServiceFlags_String/Test-5" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestServiceFlags_String/Test-6" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestServiceFlags_String/Test-7" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestServiceFlags_String/Test-8" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestServiceFlags_String/Test-9" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<testcase name="TestServiceFlags_String/Test-10" classname="github.com/cilium/cilium/pkg/loadbalancer" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/loadbalancer	coverage: 19.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/lock" tests="13" failures="0" errors="0" id="449" hostname="kind-bpf-next" time="2.111" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/lock" time="2.110"></testcase>
		<testcase name="Test/LockSuite" classname="github.com/cilium/cilium/pkg/lock" time="0.000"></testcase>
		<testcase name="Test/LockSuite/TestDebugLock" classname="github.com/cilium/cilium/pkg/lock" time="0.000"></testcase>
		<testcase name="Test/LockSuite/TestLock" classname="github.com/cilium/cilium/pkg/lock" time="0.000"></testcase>
		<testcase name="Test/SemaphoredMutexSuite" classname="github.com/cilium/cilium/pkg/lock" time="2.110"></testcase>
		<testcase name="Test/SemaphoredMutexSuite/TestAdd" classname="github.com/cilium/cilium/pkg/lock" time="0.000"></testcase>
		<testcase name="Test/SemaphoredMutexSuite/TestDone" classname="github.com/cilium/cilium/pkg/lock" time="0.000"></testcase>
		<testcase name="Test/SemaphoredMutexSuite/TestLock" classname="github.com/cilium/cilium/pkg/lock" time="0.000"></testcase>
		<testcase name="Test/SemaphoredMutexSuite/TestParallelism" classname="github.com/cilium/cilium/pkg/lock" time="2.100"></testcase>
		<testcase name="Test/SemaphoredMutexSuite/TestStop" classname="github.com/cilium/cilium/pkg/lock" time="0.000"></testcase>
		<testcase name="Test/SemaphoredMutexSuite/TestWait" classname="github.com/cilium/cilium/pkg/lock" time="0.000"></testcase>
		<testcase name="Test/SemaphoredMutexSuite/TestWaitChannel" classname="github.com/cilium/cilium/pkg/lock" time="0.000"></testcase>
		<testcase name="Test/StoppableWaitGroupSuite" classname="github.com/cilium/cilium/pkg/lock" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/lock	coverage: 83.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/lock/lockfile" tests="3" failures="0" errors="0" id="450" hostname="kind-bpf-next" time="1.006" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestLockfile" classname="github.com/cilium/cilium/pkg/lock/lockfile" time="0.000"></testcase>
		<testcase name="TestLockfileShared" classname="github.com/cilium/cilium/pkg/lock/lockfile" time="0.000"></testcase>
		<testcase name="TestLockfileCancel" classname="github.com/cilium/cilium/pkg/lock/lockfile" time="1.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/lock/lockfile	coverage: 93.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/logging" tests="10" failures="0" errors="0" id="451" hostname="kind-bpf-next" time="3.508" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/logging" time="3.500"></testcase>
		<testcase name="Test/LoggingSuite" classname="github.com/cilium/cilium/pkg/logging" time="3.500"></testcase>
		<testcase name="Test/LoggingSuite/TestGetLogFormat" classname="github.com/cilium/cilium/pkg/logging" time="0.000">
			<system-out><![CDATA[time="2023-05-31T11:43:33Z" level=warning msg="Ignoring user-configured log format" error="incorrect log format configured 'invalid', expected 'text', 'json' or 'json-ts'"]]></system-out>
		</testcase>
		<testcase name="Test/LoggingSuite/TestGetLogLevel" classname="github.com/cilium/cilium/pkg/logging" time="0.000">
			<system-out><![CDATA[time="2023-05-31T11:43:33Z" level=warning msg="Ignoring user-configured log level" error="not a valid logrus Level: \"Invalid\""]]></system-out>
		</testcase>
		<testcase name="Test/LoggingSuite/TestLimiter" classname="github.com/cilium/cilium/pkg/logging" time="3.500"></testcase>
		<testcase name="Test/LoggingSuite/TestSetDefaultLogFormat" classname="github.com/cilium/cilium/pkg/logging" time="0.000"></testcase>
		<testcase name="Test/LoggingSuite/TestSetDefaultLogLevel" classname="github.com/cilium/cilium/pkg/logging" time="0.000"></testcase>
		<testcase name="Test/LoggingSuite/TestSetLogFormat" classname="github.com/cilium/cilium/pkg/logging" time="0.000"></testcase>
		<testcase name="Test/LoggingSuite/TestSetLogLevel" classname="github.com/cilium/cilium/pkg/logging" time="0.000"></testcase>
		<testcase name="Test/LoggingSuite/TestSetupLogging" classname="github.com/cilium/cilium/pkg/logging" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/logging	coverage: 40.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/mac" tests="4" failures="0" errors="0" id="452" hostname="kind-bpf-next" time="0.005" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/mac" time="0.000"></testcase>
		<testcase name="Test/MACSuite" classname="github.com/cilium/cilium/pkg/mac" time="0.000"></testcase>
		<testcase name="Test/MACSuite/TestUint64" classname="github.com/cilium/cilium/pkg/mac" time="0.000"></testcase>
		<testcase name="Test/MACSuite/TestUnmarshalJSON" classname="github.com/cilium/cilium/pkg/mac" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/mac	coverage: 40.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maglev" tests="5" failures="0" errors="0" id="453" hostname="kind-bpf-next" time="0.545" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/maglev" time="0.530"></testcase>
		<testcase name="Test/MaglevTestSuite" classname="github.com/cilium/cilium/pkg/maglev" time="0.530"></testcase>
		<testcase name="Test/MaglevTestSuite/TestBackendRemoval" classname="github.com/cilium/cilium/pkg/maglev" time="0.000"></testcase>
		<testcase name="Test/MaglevTestSuite/TestPermutations" classname="github.com/cilium/cilium/pkg/maglev" time="0.520"></testcase>
		<testcase name="Test/MaglevTestSuite/TestWeightedBackendWithRemoval" classname="github.com/cilium/cilium/pkg/maglev" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/maglev	coverage: 93.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/authmap" tests="3" failures="0" errors="0" id="454" hostname="kind-bpf-next" time="0.020" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/maps/authmap" time="0.000"></testcase>
		<testcase name="Test/AuthMapTestSuite" classname="github.com/cilium/cilium/pkg/maps/authmap" time="0.000">
			<system-out><![CDATA[level=info msg="Detected mounted BPF filesystem at /sys/fs/bpf" subsys=bpf]]></system-out>
		</testcase>
		<testcase name="Test/AuthMapTestSuite/TestAuthMap" classname="github.com/cilium/cilium/pkg/maps/authmap" time="0.000">
			<system-out><![CDATA[level=warning msg="Removing map to allow for property upgrade (expect map data loss)" error="creating map: use pinned map cilium_auth_map: expected max entries 10, got 524288: map spec is incompatible with existing map" map=cilium_auth_map subsys=ebpf]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/maps/authmap	coverage: 17.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/ctmap" tests="11" failures="0" errors="0" id="455" hostname="kind-bpf-next" time="58.347" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/maps/ctmap" time="58.310"></testcase>
		<testcase name="Test/CTMapPrivilegedTestSuite" classname="github.com/cilium/cilium/pkg/maps/ctmap" time="0.000">
			<system-out><![CDATA[level=info msg="Detected mounted BPF filesystem at /sys/fs/bpf" subsys=bpf]]></system-out>
		</testcase>
		<testcase name="Test/CTMapPrivilegedTestSuite/TestCtGcIcmp" classname="github.com/cilium/cilium/pkg/maps/ctmap" time="0.000"></testcase>
		<testcase name="Test/CTMapPrivilegedTestSuite/TestOrphanNatGC" classname="github.com/cilium/cilium/pkg/maps/ctmap" time="0.000"></testcase>
		<testcase name="Test/CTMapTestSuite" classname="github.com/cilium/cilium/pkg/maps/ctmap" time="0.000"></testcase>
		<testcase name="Test/CTMapTestSuite/TestCalculateInterval" classname="github.com/cilium/cilium/pkg/maps/ctmap" time="0.000">
			<system-out><![CDATA[level=info msg="Conntrack garbage collector interval recalculated" deleteRatio=0.4 newInterval=36s subsys=map-ct
level=info msg="Conntrack garbage collector interval recalculated" deleteRatio=0.6 newInterval=24s subsys=map-ct
level=info msg="Conntrack garbage collector interval recalculated" deleteRatio=0.01 newInterval=15s subsys=map-ct
level=info msg="Conntrack garbage collector interval recalculated" deleteRatio=0.04 newInterval=15s subsys=map-ct
level=info msg="Conntrack garbage collector interval recalculated" deleteRatio=0.9 newInterval=10s subsys=map-ct
level=info msg="Conntrack garbage collector interval recalculated" deleteRatio=0.01 newInterval=12h0m0s subsys=map-ct]]></system-out>
		</testcase>
		<testcase name="Test/CTMapTestSuite/TestFilterMapsByProto" classname="github.com/cilium/cilium/pkg/maps/ctmap" time="0.000"></testcase>
		<testcase name="Test/CTMapTestSuite/TestInit" classname="github.com/cilium/cilium/pkg/maps/ctmap" time="0.000"></testcase>
		<testcase name="Test/PerClusterCTMapPrivilegedTestSuite" classname="github.com/cilium/cilium/pkg/maps/ctmap" time="58.310"></testcase>
		<testcase name="Test/PerClusterCTMapPrivilegedTestSuite/TestPerClusterCTMap" classname="github.com/cilium/cilium/pkg/maps/ctmap" time="19.790"></testcase>
		<testcase name="Test/PerClusterCTMapPrivilegedTestSuite/TestPerClusterCTMaps" classname="github.com/cilium/cilium/pkg/maps/ctmap" time="38.520"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/maps/ctmap	coverage: 35.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/egressmap" tests="2" failures="0" errors="0" id="456" hostname="kind-bpf-next" time="0.030" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestCell" classname="github.com/cilium/cilium/pkg/maps/egressmap" time="0.000"></testcase>
		<testcase name="TestPolicyMap" classname="github.com/cilium/cilium/pkg/maps/egressmap" time="0.000">
			<system-out><![CDATA[level=info msg="Detected mounted BPF filesystem at /sys/fs/bpf" subsys=bpf]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/maps/egressmap	coverage: 56.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/lbmap" tests="3" failures="0" errors="0" id="457" hostname="kind-bpf-next" time="0.051" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/maps/lbmap" time="0.020"></testcase>
		<testcase name="Test/MaglevSuite" classname="github.com/cilium/cilium/pkg/maps/lbmap" time="0.020"></testcase>
		<testcase name="Test/MaglevSuite/TestInitMaps" classname="github.com/cilium/cilium/pkg/maps/lbmap" time="0.020">
			<system-out><![CDATA[level=info msg="Unpinning map with incompatible properties" file-path=/sys/fs/bpf/tc/globals/cilium_lb4_services_v2 new="Type:Hash KeySize:12 ValueSize:12 MaxEntries:65536 Flags:1" old="Type:Hash KeySize:12 ValueSize:12 MaxEntries:65536 Flags:0" subsys=bpf
level=info msg="Unpinning map with incompatible properties" file-path=/sys/fs/bpf/tc/globals/cilium_lb4_reverse_nat new="Type:Hash KeySize:2 ValueSize:6 MaxEntries:65536 Flags:1" old="Type:Hash KeySize:2 ValueSize:6 MaxEntries:65536 Flags:0" subsys=bpf]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/maps/lbmap	coverage: 17.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/nat" tests="4" failures="0" errors="0" id="458" hostname="kind-bpf-next" time="35.795" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/maps/nat" time="35.780"></testcase>
		<testcase name="Test/PerClusterNATMapPrivilegedTestSuite" classname="github.com/cilium/cilium/pkg/maps/nat" time="35.780">
			<system-out><![CDATA[level=info msg="Detected mounted BPF filesystem at /sys/fs/bpf" subsys=bpf]]></system-out>
		</testcase>
		<testcase name="Test/PerClusterNATMapPrivilegedTestSuite/TestPerClusterCtMap" classname="github.com/cilium/cilium/pkg/maps/nat" time="11.850"></testcase>
		<testcase name="Test/PerClusterNATMapPrivilegedTestSuite/TestPerClusterNATMaps" classname="github.com/cilium/cilium/pkg/maps/nat" time="23.920"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/maps/nat	coverage: 27.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/policymap" tests="8" failures="0" errors="0" id="459" hostname="kind-bpf-next" time="0.024" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/maps/policymap" time="0.000"></testcase>
		<testcase name="Test/PolicyMapPrivilegedTestSuite" classname="github.com/cilium/cilium/pkg/maps/policymap" time="0.000">
			<system-out><![CDATA[level=info msg="Detected mounted BPF filesystem at /sys/fs/bpf" subsys=bpf]]></system-out>
		</testcase>
		<testcase name="Test/PolicyMapPrivilegedTestSuite/TestDeleteNonexistentKey" classname="github.com/cilium/cilium/pkg/maps/policymap" time="0.000"></testcase>
		<testcase name="Test/PolicyMapPrivilegedTestSuite/TestDenyPolicyMapDumpToSlice" classname="github.com/cilium/cilium/pkg/maps/policymap" time="0.000"></testcase>
		<testcase name="Test/PolicyMapPrivilegedTestSuite/TestPolicyMapDumpToSlice" classname="github.com/cilium/cilium/pkg/maps/policymap" time="0.000"></testcase>
		<testcase name="Test/PolicyMapTestSuite" classname="github.com/cilium/cilium/pkg/maps/policymap" time="0.000"></testcase>
		<testcase name="Test/PolicyMapTestSuite/TestPolicyEntriesDump_Less" classname="github.com/cilium/cilium/pkg/maps/policymap" time="0.000"></testcase>
		<testcase name="Test/PolicyMapTestSuite/TestPolicyMapWildcarding" classname="github.com/cilium/cilium/pkg/maps/policymap" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/maps/policymap	coverage: 34.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/maps/tunnel" tests="3" failures="0" errors="0" id="460" hostname="kind-bpf-next" time="0.018" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/maps/tunnel" time="0.000"></testcase>
		<testcase name="Test/TunnelMapTestSuite" classname="github.com/cilium/cilium/pkg/maps/tunnel" time="0.000"></testcase>
		<testcase name="Test/TunnelMapTestSuite/TestClusterAwareAddressing" classname="github.com/cilium/cilium/pkg/maps/tunnel" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/maps/tunnel	coverage: 42.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/math" tests="4" failures="0" errors="0" id="461" hostname="kind-bpf-next" time="0.004" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/math" time="0.000"></testcase>
		<testcase name="Test/MathSuite" classname="github.com/cilium/cilium/pkg/math" time="0.000"></testcase>
		<testcase name="Test/MathSuite/TestIntMax" classname="github.com/cilium/cilium/pkg/math" time="0.000"></testcase>
		<testcase name="Test/MathSuite/TestIntMin" classname="github.com/cilium/cilium/pkg/math" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/math	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/mcastmanager" tests="4" failures="0" errors="0" id="462" hostname="kind-bpf-next" time="0.007" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/mcastmanager" time="0.000"></testcase>
		<testcase name="Test/McastManagerSuite" classname="github.com/cilium/cilium/pkg/mcastmanager" time="0.000"></testcase>
		<testcase name="Test/McastManagerSuite/TestAddRemoveEndpoint" classname="github.com/cilium/cilium/pkg/mcastmanager" time="0.000">
			<system-out><![CDATA[level=info msg="Joined multicast group" device=lo mcast="ff02::1:ff00:1234" subsys=mcast-manager
level=info msg="Left multicast group" device=lo mcast="ff02::1:ff00:1234" subsys=mcast-manager]]></system-out>
		</testcase>
		<testcase name="Test/McastManagerSuite/TestAddRemoveNil" classname="github.com/cilium/cilium/pkg/mcastmanager" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/mcastmanager	coverage: 87.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/metrics" tests="7" failures="0" errors="0" id="463" hostname="kind-bpf-next" time="0.070" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/metrics" time="0.060"></testcase>
		<testcase name="Test/MetricsSuite" classname="github.com/cilium/cilium/pkg/metrics" time="0.060"></testcase>
		<testcase name="Test/MetricsSuite/TestAPIEventsTSHelperMiddleware" classname="github.com/cilium/cilium/pkg/metrics" time="0.010"></testcase>
		<testcase name="Test/MetricsSuite/TestGaugeWithThreshold" classname="github.com/cilium/cilium/pkg/metrics" time="0.050"></testcase>
		<testcase name="Test/MetricsSuite/Test_getShortPath" classname="github.com/cilium/cilium/pkg/metrics" time="0.000"></testcase>
		<testcase name="Test/StatusCollectorTest" classname="github.com/cilium/cilium/pkg/metrics" time="0.000"></testcase>
		<testcase name="Test/StatusCollectorTest/Test_statusCollector_Collect" classname="github.com/cilium/cilium/pkg/metrics" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/metrics	coverage: 22.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/modules" tests="10" failures="0" errors="0" id="464" hostname="kind-bpf-next" time="0.028" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/modules" time="0.010"></testcase>
		<testcase name="Test/ModulesPrivilegedTestSuite" classname="github.com/cilium/cilium/pkg/modules" time="0.000"></testcase>
		<testcase name="Test/ModulesPrivilegedTestSuite/TestFindOrLoadModules" classname="github.com/cilium/cilium/pkg/modules" time="0.000"></testcase>
		<testcase name="Test/ModulesLinuxTestSuite" classname="github.com/cilium/cilium/pkg/modules" time="0.000"></testcase>
		<testcase name="Test/ModulesLinuxTestSuite/TestListModules" classname="github.com/cilium/cilium/pkg/modules" time="0.000"></testcase>
		<testcase name="Test/ModulesLinuxTestSuite/TestParseBuiltinModuleFile" classname="github.com/cilium/cilium/pkg/modules" time="0.000"></testcase>
		<testcase name="Test/ModulesLinuxTestSuite/TestParseLoadedModuleFile" classname="github.com/cilium/cilium/pkg/modules" time="0.000"></testcase>
		<testcase name="Test/ModulesTestSuite" classname="github.com/cilium/cilium/pkg/modules" time="0.000"></testcase>
		<testcase name="Test/ModulesTestSuite/TestFindModules" classname="github.com/cilium/cilium/pkg/modules" time="0.000"></testcase>
		<testcase name="Test/ModulesTestSuite/TestInit" classname="github.com/cilium/cilium/pkg/modules" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/modules	coverage: 81.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/monitor" tests="5" failures="0" errors="0" id="465" hostname="kind-bpf-next" time="0.045" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/monitor" time="0.000"></testcase>
		<testcase name="Test/MonitorSuite" classname="github.com/cilium/cilium/pkg/monitor" time="0.000"></testcase>
		<testcase name="Test/MonitorSuite/TestConnectionSummary" classname="github.com/cilium/cilium/pkg/monitor" time="0.000">
			<system-out><![CDATA[level=info msg="Initializing dissection cache..." subsys=monitor]]></system-out>
		</testcase>
		<testcase name="Test/MonitorSuite/TestDecodeTraceNotify" classname="github.com/cilium/cilium/pkg/monitor" time="0.000"></testcase>
		<testcase name="Test/MonitorSuite/TestDissectSummary" classname="github.com/cilium/cilium/pkg/monitor" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/monitor	coverage: 13.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/monitor/agent" tests="3" failures="0" errors="0" id="466" hostname="kind-bpf-next" time="0.019" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/monitor/agent" time="0.000"></testcase>
		<testcase name="Test/ListenerSuite" classname="github.com/cilium/cilium/pkg/monitor/agent" time="0.000"></testcase>
		<testcase name="Test/ListenerSuite/TestListenerv1_2" classname="github.com/cilium/cilium/pkg/monitor/agent" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/monitor/agent	coverage: 5.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/monitor/api" tests="7" failures="0" errors="0" id="467" hostname="kind-bpf-next" time="0.019" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/monitor/api" time="0.000"></testcase>
		<testcase name="Test/MonitorAPISuite" classname="github.com/cilium/cilium/pkg/monitor/api" time="0.000"></testcase>
		<testcase name="Test/MonitorAPISuite/TestEmptyPolicyUpdateMessage" classname="github.com/cilium/cilium/pkg/monitor/api" time="0.000"></testcase>
		<testcase name="Test/MonitorAPISuite/TestEndpointRegenMessage" classname="github.com/cilium/cilium/pkg/monitor/api" time="0.000"></testcase>
		<testcase name="Test/MonitorAPISuite/TestPolicyDeleteMessage" classname="github.com/cilium/cilium/pkg/monitor/api" time="0.000"></testcase>
		<testcase name="Test/MonitorAPISuite/TestPolicyUpdateMessage" classname="github.com/cilium/cilium/pkg/monitor/api" time="0.000"></testcase>
		<testcase name="Test/MonitorAPISuite/TestStartMessage" classname="github.com/cilium/cilium/pkg/monitor/api" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/monitor/api	coverage: 19.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/monitor/format" tests="1" failures="0" errors="0" id="468" hostname="kind-bpf-next" time="0.037" timestamp="2023-05-31T11:45:55Z">
		<testcase name="FuzzFormatEvent" classname="github.com/cilium/cilium/pkg/monitor/format" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/monitor/format	coverage: 0.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/monitor/payload" tests="4" failures="0" errors="0" id="469" hostname="kind-bpf-next" time="0.013" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/monitor/payload" time="0.010"></testcase>
		<testcase name="Test/PayloadSuite" classname="github.com/cilium/cilium/pkg/monitor/payload" time="0.010"></testcase>
		<testcase name="Test/PayloadSuite/TestMeta_UnMarshalBinary" classname="github.com/cilium/cilium/pkg/monitor/payload" time="0.010"></testcase>
		<testcase name="Test/PayloadSuite/TestPayload_UnMarshalBinary" classname="github.com/cilium/cilium/pkg/monitor/payload" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/monitor/payload	coverage: 88.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/mountinfo" tests="7" failures="0" errors="0" id="470" hostname="kind-bpf-next" time="0.021" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/mountinfo" time="0.000"></testcase>
		<testcase name="Test/MountInfoPrivilegedTestSuite" classname="github.com/cilium/cilium/pkg/mountinfo" time="0.000"></testcase>
		<testcase name="Test/MountInfoPrivilegedTestSuite/TestIsMountFSbyMount" classname="github.com/cilium/cilium/pkg/mountinfo" time="0.000"></testcase>
		<testcase name="Test/MountInfoTestSuite" classname="github.com/cilium/cilium/pkg/mountinfo" time="0.000"></testcase>
		<testcase name="Test/MountInfoTestSuite/TestGetMountInfo" classname="github.com/cilium/cilium/pkg/mountinfo" time="0.000"></testcase>
		<testcase name="Test/MountInfoTestSuite/TestIsMountFS" classname="github.com/cilium/cilium/pkg/mountinfo" time="0.000"></testcase>
		<testcase name="Test/MountInfoTestSuite/TestParseMountInfoFile" classname="github.com/cilium/cilium/pkg/mountinfo" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/mountinfo	coverage: 75.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/mtu" tests="4" failures="0" errors="0" id="471" hostname="kind-bpf-next" time="0.022" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/mtu" time="0.000"></testcase>
		<testcase name="Test/MTUSuite" classname="github.com/cilium/cilium/pkg/mtu" time="0.000"></testcase>
		<testcase name="Test/MTUSuite/TestAutoDetect" classname="github.com/cilium/cilium/pkg/mtu" time="0.000">
			<system-out><![CDATA[level=info msg="Detected MTU 1500" subsys=mtu]]></system-out>
		</testcase>
		<testcase name="Test/MTUSuite/TestNewConfiguration" classname="github.com/cilium/cilium/pkg/mtu" time="0.000">
			<system-out><![CDATA[level=info msg="Detected MTU 1500" subsys=mtu
level=warning msg="Unable to automatically detect MTU" error="No interface contains the provided ip: 0.0.0.0" subsys=mtu
level=info msg="Inheriting MTU from external network interface" device=lo ipAddr=127.0.0.1 mtu=65536 subsys=mtu]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/mtu	coverage: 68.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/multicast" tests="5" failures="0" errors="0" id="472" hostname="kind-bpf-next" time="0.012" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/multicast" time="0.010"></testcase>
		<testcase name="Test/MulticastSuite" classname="github.com/cilium/cilium/pkg/multicast" time="0.010"></testcase>
		<testcase name="Test/MulticastSuite/TestGroupOps" classname="github.com/cilium/cilium/pkg/multicast" time="0.010"></testcase>
		<testcase name="Test/MulticastSuite/TestMcastKey" classname="github.com/cilium/cilium/pkg/multicast" time="0.000"></testcase>
		<testcase name="Test/MulticastSuite/TestSolicitedNodeMaddr" classname="github.com/cilium/cilium/pkg/multicast" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/multicast	coverage: 79.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/murmur3" tests="6" failures="0" errors="0" id="473" hostname="kind-bpf-next" time="0.010" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestMurmur3" classname="github.com/cilium/cilium/pkg/murmur3" time="0.000"></testcase>
		<testcase name="TestMurmur3/#00" classname="github.com/cilium/cilium/pkg/murmur3" time="0.000"></testcase>
		<testcase name="TestMurmur3/hello_world" classname="github.com/cilium/cilium/pkg/murmur3" time="0.000"></testcase>
		<testcase name="TestMurmur3/lorem_ipsum_dolor_sit_amet" classname="github.com/cilium/cilium/pkg/murmur3" time="0.000"></testcase>
		<testcase name="TestMurmur3/this_is_a_test_of_31_bytes_long" classname="github.com/cilium/cilium/pkg/murmur3" time="0.000"></testcase>
		<testcase name="TestMurmur3/The_quick_brown_fox_jumps_over_the_lazy_dog." classname="github.com/cilium/cilium/pkg/murmur3" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/murmur3	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/node" tests="7" failures="0" errors="0" id="474" hostname="kind-bpf-next" time="0.102" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/node" time="0.070"></testcase>
		<testcase name="Test/NodePrivilegedSuite" classname="github.com/cilium/cilium/pkg/node" time="0.070"></testcase>
		<testcase name="Test/NodePrivilegedSuite/Test_firstGlobalV4Addr" classname="github.com/cilium/cilium/pkg/node" time="0.070"></testcase>
		<testcase name="Test/NodeSuite" classname="github.com/cilium/cilium/pkg/node" time="0.000"></testcase>
		<testcase name="Test/NodeSuite/Test_chooseHostIPsToRestore" classname="github.com/cilium/cilium/pkg/node" time="0.000"></testcase>
		<testcase name="Test/NodeSuite/Test_getCiliumHostIPsFromFile" classname="github.com/cilium/cilium/pkg/node" time="0.000"></testcase>
		<testcase name="TestLocalNodeStore" classname="github.com/cilium/cilium/pkg/node" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="55.413µs" function="node_test.TestLocalNodeStore.func1 (local_node_store_test.go:35)" subsys=hive
level=info msg=Invoked duration="13.745µs" function="node_test.TestLocalNodeStore.func2 (local_node_store_test.go:49)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="12.904µs" function="node.NewLocalNodeStore.func1 (local_node_store.go:89)" subsys=hive
level=info msg="Start hook executed" duration="3.266µs" function="node_test.TestLocalNodeStore.func2.1 (local_node_store_test.go:51)" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="2.565µs" function="node.NewLocalNodeStore.func2 (local_node_store.go:104)" subsys=hive]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/node	coverage: 28.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/node/manager" tests="11" failures="0" errors="0" id="475" hostname="kind-bpf-next" skipped="1" time="1.249" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/node/manager" time="1.210"></testcase>
		<testcase name="Test/managerTestSuite" classname="github.com/cilium/cilium/pkg/node/manager" time="1.210"></testcase>
		<testcase name="Test/managerTestSuite/TestBackgroundSync" classname="github.com/cilium/cilium/pkg/node/manager" time="0.000">
			<skipped message="Skipped"><![CDATA[    manager_test.go:384: GH-6751 Test is disabled due to being unstable]]></skipped>
		</testcase>
		<testcase name="Test/managerTestSuite/TestClusterSizeDependantInterval" classname="github.com/cilium/cilium/pkg/node/manager" time="0.000"></testcase>
		<testcase name="Test/managerTestSuite/TestIpcache" classname="github.com/cilium/cilium/pkg/node/manager" time="0.000"></testcase>
		<testcase name="Test/managerTestSuite/TestIpcacheHealthIP" classname="github.com/cilium/cilium/pkg/node/manager" time="0.000"></testcase>
		<testcase name="Test/managerTestSuite/TestMultipleSources" classname="github.com/cilium/cilium/pkg/node/manager" time="0.200"></testcase>
		<testcase name="Test/managerTestSuite/TestNode" classname="github.com/cilium/cilium/pkg/node/manager" time="1.000"></testcase>
		<testcase name="Test/managerTestSuite/TestNodeEncryption" classname="github.com/cilium/cilium/pkg/node/manager" time="0.000"></testcase>
		<testcase name="Test/managerTestSuite/TestNodeLifecycle" classname="github.com/cilium/cilium/pkg/node/manager" time="0.000"></testcase>
		<testcase name="Test/managerTestSuite/TestRemoteNodeIdentities" classname="github.com/cilium/cilium/pkg/node/manager" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/node/manager	coverage: 66.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/node/types" tests="7" failures="0" errors="0" id="476" hostname="kind-bpf-next" time="0.039" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/node/types" time="0.000"></testcase>
		<testcase name="Test/NodeSuite" classname="github.com/cilium/cilium/pkg/node/types" time="0.000"></testcase>
		<testcase name="Test/NodeSuite/TestGetIPByType" classname="github.com/cilium/cilium/pkg/node/types" time="0.000"></testcase>
		<testcase name="Test/NodeSuite/TestGetNodeIP" classname="github.com/cilium/cilium/pkg/node/types" time="0.000"></testcase>
		<testcase name="Test/NodeSuite/TestHostname" classname="github.com/cilium/cilium/pkg/node/types" time="0.000"></testcase>
		<testcase name="Test/NodeSuite/TestNode_ToCiliumNode" classname="github.com/cilium/cilium/pkg/node/types" time="0.000"></testcase>
		<testcase name="Test/NodeSuite/TestParseCiliumNode" classname="github.com/cilium/cilium/pkg/node/types" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/node/types	coverage: 25.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/option" tests="94" failures="0" errors="0" id="477" hostname="kind-bpf-next" time="0.042" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestGetEnvName" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestGetEnvName/Normal_option" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestGetEnvName/Capital_option" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestGetEnvName/with_numbers" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestGetEnvName/mix_numbers_small_letters" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestGetEnvName/mix_numbers_small_letters_and_dashes" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestGetEnvName/normal_option" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/default_map_sizes" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/arbitrary_map_sizes_within_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/Auth_map_size_below_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/Auth_map_size_above_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/CT_TCP_map_size_below_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/CT_TCP_map_size_above_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/CT_Any_map_size_below_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/CT_Any_map_size_above_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/NAT_map_size_below_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/NAT_map_size_above_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/NAT_map_auto_sizing_with_default_size" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/NAT_map_auto_sizing_outside_of_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/Policy_map_size_below_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/Policy_map_size_above_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/Fragments_map_size_below_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckMapSizeLimits/Fragments_map_size_above_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPv4NativeRoutingCIDR" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPv4NativeRoutingCIDR/with_native_routing_cidr" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPv4NativeRoutingCIDR/without_native_routing_cidr_and_no_masquerade" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPv4NativeRoutingCIDR/without_native_routing_cidr_and_tunnel_enabled" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPv4NativeRoutingCIDR/without_native_routing_cidr_and_tunnel_disabled" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPv4NativeRoutingCIDR/without_native_routing_cidr_and_with_masquerade_and_tunnel_disabled_and_ipam_not_eni" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPv6NativeRoutingCIDR" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPv6NativeRoutingCIDR/with_native_routing_cidr" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPv6NativeRoutingCIDR/without_native_routing_cidr_and_no_masquerade" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPv6NativeRoutingCIDR/without_native_routing_cidr_and_tunnel_enabled" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPv6NativeRoutingCIDR/without_native_routing_cidr_and_tunnel_disabled" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPAMDelegatedPlugin" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPAMDelegatedPlugin/IPAMDelegatedPlugin_with_local_router_IPv4_set_and_endpoint_health_checking_disabled" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPAMDelegatedPlugin/IPAMDelegatedPlugin_with_local_router_IPv6_set_and_endpoint_health_checking_disabled" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPAMDelegatedPlugin/IPAMDelegatedPlugin_with_health_checking_enabled" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPAMDelegatedPlugin/IPAMDelegatedPlugin_without_local_router_IPv4" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestCheckIPAMDelegatedPlugin/IPAMDelegatedPlugin_without_local_router_IPv6" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test_populateNodePortRange" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test_populateNodePortRange/NodePortRange_is_valid" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test_populateNodePortRange/NodePortRange_not_set_in_viper" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test_populateNodePortRange/NodePortMin_greater_than_NodePortMax" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test_populateNodePortRange/NodePortMin_equal_NodePortMax" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test_populateNodePortRange/NodePortMin_not_a_number" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test_populateNodePortRange/NodePortMax_not_a_number" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test_populateNodePortRange/NodePortRange_slice_length_not_equal_2" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test_populateNodePortRange/NodePortRange_passed_as_empty" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[level=warning msg="NodePort range was set but is empty." subsys=config]]></system-out>
		</testcase>
		<testcase name="TestBPFMapSizeCalculation" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestBPFMapSizeCalculation/static_default_sizes" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestBPFMapSizeCalculation/static,_non-default_sizes_inside_range" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="TestBPFMapSizeCalculation/dynamic_size_without_any_static_sizes_(512MB,_0.25%)" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[level=info msg="Memory available for map entries (0.003% of 536870912B): 1342177B" subsys=config
level=info msg="option bpf-ct-global-tcp-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 65536" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 65536" subsys=config]]></system-out>
		</testcase>
		<testcase name="TestBPFMapSizeCalculation/dynamic_size_without_any_static_sizes_(1GiB,_0.25%)" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[level=info msg="Memory available for map entries (0.003% of 1073741824B): 2684354B" subsys=config
level=info msg="option bpf-ct-global-tcp-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 65536" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 65536" subsys=config]]></system-out>
		</testcase>
		<testcase name="TestBPFMapSizeCalculation/dynamic_size_without_any_static_sizes_(2GiB,_0.25%)" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[level=info msg="Memory available for map entries (0.003% of 2147483648B): 5368709B" subsys=config
level=info msg="option bpf-ct-global-tcp-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 65536" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 65536" subsys=config]]></system-out>
		</testcase>
		<testcase name="TestBPFMapSizeCalculation/dynamic_size_without_any_static_sizes_(7.5GiB,_0.25%)" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[level=info msg="Memory available for map entries (0.003% of 8053063680B): 20132659B" subsys=config
level=info msg="option bpf-ct-global-tcp-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 65536" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 65536" subsys=config]]></system-out>
		</testcase>
		<testcase name="TestBPFMapSizeCalculation/dynamic_size_without_any_static_sizes_(16GiB,_0.25%)" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[level=info msg="Memory available for map entries (0.003% of 17179869184B): 42949672B" subsys=config
level=info msg="option bpf-ct-global-tcp-max set by dynamic sizing to 151765" subsys=config
level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 75882" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 151765" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 151765" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 75882" subsys=config]]></system-out>
		</testcase>
		<testcase name="TestBPFMapSizeCalculation/dynamic_size_without_any_static_sizes_(120GiB,_0.25%)" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[level=info msg="Memory available for map entries (0.003% of 32212254720B): 80530636B" subsys=config
level=info msg="option bpf-ct-global-tcp-max set by dynamic sizing to 284560" subsys=config
level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 142280" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 284560" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 284560" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 142280" subsys=config]]></system-out>
		</testcase>
		<testcase name="TestBPFMapSizeCalculation/dynamic_size_without_any_static_sizes_(240GiB,_0.25%)" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[level=info msg="Memory available for map entries (0.003% of 257698037760B): 644245094B" subsys=config
level=info msg="option bpf-ct-global-tcp-max set by dynamic sizing to 2276484" subsys=config
level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 1138242" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 2276484" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 2276484" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 1138242" subsys=config]]></system-out>
		</testcase>
		<testcase name="TestBPFMapSizeCalculation/dynamic_size_without_any_static_sizes_(360GiB,_0.25%)" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[level=info msg="Memory available for map entries (0.003% of 386547056640B): 966367641B" subsys=config
level=info msg="option bpf-ct-global-tcp-max set by dynamic sizing to 3414726" subsys=config
level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 1707363" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 3414726" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 3414726" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 1707363" subsys=config]]></system-out>
		</testcase>
		<testcase name="TestBPFMapSizeCalculation/dynamic_size_with_static_CT_TCP_size_(4GiB,_0.25%)" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[level=info msg="Memory available for map entries (0.003% of 4294967296B): 10737418B" subsys=config
level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 65536" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 65536" subsys=config]]></system-out>
		</testcase>
		<testcase name="TestBPFMapSizeCalculation/huge_dynamic_size_ratio_gets_clamped_(8GiB,_98%)" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[level=info msg="Memory available for map entries (0.980% of 17179869184B): 16836271800B" subsys=config
level=info msg="option bpf-ct-global-tcp-max set by dynamic sizing to 16777216" subsys=config
level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 16777216" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 16777216" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 16777216" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 16777216" subsys=config]]></system-out>
		</testcase>
		<testcase name="TestBPFMapSizeCalculation/dynamic_size_NAT_size_above_limit_with_static_CT_sizes_(issue_#13843)" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[level=info msg="Memory available for map entries (0.003% of 137438953472B): 343597383B" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 1214125" subsys=config
level=warning msg="option bpf-nat-global-max would exceed maximum determined by CT table sizes, capping to 524288" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 524288" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 607062" subsys=config]]></system-out>
		</testcase>
		<testcase name="Test_parseEventBufferTupleString" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/option" time="0.010"></testcase>
		<testcase name="Test/OptionSuite" classname="github.com/cilium/cilium/pkg/option" time="0.010"></testcase>
		<testcase name="Test/OptionSuite/TestApplyValidated" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestBindEnv" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestDelete" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestEnabledFunctions" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestEndpointStatusIsEnabled" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestEndpointStatusValues" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestGetDefaultMonitorQueueSize" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestGetFmtOpt" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestGetFmtOpts" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestGetImmutableModel" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestGetMutableModel" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestGetValue" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestInheritDefault" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestIsEnabled" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestLocalAddressExclusion" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestParseKeyValue" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestParseKeyValueWithDefaultParseFunc" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestParseMonitorAggregationLevel" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestParseOption" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestReadDirConfig" classname="github.com/cilium/cilium/pkg/option" time="0.000">
			<system-out><![CDATA[/tmp/TestOptionSuiteTestReadDirConfig193104135/002/test]]></system-out>
		</testcase>
		<testcase name="Test/OptionSuite/TestSetBool" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestSetIfUnset" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestSetValidated" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestValidate" classname="github.com/cilium/cilium/pkg/option" time="0.010"></testcase>
		<testcase name="Test/OptionSuite/TestValidateIPv6ClusterAllocCIDR" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/TestVerifyMonitorAggregationLevel" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<testcase name="Test/OptionSuite/Test_backupFiles" classname="github.com/cilium/cilium/pkg/option" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/option	coverage: 30.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/option/resolver" tests="7" failures="0" errors="0" id="478" hostname="kind-bpf-next" time="0.028" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestWriteConfigurations" classname="github.com/cilium/cilium/pkg/option/resolver" time="0.000"></testcase>
		<testcase name="TestResolveConfigurations" classname="github.com/cilium/cilium/pkg/option/resolver" time="0.000">
			<system-out><![CDATA[level=info msg="Reading configuration from config-map:test-ns/cm" configSource="config-map:test-ns/cm" subsys=option-resolver
level=info msg="Got 1 config pairs from source" configSource="config-map:test-ns/cm" subsys=option-resolver
level=info msg="Reading configuration from cilium-node-config:test-ns/" configSource="cilium-node-config:test-ns/" subsys=option-resolver
level=info msg="Got 1 config pairs from source" configSource="cilium-node-config:test-ns/" subsys=option-resolver
level=info msg="Reading configuration from node:/nodename" configSource="node:/nodename" subsys=option-resolver
level=info msg="Got 1 config pairs from source" configSource="node:/nodename" subsys=option-resolver
level=info msg="Reading configuration from cilium-node-config:test-ns/specific" configSource="cilium-node-config:test-ns/specific" subsys=option-resolver
level=info msg="Got 1 config pairs from source" configSource="cilium-node-config:test-ns/specific" subsys=option-resolver]]></system-out>
		</testcase>
		<testcase name="TestWithBlockedFields" classname="github.com/cilium/cilium/pkg/option/resolver" time="0.000">
			<system-out><![CDATA[level=info msg="Reading configuration from config-map:test-ns/cm" configSource="config-map:test-ns/cm" subsys=option-resolver
level=info msg="Got 1 config pairs from source" configSource="config-map:test-ns/cm" subsys=option-resolver
level=info msg="Reading configuration from cilium-node-config:test-ns/" configSource="cilium-node-config:test-ns/" subsys=option-resolver
level=info msg="Got 2 config pairs from source" configSource="cilium-node-config:test-ns/" subsys=option-resolver
level=warning msg="Source has non-overridable key" configKey=blocked-key configSource="cilium-node-config:test-ns/" subsys=option-resolver
level=info msg="Reading configuration from config-map:test-ns/cm" configSource="config-map:test-ns/cm" subsys=option-resolver
level=info msg="Got 1 config pairs from source" configSource="config-map:test-ns/cm" subsys=option-resolver
level=info msg="Reading configuration from cilium-node-config:test-ns/" configSource="cilium-node-config:test-ns/" subsys=option-resolver
level=info msg="Got 2 config pairs from source" configSource="cilium-node-config:test-ns/" subsys=option-resolver
level=warning msg="Source has non-overridable key" configKey=blocked-key configSource="cilium-node-config:test-ns/" subsys=option-resolver]]></system-out>
		</testcase>
		<testcase name="TestReadNodeConfigs" classname="github.com/cilium/cilium/pkg/option/resolver" time="0.000"></testcase>
		<testcase name="TestReadNodeConfigs/one-matching" classname="github.com/cilium/cilium/pkg/option/resolver" time="0.000"></testcase>
		<testcase name="TestReadNodeConfigs/none-matching" classname="github.com/cilium/cilium/pkg/option/resolver" time="0.000"></testcase>
		<testcase name="TestReadNodeConfigs/two-matching" classname="github.com/cilium/cilium/pkg/option/resolver" time="0.000">
			<system-out><![CDATA[level=info msg="Key key-1 set in multiple CiliumNodeConfigs" configKey=key-1 subsys=option-resolver]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/option/resolver	coverage: 76.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/pidfile" tests="7" failures="0" errors="0" id="479" hostname="kind-bpf-next" time="0.010" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/pidfile" time="0.010"></testcase>
		<testcase name="Test/PidfileTestSuite" classname="github.com/cilium/cilium/pkg/pidfile" time="0.010"></testcase>
		<testcase name="Test/PidfileTestSuite/TestKill" classname="github.com/cilium/cilium/pkg/pidfile" time="0.000"></testcase>
		<testcase name="Test/PidfileTestSuite/TestKillAlreadyFinished" classname="github.com/cilium/cilium/pkg/pidfile" time="0.000"></testcase>
		<testcase name="Test/PidfileTestSuite/TestKillFailedParsePid" classname="github.com/cilium/cilium/pkg/pidfile" time="0.000"></testcase>
		<testcase name="Test/PidfileTestSuite/TestKillPidfileNotExist" classname="github.com/cilium/cilium/pkg/pidfile" time="0.000"></testcase>
		<testcase name="Test/PidfileTestSuite/TestWrite" classname="github.com/cilium/cilium/pkg/pidfile" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/pidfile	coverage: 72.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/policy" tests="441" failures="0" errors="0" id="480" hostname="kind-bpf-next" time="0.701" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test_MergeL3" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeL3/permutation_0" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeL3/permutation_1" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules" classname="github.com/cilium/cilium/pkg/policy" time="0.250"></testcase>
		<testcase name="Test_MergeRules/permutation_0" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_1" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_2" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_3" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_4" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_5" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_6" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_7" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_8" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_9" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_10" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_11" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_12" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_13" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_14" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_15" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_16" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_17" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_18" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_19" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_20" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_21" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_22" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_23" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_24" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_25" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_26" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_27" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_28" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_29" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_30" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_31" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_32" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_33" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_34" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_35" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_36" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_37" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_38" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_39" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_40" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_41" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_42" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_43" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_44" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_45" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_46" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_47" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_48" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_49" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_50" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_51" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_52" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_53" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_54" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_55" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_56" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_57" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_58" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_59" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_60" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_61" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_62" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_63" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_64" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_65" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_66" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_67" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_68" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_69" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_70" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_71" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_72" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_73" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_74" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_75" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_76" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_77" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_78" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_79" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_80" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_81" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_82" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_83" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_84" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_85" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_86" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_87" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_88" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_89" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_90" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_91" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_92" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_93" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_94" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_95" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_96" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_97" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_98" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_99" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_100" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_101" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_102" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_103" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_104" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_105" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_106" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_107" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_108" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_109" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_110" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_111" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_112" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_113" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_114" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_115" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_116" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_117" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_118" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_119" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_120" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_121" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_122" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_123" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_124" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_125" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_126" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_127" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_128" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_129" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_130" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_131" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_132" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_133" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_134" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_135" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_136" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_137" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_138" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_139" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_140" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_141" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_142" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_143" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_144" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_145" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_146" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_147" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_148" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_149" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_150" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_151" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_152" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_153" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_154" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_155" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_156" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_157" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_158" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_159" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_160" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_161" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_162" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_163" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_164" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_165" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_166" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_167" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_168" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_169" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_170" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_171" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_172" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_173" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_174" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_175" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_176" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_177" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_178" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_179" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_180" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_181" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_182" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_183" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_184" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_185" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_186" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_187" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_188" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_189" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_190" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_191" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_192" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_193" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_194" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_195" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_196" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_197" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_198" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_199" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_200" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_201" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_202" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_203" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_204" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_205" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_206" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_207" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_208" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_209" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_210" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_211" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_212" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_213" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_214" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_215" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_216" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_217" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_218" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_219" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_220" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_221" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_222" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_223" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_224" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_225" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_226" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_227" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_228" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_229" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_230" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_231" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_232" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_233" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_234" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_235" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_236" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_237" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_238" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_239" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_240" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_241" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_242" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_243" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_244" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_245" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_246" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_247" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_248" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_249" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_250" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_251" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_252" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_253" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_254" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRules/permutation_255" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts" classname="github.com/cilium/cilium/pkg/policy" time="0.010"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_0" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_1" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_2" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_3" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_4" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_5" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_6" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_7" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_8" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_9" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_10" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_11" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_12" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_13" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_14" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_15" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_16" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_17" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_18" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_19" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_20" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_21" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_22" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_23" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_24" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_25" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_26" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_27" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_28" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_29" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_30" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_MergeRulesWithNamedPorts/permutation_31" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_AllowAll" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_AllowAll/permutation_0" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_AllowAll/permutation_1" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_EnsureDeniesPrecedeAllows" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_EnsureDeniesPrecedeAllows/deny_world_no_labels" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_EnsureDeniesPrecedeAllows/deny_world_with_labels" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_EnsureDeniesPrecedeAllows/deny_one_ip_with_a_larger_subnet" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_EnsureDeniesPrecedeAllows/deny_part_of_a_subnet_with_an_ip" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_EnsureDeniesPrecedeAllows/broad_deny_is_a_portproto_subset_of_a_specific_allow" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test_EnsureDeniesPrecedeAllows/broad_allow_is_a_portproto_subset_of_a_specific_deny" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/policy" time="0.390"></testcase>
		<testcase name="Test/DistilleryTestSuite" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/DistilleryTestSuite/TestCacheManagement" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/DistilleryTestSuite/TestCachePopulation" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite" classname="github.com/cilium/cilium/pkg/policy" time="0.380"></testcase>
		<testcase name="Test/PolicyTestSuite/TestAddSearchDelete" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestAllowingLocalhostShadowsL7" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestAllowsEgress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestAllowsIngress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestComputePolicyDenyEnforcementAndRules" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestComputePolicyEnforcementAndRules" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestCreateL4Filter" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestCreateL4FilterAuthRequired" classname="github.com/cilium/cilium/pkg/policy" time="0.010"></testcase>
		<testcase name="Test/PolicyTestSuite/TestCreateL4FilterMissingSecret" classname="github.com/cilium/cilium/pkg/policy" time="0.000">
			<system-out><![CDATA[level=warning msg="policy: Error getting terminating TLS Context." error="Unknown test secret 'notExisting'" subsys=policy
level=warning msg="policy: Error getting terminating TLS Context." error="Unknown test secret 'notExisting'" subsys=policy
level=warning msg="policy: Error getting terminating TLS Context." error="Unknown test secret 'notExisting'" subsys=policy
level=warning msg="policy: Error getting terminating TLS Context." error="Unknown test secret 'notExisting'" subsys=policy]]></system-out>
		</testcase>
		<testcase name="Test/PolicyTestSuite/TestDeniesEgress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestDeniesIngress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestEgressAllowAll" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestEgressL3AllowAllEntity" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestEgressL3AllowWorld" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestEgressL4AllowAll" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestEgressL4AllowAllEntity" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestEgressL4AllowWorld" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestEgressRuleRestrictions" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestEntitiesL3" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestForEachGo" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestGenerateL7RulesByParser" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestGetCIDRPrefixes" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestGetRulesMatching" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestICMPPolicy" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestIngressAllowAll" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestIngressAllowAllL4Overlap" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestIngressAllowAllL4OverlapNamedPort" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestIngressL4AllowAll" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestIngressL4AllowAllNamedPort" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestIterate" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestJSONMarshal" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestJoinPath" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3AllowRuleShadowedByL3DenyAll" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3DenyRuleShadowedByL3DenyAll" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3DependentL4EgressDenyFromRequires" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3DependentL4EgressFromRequires" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3DependentL4IngressDenyFromRequires" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3DependentL4IngressFromRequires" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3L4AllowRuleWithByL3DenyAll" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3L4L7Merge" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3Policy" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3RuleLabels" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3RuleShadowedByL3AllowAll" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3RuleWithL7RulePartiallyShadowedByL3AllowAll" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3RuleWithL7RuleShadowedByL3AllowAll" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3SelectingEndpointAndL3AllowAllMergeConflictingL7" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3WithIngressDenyWildcard" classname="github.com/cilium/cilium/pkg/policy" time="0.030"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL3WithLocalHostWildcardd" classname="github.com/cilium/cilium/pkg/policy" time="0.010"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL4Policy" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL4RuleLabels" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL4WildcardMerge" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL7WithIngressWildcard" classname="github.com/cilium/cilium/pkg/policy" time="0.020"></testcase>
		<testcase name="Test/PolicyTestSuite/TestL7WithLocalHostWildcardd" classname="github.com/cilium/cilium/pkg/policy" time="0.010"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMapStateWithIngress" classname="github.com/cilium/cilium/pkg/policy" time="0.110">
			<system-out><![CDATA[level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=193 subsys=policy]]></system-out>
		</testcase>
		<testcase name="Test/PolicyTestSuite/TestMapStateWithIngressDeny" classname="github.com/cilium/cilium/pkg/policy" time="0.010">
			<system-out><![CDATA[level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=193 subsys=policy]]></system-out>
		</testcase>
		<testcase name="Test/PolicyTestSuite/TestMapStateWithIngressDenyWildcard" classname="github.com/cilium/cilium/pkg/policy" time="0.010"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMapStateWithIngressWildcard" classname="github.com/cilium/cilium/pkg/policy" time="0.110"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMapState_AccumulateMapChanges" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMapState_AccumulateMapChangesDeny" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMapState_AccumulateMapChangesOnVisibilityKeys" classname="github.com/cilium/cilium/pkg/policy" time="0.000">
			<system-out><![CDATA[level=info msg="AddVisibilityKeys: Adding L4-only ALLOW key for visibilty redirect" bpfMapKey="Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=12345" subsys=policy
level=info msg="AddVisibilityKeys: Extending L3-only DENY key to L3/L4 key to deny a port with visibility annotation" bpfMapKey="Identity=234,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=0" subsys=policy
level=info msg="AddVisibilityKeys: Extending L3-only ALLOW key to L3/L4 key for visibilty redirect" bpfMapKey="Identity=236,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=12345" subsys=policy
level=info msg="AddVisibilityKeys: Extending L3-only ALLOW key to L3/L4 key for visibilty redirect" bpfMapKey="Identity=235,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=12345" subsys=policy
level=info msg="AddVisibilityKeys: Extending L3-only ALLOW key to L3/L4 key for visibilty redirect" bpfMapKey="Identity=237,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=12345" subsys=policy
level=info msg="AddVisibilityKeys: Adding L4-only ALLOW key for visibilty redirect" bpfMapKey="Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=12345" subsys=policy
level=info msg="AddVisibilityKeys: Changing L4-only ALLOW key for visibility redirect" bpfMapKey="Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=1" bpfMapValue="ProxyPort=12346" subsys=policy
level=info msg="AddVisibilityKeys: Changing L3/L4 ALLOW key for visibility redirect" bpfMapKey="Identity=42,DestPort=53,Nexthdr=17,TrafficDirection=1" bpfMapValue="ProxyPort=12347" subsys=policy]]></system-out>
		</testcase>
		<testcase name="Test/PolicyTestSuite/TestMapState_AddVisibilityKeys" classname="github.com/cilium/cilium/pkg/policy" time="0.000">
			<system-out><![CDATA[level=info msg="AddVisibilityKeys: Adding L4-only ALLOW key for visibilty redirect" bpfMapKey="Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=12345" subsys=policy
Adds: map[Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0:{}]
wantAdds: map[Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0:{}]
Adds: map[]
wantAdds: map[]
level=info msg="AddVisibilityKeys: Changing L4-only ALLOW key for visibility redirect" bpfMapKey="Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=12345" subsys=policy
Adds: map[Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0:{}]
wantAdds: map[Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0:{}]
level=info msg="AddVisibilityKeys: Changing L3/L4 ALLOW key for visibility redirect" bpfMapKey="Identity=123,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=12345" subsys=policy
Adds: map[Identity=123,DestPort=80,Nexthdr=6,TrafficDirection=0:{}]
wantAdds: map[Identity=123,DestPort=80,Nexthdr=6,TrafficDirection=0:{}]
level=info msg="AddVisibilityKeys: Extending L3-only ALLOW key to L3/L4 key for visibilty redirect" bpfMapKey="Identity=1,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=12345" subsys=policy
Adds: map[Identity=1,DestPort=0,Nexthdr=0,TrafficDirection=0:{} Identity=1,DestPort=80,Nexthdr=6,TrafficDirection=0:{}]
wantAdds: map[Identity=1,DestPort=0,Nexthdr=0,TrafficDirection=0:{} Identity=1,DestPort=80,Nexthdr=6,TrafficDirection=0:{}]
Adds: map[]
wantAdds: map[]
Adds: map[]
wantAdds: map[]
level=info msg="AddVisibilityKeys: Adding L4-only ALLOW key for visibilty redirect" bpfMapKey="Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=12345" subsys=policy
level=info msg="AddVisibilityKeys: Extending L3-only DENY key to L3/L4 key to deny a port with visibility annotation" bpfMapKey="Identity=234,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=0" subsys=policy
Adds: map[Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0:{} Identity=234,DestPort=0,Nexthdr=0,TrafficDirection=0:{} Identity=234,DestPort=80,Nexthdr=6,TrafficDirection=0:{}]
wantAdds: map[Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0:{} Identity=234,DestPort=0,Nexthdr=0,TrafficDirection=0:{} Identity=234,DestPort=80,Nexthdr=6,TrafficDirection=0:{}]
level=info msg="AddVisibilityKeys: Adding L4-only ALLOW key for visibilty redirect" bpfMapKey="Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0" bpfMapValue="ProxyPort=12345" subsys=policy
Adds: map[Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0:{}]
wantAdds: map[Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=0:{}]
level=info msg="AddVisibilityKeys: Adding L4-only ALLOW key for visibilty redirect" bpfMapKey="Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=1" bpfMapValue="ProxyPort=12346" subsys=policy
Adds: map[Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=1:{}]
wantAdds: map[Identity=0,DestPort=80,Nexthdr=6,TrafficDirection=1:{}]]]></system-out>
		</testcase>
		<testcase name="Test/PolicyTestSuite/TestMapState_DenyPreferredInsert" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMapState_DenyPreferredInsertWithSubnets" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMatches" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeAllowAllL3AndAllowAllL7" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeAllowAllL3AndShadowedL7" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeDenyAllL3" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeIdenticalAllowAllL3AndMismatchingParsers" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeIdenticalAllowAllL3AndRestrictedL7HTTP" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeIdenticalAllowAllL3AndRestrictedL7Kafka" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeL4PolicyEgress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeL4PolicyIngress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeL7PolicyEgress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeL7PolicyEgressWithMultipleSelectors" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeL7PolicyIngress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeListenerPolicy" classname="github.com/cilium/cilium/pkg/policy" time="0.000">
			<system-out><![CDATA[level=info msg="res: map[443/TCP:{\"port\":443,\"protocol\":\"TCP\",\"l7-rules\":[{\"\\u0026LabelSelector{MatchLabels:map[string]string{any.id: a,},MatchExpressions:[]LabelSelectorRequirement{},}\":null},{\"\\u0026LabelSelector{MatchLabels:map[string]string{any.id: c,},MatchExpressions:[]LabelSelectorRequirement{},}\":{}}],\"listener\":\"shared-cec/test\"}]" subsys=policy
level=info msg="res: map[443/TCP:{\"port\":443,\"protocol\":\"TCP\",\"l7-rules\":[{\"\\u0026LabelSelector{MatchLabels:map[string]string{any.id: a,},MatchExpressions:[]LabelSelectorRequirement{},}\":null},{\"\\u0026LabelSelector{MatchLabels:map[string]string{any.id: c,},MatchExpressions:[]LabelSelectorRequirement{},}\":{}}],\"listener\":\"default/test-cec/test\"}]" subsys=policy
level=info msg="res: map[443/TCP:{\"port\":443,\"protocol\":\"TCP\",\"l7-rules\":[{\"\\u0026LabelSelector{MatchLabels:map[string]string{any.id: a,},MatchExpressions:[]LabelSelectorRequirement{},}\":null},{\"\\u0026LabelSelector{MatchLabels:map[string]string{any.id: c,},MatchExpressions:[]LabelSelectorRequirement{},}\":{}}],\"listener\":\"shared-cec/test\"}]" subsys=policy]]></system-out>
		</testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeTLSHTTPPolicy" classname="github.com/cilium/cilium/pkg/policy" time="0.000">
			<system-out><![CDATA[level=info msg="res: map[443/TCP:{\"port\":443,\"protocol\":\"TCP\",\"l7-rules\":[{\"\\u0026LabelSelector{MatchLabels:map[string]string{any.id: a,},MatchExpressions:[]LabelSelectorRequirement{},}\":null},{\"\\u0026LabelSelector{MatchLabels:map[string]string{any.id: c,},MatchExpressions:[]LabelSelectorRequirement{},}\":{\"terminatingTLS\":{\"certificateChain\":\"[redacted]\",\"privateKey\":\"[redacted]\"},\"originatingTLS\":{\"trustedCA\":\"[redacted]\"},\"http\":[{}]}}]}]" subsys=policy]]></system-out>
		</testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeTLSSNIPolicy" classname="github.com/cilium/cilium/pkg/policy" time="0.000">
			<system-out><![CDATA[level=info msg="res: map[443/TCP:{\"port\":443,\"protocol\":\"TCP\",\"l7-rules\":[{\"\\u0026LabelSelector{MatchLabels:map[string]string{any.id: a,},MatchExpressions:[]LabelSelectorRequirement{},}\":null},{\"\\u0026LabelSelector{MatchLabels:map[string]string{any.id: c,},MatchExpressions:[]LabelSelectorRequirement{},}\":{\"terminatingTLS\":{\"certificateChain\":\"[redacted]\",\"privateKey\":\"[redacted]\"},\"originatingTLS\":{\"trustedCA\":\"[redacted]\"},\"serverNames\":{\"www.bar.com\":{},\"www.foo.com\":{}},\"http\":[{}]}}]}]" subsys=policy]]></system-out>
		</testcase>
		<testcase name="Test/PolicyTestSuite/TestMergeTLSTCPPolicy" classname="github.com/cilium/cilium/pkg/policy" time="0.000">
			<system-out><![CDATA[level=info msg="res: map[443/TCP:{\"port\":443,\"protocol\":\"TCP\",\"l7-rules\":[{\"\\u0026LabelSelector{MatchLabels:map[string]string{any.id: a,},MatchExpressions:[]LabelSelectorRequirement{},}\":null},{\"\\u0026LabelSelector{MatchLabels:map[string]string{any.id: c,},MatchExpressions:[]LabelSelectorRequirement{},}\":{\"terminatingTLS\":{\"certificateChain\":\"[redacted]\",\"privateKey\":\"[redacted]\"},\"originatingTLS\":{\"trustedCA\":\"[redacted]\"}}}]}]" subsys=policy]]></system-out>
		</testcase>
		<testcase name="Test/PolicyTestSuite/TestMergingWithDifferentEndpointSelectedAllowAllL7" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergingWithDifferentEndpointSelectedDenyAllL7" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMergingWithDifferentEndpointsSelectedAllowSameL7" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMinikubeGettingStarted" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestMinikubeGettingStartedDeny" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestNewEndpointSet" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestParserTypeMerge" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestPolicyDenyTrace" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestPolicyEntityValidationEgress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestPolicyEntityValidationEntitySelectorsFill" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestPolicyEntityValidationIngress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestPolicyKeyTrafficDirection" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestPolicyTrace" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestProxyID" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestRedirectType" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestRemoveIdentityFromRuleDenyCaches" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestRuleWithNoEndpointSelector" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestSearchContextString" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestVisibilityPolicyCreation" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardCIDRRulesEgress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardCIDRRulesEgressDeny" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardL3RulesEgress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardL3RulesEgressDeny" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardL3RulesEgressDenyToEntities" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardL3RulesEgressToEntities" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardL3RulesIngress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardL3RulesIngressDeny" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardL3RulesIngressDenyFromEntities" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardL3RulesIngressFromEntities" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardL4RulesEgress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardL4RulesEgressDeny" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardL4RulesIngress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestWildcardL4RulesIngressDeny" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestgetPrefixesFromCIDR" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/PolicyTestSuite/TestremoveIdentityFromRuleCaches" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/SelectorCacheTestSuite" classname="github.com/cilium/cilium/pkg/policy" time="0.010"></testcase>
		<testcase name="Test/SelectorCacheTestSuite/TestAddRemoveSelector" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/SelectorCacheTestSuite/TestFQDNSelectorUpdates" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/SelectorCacheTestSuite/TestIdentityUpdates" classname="github.com/cilium/cilium/pkg/policy" time="0.010"></testcase>
		<testcase name="Test/SelectorCacheTestSuite/TestIdentityUpdatesMultipleUsers" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/SelectorCacheTestSuite/TestMultipleIdentitySelectors" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="Test/SelectorCacheTestSuite/TestRemoveIdentitiesFQDNSelectors" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="TestEndpointPolicy_AllowsIdentity" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="TestEndpointPolicy_AllowsIdentity/policy_disabled" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="TestEndpointPolicy_AllowsIdentity/policy_enabled" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="TestEndpointPolicy_AllowsIdentity/policy_enabled_for_ingress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="TestEndpointPolicy_AllowsIdentity/policy_enabled_for_egress" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="TestEndpointPolicy_AllowsIdentity/policy_enabled_for_ingress_with_deny_policy" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="TestEndpointPolicy_AllowsIdentity/policy_disabled_for_ingress_with_deny_policy" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="TestEndpointPolicy_AllowsIdentity/policy_enabled_for_egress_with_deny_policy" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="TestEndpointPolicy_AllowsIdentity/policy_disabled_for_egress_with_deny_policy" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="FuzzResolveEgressPolicy" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="FuzzDenyPreferredInsert" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<testcase name="FuzzAccumulateMapChange" classname="github.com/cilium/cilium/pkg/policy" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/policy	coverage: 82.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/policy/api" tests="97" failures="0" errors="0" id="481" hostname="kind-bpf-next" time="0.062" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/policy/api" time="0.030"></testcase>
		<testcase name="Test/PolicyAPITestSuite" classname="github.com/cilium/cilium/pkg/policy/api" time="0.010"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestCIDRMatchesAll" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestCIDRRegex" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestCIDRsanitize" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestCreateDerivative" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestCreateDerivativeRuleWithToGroupsAndToPorts" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestCreateDerivativeRuleWithToGroupsWitInvalidRegisterCallback" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestCreateDerivativeRuleWithoutToGroups" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestCreateDerivativeWithoutErrorAndNoIPs" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestEntityHostAllowsRemoteNode" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestEntityMatches" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestEntitySliceMatches" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestFQDNSelectorSanitize" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestGetAsEndpointSelectors" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestGetCIDRSetWithError" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestGetCIDRSetWithMultipleSorted" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestGetCIDRSetWithUniqueCIDRRule" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestGetCIDRSetWithValidValue" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestHTTPEqual" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestHTTPRuleRegexes" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestICMPRuleWithOtherRuleFailed" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestInvalidEndpointSelectors" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000">
			<system-out><![CDATA[level=error msg="unable to construct selector in label selector" EndpointLabelSelector="&LabelSelector{MatchLabels:map[string]string{k8s.any.foo: bar,k8s.k8s.baz: alice,},MatchExpressions:[]LabelSelectorRequirement{LabelSelectorRequirement{Key:k8s.any.foo,Operator:asdfasdfasdf,Values:[default],},},}" error="\"asdfasdfasdf\" is not a valid pod selector operator" subsys=policy-api]]></system-out>
		</testcase>
		<testcase name="Test/PolicyAPITestSuite/TestIsLabelBasedEgress" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestIsLabelBasedIngress" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestJSONMarshalling" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestKafkaEqual" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestL7Equal" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestL7RuleDirectionalitySupport" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestL7RuleRejectsEmptyPort" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestL7Rules" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestL7RulesWithNodeSelector" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestL7RulesWithNonTCPProtocols" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestLabelSelectorToRequirements" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestNodeSelector" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000">
			<system-out><![CDATA[level=error msg="unable to construct selector in label selector" EndpointLabelSelector="&LabelSelector{MatchLabels:map[string]string{k8s.any.foo: bar,k8s.k8s.baz: alice,},MatchExpressions:[]LabelSelectorRequirement{LabelSelectorRequirement{Key:k8s.any.foo,Operator:asdfasdfasdf,Values:[default],},},}" error="\"asdfasdfasdf\" is not a valid pod selector operator" subsys=policy-api]]></system-out>
		</testcase>
		<testcase name="Test/PolicyAPITestSuite/TestParseL4Proto" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestPortRuleDNSSanitize" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestRequiresDerivative" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestRequiresDerivativeRuleWithToGroups" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestRequiresDerivativeRuleWithoutToGroups" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestResourceQualifiedName" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestRulesDeepEqual" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestSelectsAllEndpoints" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestToServicesSanitize" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestTooManyICMPFields" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestTooManyPortsRule" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestValidateL4Proto" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestWithoutProviderRegister" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite/TestWrongICMPFieldFamily" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01" classname="github.com/cilium/cilium/pkg/policy/api" time="0.010"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestCIDRMatchesAll" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestCIDRRegex" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestCIDRsanitize" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestCreateDerivative" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestCreateDerivativeRuleWithToGroupsAndToPorts" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestCreateDerivativeRuleWithToGroupsWitInvalidRegisterCallback" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestCreateDerivativeRuleWithoutToGroups" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestCreateDerivativeWithoutErrorAndNoIPs" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestEntityHostAllowsRemoteNode" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestEntityMatches" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestEntitySliceMatches" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestFQDNSelectorSanitize" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestGetAsEndpointSelectors" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestGetCIDRSetWithError" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestGetCIDRSetWithMultipleSorted" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestGetCIDRSetWithUniqueCIDRRule" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestGetCIDRSetWithValidValue" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestHTTPEqual" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestHTTPRuleRegexes" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestICMPRuleWithOtherRuleFailed" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestInvalidEndpointSelectors" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000">
			<system-out><![CDATA[level=error msg="unable to construct selector in label selector" EndpointLabelSelector="&LabelSelector{MatchLabels:map[string]string{k8s.any.foo: bar,k8s.k8s.baz: alice,},MatchExpressions:[]LabelSelectorRequirement{LabelSelectorRequirement{Key:k8s.any.foo,Operator:asdfasdfasdf,Values:[default],},},}" error="\"asdfasdfasdf\" is not a valid pod selector operator" subsys=policy-api]]></system-out>
		</testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestIsLabelBasedEgress" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestIsLabelBasedIngress" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestJSONMarshalling" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestKafkaEqual" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestL7Equal" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestL7RuleDirectionalitySupport" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestL7RuleRejectsEmptyPort" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestL7Rules" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestL7RulesWithNodeSelector" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestL7RulesWithNonTCPProtocols" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestLabelSelectorToRequirements" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestNodeSelector" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000">
			<system-out><![CDATA[level=error msg="unable to construct selector in label selector" EndpointLabelSelector="&LabelSelector{MatchLabels:map[string]string{k8s.any.foo: bar,k8s.k8s.baz: alice,},MatchExpressions:[]LabelSelectorRequirement{LabelSelectorRequirement{Key:k8s.any.foo,Operator:asdfasdfasdf,Values:[default],},},}" error="\"asdfasdfasdf\" is not a valid pod selector operator" subsys=policy-api]]></system-out>
		</testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestParseL4Proto" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestPortRuleDNSSanitize" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestRequiresDerivative" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestRequiresDerivativeRuleWithToGroups" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestRequiresDerivativeRuleWithoutToGroups" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestResourceQualifiedName" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestRulesDeepEqual" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestSelectsAllEndpoints" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestToServicesSanitize" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestTooManyICMPFields" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestTooManyPortsRule" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestValidateL4Proto" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestWithoutProviderRegister" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<testcase name="Test/PolicyAPITestSuite#01/TestWrongICMPFieldFamily" classname="github.com/cilium/cilium/pkg/policy/api" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/policy/api	coverage: 36.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/policy/groups" tests="6" failures="0" errors="0" id="482" hostname="kind-bpf-next" time="0.031" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/policy/groups" time="0.000"></testcase>
		<testcase name="Test/GroupsTestSuite" classname="github.com/cilium/cilium/pkg/policy/groups" time="0.000"></testcase>
		<testcase name="Test/GroupsTestSuite/TestCacheWorkingCorrectly" classname="github.com/cilium/cilium/pkg/policy/groups" time="0.000"></testcase>
		<testcase name="Test/GroupsTestSuite/TestCorrectDerivativeName" classname="github.com/cilium/cilium/pkg/policy/groups" time="0.000"></testcase>
		<testcase name="Test/GroupsTestSuite/TestDerivativePoliciesAreDeletedIfNoToGroups" classname="github.com/cilium/cilium/pkg/policy/groups" time="0.000"></testcase>
		<testcase name="Test/GroupsTestSuite/TestDerivativePoliciesAreInheritCorrectly" classname="github.com/cilium/cilium/pkg/policy/groups" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/policy/groups	coverage: 19.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/pprof" tests="2" failures="0" errors="0" id="483" hostname="kind-bpf-next" time="0.011" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestPprofDisabled" classname="github.com/cilium/cilium/pkg/pprof" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="56.194µs" function="pprof.TestPprofDisabled.func1 (cell_test.go:31)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg=Stopping subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestPprofHandlers" classname="github.com/cilium/cilium/pkg/pprof" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="51.526µs" function="pprof.TestPprofHandlers.func1 (cell_test.go:61)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Started pprof server" ip=127.0.0.1 port=36215 subsys=hive
level=info msg="Start hook executed" duration="484.282µs" function="*pprof.server.Start" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stopped pprof server" ip=127.0.0.1 port=36215 subsys=hive
level=info msg="Stop hook executed" duration="180.025µs" function="*pprof.server.Stop" subsys=hive]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/pprof	coverage: 81.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/promise" tests="3" failures="0" errors="0" id="484" hostname="kind-bpf-next" time="0.004" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestPromiseResolve" classname="github.com/cilium/cilium/pkg/promise" time="0.000"></testcase>
		<testcase name="TestPromiseReject" classname="github.com/cilium/cilium/pkg/promise" time="0.000"></testcase>
		<testcase name="TestPromiseCancelled" classname="github.com/cilium/cilium/pkg/promise" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/promise	coverage: 94.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/proxy" tests="3" failures="0" errors="0" id="485" hostname="kind-bpf-next" time="0.050" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/proxy" time="0.010"></testcase>
		<testcase name="Test/ProxySuite" classname="github.com/cilium/cilium/pkg/proxy" time="0.010"></testcase>
		<testcase name="Test/ProxySuite/TestPortAllocator" classname="github.com/cilium/cilium/pkg/proxy" time="0.010">
			<system-out><![CDATA[level=info msg="Envoy: Starting xDS gRPC server listening on /tmp/TestProxySuiteTestPortAllocator2479975115/001/envoy/sockets/xds.sock" subsys=envoy-manager
level=info msg="Adding new proxy port rules for listener1:16317" id=listener1 subsys=proxy
level=info msg="Adding new proxy port rules for listener1:16158" id=listener1 subsys=proxy]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/proxy	coverage: 23.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/rand" tests="5" failures="0" errors="0" id="486" hostname="kind-bpf-next" time="0.006" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/rand" time="0.000"></testcase>
		<testcase name="Test/RandSuite" classname="github.com/cilium/cilium/pkg/rand" time="0.000"></testcase>
		<testcase name="Test/RandSuite/TestRandomString" classname="github.com/cilium/cilium/pkg/rand" time="0.000"></testcase>
		<testcase name="TestIntn" classname="github.com/cilium/cilium/pkg/rand" time="0.000"></testcase>
		<testcase name="TestShuffle" classname="github.com/cilium/cilium/pkg/rand" time="0.000"></testcase>
		<system-out><![CDATA[testing: warning: no tests to run
	github.com/cilium/cilium/pkg/proxy/logger	coverage: 29.7% of statements
ok  	github.com/cilium/cilium/pkg/proxy/logger	0.029s	coverage: 29.7% of statements [no tests to run]
	github.com/cilium/cilium/pkg/rand	coverage: 37.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/rate" tests="29" failures="0" errors="0" id="487" hostname="kind-bpf-next" time="3.351" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/rate" time="3.330"></testcase>
		<testcase name="Test/ControllerSuite" classname="github.com/cilium/cilium/pkg/rate" time="3.330"></testcase>
		<testcase name="Test/ControllerSuite/TestAPILimiterMergeUserConfig" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestAPILimiterMetrics" classname="github.com/cilium/cilium/pkg/rate" time="0.110">
			<system-out><![CDATA[level=info msg="Processing API request with rate limiter" maxWaitDuration=200ms name=foo parallelRequests=2 subsys=rate uuid=92ae2071-0847-4612-9059-5ea27f50fe36
level=info msg="API request released by rate limiter" burst=2 limit=10.00/s maxWaitDuration=200ms maxWaitDurationLimiter=199.913458ms name=foo parallelRequests=2 subsys=rate uuid=92ae2071-0847-4612-9059-5ea27f50fe36 waitDurationLimiter=0s waitDurationTotal="95.679µs"
level=info msg="API call has been processed" name=foo processingDuration=5.220585ms subsys=rate totalDuration=5.347892ms uuid=92ae2071-0847-4612-9059-5ea27f50fe36 waitDurationTotal="95.679µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=200ms name=foo parallelRequests=2 subsys=rate uuid=f6872852-487c-4ed3-9c77-4fb61a3ccf01
level=info msg="API request released by rate limiter" burst=2 limit=10.00/s maxWaitDuration=200ms maxWaitDurationLimiter=199.97841ms name=foo parallelRequests=2 subsys=rate uuid=f6872852-487c-4ed3-9c77-4fb61a3ccf01 waitDurationLimiter=0s waitDurationTotal="27.812µs"
level=info msg="API call has been processed" name=foo processingDuration=5.710973ms subsys=rate totalDuration=5.770815ms uuid=f6872852-487c-4ed3-9c77-4fb61a3ccf01 waitDurationTotal="27.812µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=200ms name=foo parallelRequests=2 subsys=rate uuid=801d3c35-04d9-40f3-bbfa-efa025de9b0a
level=info msg="API request released by rate limiter" burst=2 limit=10.00/s maxWaitDuration=200ms maxWaitDurationLimiter=199.968272ms name=foo parallelRequests=2 subsys=rate uuid=801d3c35-04d9-40f3-bbfa-efa025de9b0a waitDurationLimiter=88.814969ms waitDurationTotal=89.282684ms
level=info msg="API call has been processed" error=error name=foo processingDuration=5.240868ms subsys=rate totalDuration=94.609602ms uuid=801d3c35-04d9-40f3-bbfa-efa025de9b0a waitDurationTotal=89.282684ms]]></system-out>
		</testcase>
		<testcase name="Test/ControllerSuite/TestAdjustedBurst" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestAdjustedLimit" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestAdjustedParallelRequests" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestAdjustmentLimit" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestAutoAdjust" classname="github.com/cilium/cilium/pkg/rate" time="0.270"></testcase>
		<testcase name="Test/ControllerSuite/TestCalcMeanDuration" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestCalculateAdjustmentFactor" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestCancelContext" classname="github.com/cilium/cilium/pkg/rate" time="0.000">
			<system-out><![CDATA[level=warning msg="Not processing API request due to cancelled context" name=foo parallelRequests=0 subsys=rate uuid=a4bf2a7f-ef4c-4ca7-8f08-555eb45f6596
level=info msg="API call has been processed" error="request cancelled while waiting for rate limiting slot: context canceled" name=foo processingDuration=0s subsys=rate totalDuration="71.534µs" uuid=a4bf2a7f-ef4c-4ca7-8f08-555eb45f6596 waitDurationTotal=0s]]></system-out>
		</testcase>
		<testcase name="Test/ControllerSuite/TestDelayedAdjustment" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestLimitCancelContext" classname="github.com/cilium/cilium/pkg/rate" time="0.010">
			<system-out><![CDATA[level=info msg="Processing API request with rate limiter" minWaitDuration=1m0s name=foo parallelRequests=0 subsys=rate uuid=f08e7da3-f5d3-4fb2-b850-78b18b5a30da
level=warning msg="Not processing API request due to cancelled context while waiting" burst=1 limit=0.02/s maxWaitDurationLimiter="-44.453µs" minWaitDuration=1m0s name=foo parallelRequests=0 subsys=rate uuid=f08e7da3-f5d3-4fb2-b850-78b18b5a30da waitDurationLimiter=0s
level=info msg="API call has been processed" error="request cancelled while waiting for rate limiting slot: context canceled" name=foo processingDuration=0s subsys=rate totalDuration=10.248626ms uuid=f08e7da3-f5d3-4fb2-b850-78b18b5a30da waitDurationTotal="44.453µs"]]></system-out>
		</testcase>
		<testcase name="Test/ControllerSuite/TestLimitWaitDurationExceeded" classname="github.com/cilium/cilium/pkg/rate" time="0.020">
			<system-out><![CDATA[level=info msg="Processing API request with rate limiter" maxWaitDuration=1ms name=foo parallelRequests=0 subsys=rate uuid=a1c3311c-5f46-4c0b-9408-32d6b48bc92c
level=info msg="API request released by rate limiter" burst=2 limit=0.02/s maxWaitDuration=1ms maxWaitDurationLimiter="973.21µs" name=foo parallelRequests=0 subsys=rate uuid=a1c3311c-5f46-4c0b-9408-32d6b48bc92c waitDurationLimiter=0s waitDurationTotal="32.701µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=1ms name=foo parallelRequests=0 subsys=rate uuid=b49e086f-4ab7-41c9-bc7f-5d7ba9f327b1
level=info msg="Processing API request with rate limiter" maxWaitDuration=1ms name=foo parallelRequests=0 subsys=rate uuid=efacb27c-1295-406d-81f9-bb2842602780
level=info msg="API request released by rate limiter" burst=2 limit=0.02/s maxWaitDuration=1ms maxWaitDurationLimiter="934.358µs" name=foo parallelRequests=0 subsys=rate uuid=efacb27c-1295-406d-81f9-bb2842602780 waitDurationLimiter=0s waitDurationTotal="76.222µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=1ms name=foo parallelRequests=0 subsys=rate uuid=4f0ba640-3780-45f8-9a21-866cf5e92fc7
level=warning msg="Not processing API request. Wait duration exceeds maximum" burst=2 limit=0.02/s maxWaitDuration=1ms maxWaitDurationLimiter="978.971µs" name=foo parallelRequests=0 subsys=rate uuid=4f0ba640-3780-45f8-9a21-866cf5e92fc7 waitDurationLimiter=59.99987681s
level=info msg="API call has been processed" error="request would have to wait 59.99987681s to be served (maximum wait duration: 978.971µs)" name=foo processingDuration=0s subsys=rate totalDuration="42.139µs" uuid=4f0ba640-3780-45f8-9a21-866cf5e92fc7 waitDurationTotal="21.029µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=1ms name=foo parallelRequests=0 subsys=rate uuid=f849707f-eb94-4a88-a6c1-3b2fa8e73754
level=warning msg="Not processing API request. Wait duration exceeds maximum" burst=2 limit=0.02/s maxWaitDuration=1ms maxWaitDurationLimiter="981.115µs" name=foo parallelRequests=0 subsys=rate uuid=f849707f-eb94-4a88-a6c1-3b2fa8e73754 waitDurationLimiter=59.99982702s
level=info msg="Processing API request with rate limiter" maxWaitDuration=1ms name=foo parallelRequests=0 subsys=rate uuid=9937ade0-e8e2-4c09-a20d-b762d8d6c66f
level=info msg="Processing API request with rate limiter" maxWaitDuration=1ms name=foo parallelRequests=0 subsys=rate uuid=050b6dc9-b2ed-43c9-9614-a9119ab4958f
level=warning msg="Not processing API request. Wait duration exceeds maximum" burst=2 limit=0.02/s maxWaitDuration=1ms maxWaitDurationLimiter="984.611µs" name=foo parallelRequests=0 subsys=rate uuid=b49e086f-4ab7-41c9-bc7f-5d7ba9f327b1 waitDurationLimiter=59.999796673s
level=info msg="API call has been processed" error="request would have to wait 59.999796673s to be served (maximum wait duration: 984.611µs)" name=foo processingDuration=0s subsys=rate totalDuration="195.823µs" uuid=b49e086f-4ab7-41c9-bc7f-5d7ba9f327b1 waitDurationTotal="15.389µs"
level=info msg="API call has been processed" error="request would have to wait 59.99982702s to be served (maximum wait duration: 981.115µs)" name=foo processingDuration=0s subsys=rate totalDuration="43.211µs" uuid=f849707f-eb94-4a88-a6c1-3b2fa8e73754 waitDurationTotal="18.885µs"
level=warning msg="Not processing API request. Wait duration exceeds maximum" burst=2 limit=0.02/s maxWaitDuration=1ms maxWaitDurationLimiter="914.278µs" name=foo parallelRequests=0 subsys=rate uuid=050b6dc9-b2ed-43c9-9614-a9119ab4958f waitDurationLimiter=1m59.999789047s
level=warning msg="Not processing API request. Wait duration exceeds maximum" burst=2 limit=0.02/s maxWaitDuration=1ms maxWaitDurationLimiter="986.074µs" name=foo parallelRequests=0 subsys=rate uuid=9937ade0-e8e2-4c09-a20d-b762d8d6c66f waitDurationLimiter=2m59.999686927s
level=info msg="API call has been processed" error="request would have to wait 2m59.999686927s to be served (maximum wait duration: 986.074µs)" name=foo processingDuration=0s subsys=rate totalDuration="149.198µs" uuid=9937ade0-e8e2-4c09-a20d-b762d8d6c66f waitDurationTotal="13.926µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=1ms name=foo parallelRequests=0 subsys=rate uuid=782baad6-8252-4088-a5bb-b632fd5b4577
level=warning msg="Not processing API request. Wait duration exceeds maximum" burst=2 limit=0.02/s maxWaitDuration=1ms maxWaitDurationLimiter="633.978µs" name=foo parallelRequests=0 subsys=rate uuid=782baad6-8252-4088-a5bb-b632fd5b4577 waitDurationLimiter=2m59.99958714s
level=info msg="API call has been processed" error="request would have to wait 2m59.99958714s to be served (maximum wait duration: 633.978µs)" name=foo processingDuration=0s subsys=rate totalDuration="416.265µs" uuid=782baad6-8252-4088-a5bb-b632fd5b4577 waitDurationTotal="366.022µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=1ms name=foo parallelRequests=0 subsys=rate uuid=d596ee48-7592-4cb0-af80-f4f7f18a4dd0
level=warning msg="Not processing API request. Wait duration exceeds maximum" burst=2 limit=0.02/s maxWaitDuration=1ms maxWaitDurationLimiter="542.687µs" name=foo parallelRequests=0 subsys=rate uuid=d596ee48-7592-4cb0-af80-f4f7f18a4dd0 waitDurationLimiter=2m59.999461066s
level=info msg="API call has been processed" error="request would have to wait 2m59.999461066s to be served (maximum wait duration: 542.687µs)" name=foo processingDuration=0s subsys=rate totalDuration="474.044µs" uuid=d596ee48-7592-4cb0-af80-f4f7f18a4dd0 waitDurationTotal="457.313µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=1ms name=foo parallelRequests=0 subsys=rate uuid=d11f3bf4-cdad-43b1-8287-45866291dc05
level=warning msg="Not processing API request. Wait duration exceeds maximum" burst=2 limit=0.02/s maxWaitDuration=1ms maxWaitDurationLimiter="547.164µs" name=foo parallelRequests=0 subsys=rate uuid=d11f3bf4-cdad-43b1-8287-45866291dc05 waitDurationLimiter=2m59.999405552s
level=info msg="API call has been processed" error="request would have to wait 1m59.999789047s to be served (maximum wait duration: 914.278µs)" name=foo processingDuration=0s subsys=rate totalDuration="200.322µs" uuid=050b6dc9-b2ed-43c9-9614-a9119ab4958f waitDurationTotal="85.722µs"
level=info msg="API call has been processed" error="request would have to wait 2m59.999405552s to be served (maximum wait duration: 547.164µs)" name=foo processingDuration=0s subsys=rate totalDuration="470.689µs" uuid=d11f3bf4-cdad-43b1-8287-45866291dc05 waitDurationTotal="452.836µs"
level=info msg="API call has been processed" name=foo processingDuration=10.684148ms subsys=rate totalDuration=10.784484ms uuid=efacb27c-1295-406d-81f9-bb2842602780 waitDurationTotal="76.222µs"
level=info msg="API call has been processed" name=foo processingDuration=10.810904ms subsys=rate totalDuration=10.858884ms uuid=a1c3311c-5f46-4c0b-9408-32d6b48bc92c waitDurationTotal="32.701µs"]]></system-out>
		</testcase>
		<testcase name="Test/ControllerSuite/TestLimiter" classname="github.com/cilium/cilium/pkg/rate" time="1.100"></testcase>
		<testcase name="Test/ControllerSuite/TestMaxParallelRequests" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestMaxWaitDurationExceeded" classname="github.com/cilium/cilium/pkg/rate" time="0.020">
			<system-out><![CDATA[level=info msg="Processing API request with rate limiter" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=de3e73fe-2490-4ab0-bcee-e5f7fc407809
level=info msg="API request released by rate limiter" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=de3e73fe-2490-4ab0-bcee-e5f7fc407809 waitDurationTotal="43.641µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=8a5ddec7-c4e4-49fe-b076-8ca88219c8c4
level=info msg="Processing API request with rate limiter" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=cfb7f96b-a26c-4e99-a9e0-234c0f969a9f
level=info msg="Processing API request with rate limiter" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=6869a2d0-e79f-4b2d-9f00-381b6d8e8562
level=info msg="Processing API request with rate limiter" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=d88bf17a-d861-4ef6-9a1f-291e253be4de
level=info msg="API request released by rate limiter" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=8a5ddec7-c4e4-49fe-b076-8ca88219c8c4 waitDurationTotal="37.094µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=3926bf4b-8dd5-42c3-b313-c161a565a518
level=info msg="Processing API request with rate limiter" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=90c3f428-9b19-4b01-9835-596069c0f18e
level=info msg="Processing API request with rate limiter" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=51817c3a-cc2e-4281-a29e-b9b6033b2837
level=info msg="Processing API request with rate limiter" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=baf7867d-86a6-418e-bb15-ddcaf277d034
level=info msg="Processing API request with rate limiter" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=99c7ad00-2640-4cdb-9cbd-8d0d3d34eaef
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=99c7ad00-2640-4cdb-9cbd-8d0d3d34eaef
level=info msg="API call has been processed" error="timed out while waiting to be served with 2 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=10.197886ms uuid=99c7ad00-2640-4cdb-9cbd-8d0d3d34eaef waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=51817c3a-cc2e-4281-a29e-b9b6033b2837
level=info msg="API call has been processed" name=foo processingDuration=10.306418ms subsys=rate totalDuration=10.36629ms uuid=de3e73fe-2490-4ab0-bcee-e5f7fc407809 waitDurationTotal="43.641µs"
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=90c3f428-9b19-4b01-9835-596069c0f18e
level=warning msg="Not processing API request. Wait duration exceeds maximum" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=cfb7f96b-a26c-4e99-a9e0-234c0f969a9f
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=3926bf4b-8dd5-42c3-b313-c161a565a518
level=info msg="API call has been processed" error="timed out while waiting to be served with 2 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=10.27275ms uuid=90c3f428-9b19-4b01-9835-596069c0f18e waitDurationTotal=0s
level=info msg="API call has been processed" error="timed out while waiting to be served with 2 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=10.235993ms uuid=51817c3a-cc2e-4281-a29e-b9b6033b2837 waitDurationTotal=0s
level=info msg="API call has been processed" error="request would have to wait 0s to be served (maximum wait duration: -321.672µs)" name=foo processingDuration=0s subsys=rate totalDuration=10.338183ms uuid=cfb7f96b-a26c-4e99-a9e0-234c0f969a9f waitDurationTotal=10.321672ms
level=info msg="API call has been processed" error="timed out while waiting to be served with 2 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=10.305653ms uuid=3926bf4b-8dd5-42c3-b313-c161a565a518 waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=d88bf17a-d861-4ef6-9a1f-291e253be4de
level=info msg="API call has been processed" error="timed out while waiting to be served with 2 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=10.367166ms uuid=d88bf17a-d861-4ef6-9a1f-291e253be4de waitDurationTotal=0s
level=info msg="API call has been processed" name=foo processingDuration=10.278937ms subsys=rate totalDuration=10.363565ms uuid=8a5ddec7-c4e4-49fe-b076-8ca88219c8c4 waitDurationTotal="37.094µs"
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=6869a2d0-e79f-4b2d-9f00-381b6d8e8562
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=10ms name=foo parallelRequests=2 subsys=rate uuid=baf7867d-86a6-418e-bb15-ddcaf277d034
level=info msg="API call has been processed" error="timed out while waiting to be served with 2 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=10.452891ms uuid=baf7867d-86a6-418e-bb15-ddcaf277d034 waitDurationTotal=0s
level=info msg="API call has been processed" error="timed out while waiting to be served with 2 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=10.417926ms uuid=6869a2d0-e79f-4b2d-9f00-381b6d8e8562 waitDurationTotal=0s]]></system-out>
		</testcase>
		<testcase name="Test/ControllerSuite/TestMeanProcessingDuration" classname="github.com/cilium/cilium/pkg/rate" time="0.010"></testcase>
		<testcase name="Test/ControllerSuite/TestMinParallelRequests" classname="github.com/cilium/cilium/pkg/rate" time="0.020">
			<system-out><![CDATA[level=info msg="Processing API request with rate limiter" name=foo parallelRequests=10 subsys=rate uuid=247cd4b0-7c29-440f-baac-fc7424b60d84
level=info msg="API request released by rate limiter" name=foo parallelRequests=10 subsys=rate uuid=247cd4b0-7c29-440f-baac-fc7424b60d84 waitDurationTotal="101.88µs"
level=info msg="Processing API request with rate limiter" name=foo parallelRequests=10 subsys=rate uuid=dceb15ff-7ed3-49de-9765-e290f834ce2c
level=info msg="API request released by rate limiter" name=foo parallelRequests=10 subsys=rate uuid=dceb15ff-7ed3-49de-9765-e290f834ce2c waitDurationTotal="12.604µs"
level=info msg="Processing API request with rate limiter" name=foo parallelRequests=10 subsys=rate uuid=1b0b15ba-f301-47e9-a094-53ba9d4e955e
level=info msg="API request released by rate limiter" name=foo parallelRequests=10 subsys=rate uuid=1b0b15ba-f301-47e9-a094-53ba9d4e955e waitDurationTotal="7.955µs"
level=info msg="Processing API request with rate limiter" name=foo parallelRequests=10 subsys=rate uuid=8aac6c1d-1e30-47ba-9eb8-578e8f4a76de
level=info msg="API request released by rate limiter" name=foo parallelRequests=10 subsys=rate uuid=8aac6c1d-1e30-47ba-9eb8-578e8f4a76de waitDurationTotal="7.624µs"
level=info msg="Processing API request with rate limiter" name=foo parallelRequests=10 subsys=rate uuid=2c2198da-110e-4660-ab82-95a2359c85f6
level=info msg="API request released by rate limiter" name=foo parallelRequests=10 subsys=rate uuid=2c2198da-110e-4660-ab82-95a2359c85f6 waitDurationTotal="7.253µs"
level=info msg="Processing API request with rate limiter" name=foo parallelRequests=10 subsys=rate uuid=11ed6116-dc30-443d-83af-a76180a4a2fd
level=info msg="API request released by rate limiter" name=foo parallelRequests=10 subsys=rate uuid=11ed6116-dc30-443d-83af-a76180a4a2fd waitDurationTotal="7.343µs"
level=info msg="Processing API request with rate limiter" name=foo parallelRequests=10 subsys=rate uuid=523e0503-05af-4df4-8a33-03017f941722
level=info msg="API request released by rate limiter" name=foo parallelRequests=10 subsys=rate uuid=523e0503-05af-4df4-8a33-03017f941722 waitDurationTotal="8.005µs"
level=info msg="Processing API request with rate limiter" name=foo parallelRequests=10 subsys=rate uuid=46a34bc6-b288-4ac0-9a54-c82c9d94aec8
level=info msg="API request released by rate limiter" name=foo parallelRequests=10 subsys=rate uuid=46a34bc6-b288-4ac0-9a54-c82c9d94aec8 waitDurationTotal="7.104µs"
level=info msg="Processing API request with rate limiter" name=foo parallelRequests=10 subsys=rate uuid=cfb12d41-6f95-4012-9353-739ad30ea773
level=info msg="API request released by rate limiter" name=foo parallelRequests=10 subsys=rate uuid=cfb12d41-6f95-4012-9353-739ad30ea773 waitDurationTotal="7.764µs"
level=info msg="Processing API request with rate limiter" name=foo parallelRequests=10 subsys=rate uuid=d74c406e-69f4-4008-ab5b-4b4af2e04a0d
level=info msg="API request released by rate limiter" name=foo parallelRequests=10 subsys=rate uuid=d74c406e-69f4-4008-ab5b-4b4af2e04a0d waitDurationTotal="7.213µs"
level=info msg="API call has been processed" name=foo processingDuration=10.221154ms subsys=rate totalDuration=10.236543ms uuid=cfb12d41-6f95-4012-9353-739ad30ea773 waitDurationTotal="7.764µs"
level=info msg="API call has been processed" name=foo processingDuration=10.274825ms subsys=rate totalDuration=10.291736ms uuid=d74c406e-69f4-4008-ab5b-4b4af2e04a0d waitDurationTotal="7.213µs"
level=info msg="API call has been processed" name=foo processingDuration=10.458307ms subsys=rate totalDuration=10.574503ms uuid=247cd4b0-7c29-440f-baac-fc7424b60d84 waitDurationTotal="101.88µs"
level=info msg="API call has been processed" name=foo processingDuration=10.447156ms subsys=rate totalDuration=10.467985ms uuid=dceb15ff-7ed3-49de-9765-e290f834ce2c waitDurationTotal="12.604µs"
level=info msg="API call has been processed" name=foo processingDuration=10.445843ms subsys=rate totalDuration=10.461342ms uuid=1b0b15ba-f301-47e9-a094-53ba9d4e955e waitDurationTotal="7.955µs"
level=info msg="API call has been processed" name=foo processingDuration=10.445683ms subsys=rate totalDuration=10.461061ms uuid=8aac6c1d-1e30-47ba-9eb8-578e8f4a76de waitDurationTotal="7.624µs"
level=info msg="API call has been processed" name=foo processingDuration=10.442827ms subsys=rate totalDuration=10.458968ms uuid=2c2198da-110e-4660-ab82-95a2359c85f6 waitDurationTotal="7.253µs"
level=info msg="API call has been processed" name=foo processingDuration=10.441906ms subsys=rate totalDuration=10.456713ms uuid=11ed6116-dc30-443d-83af-a76180a4a2fd waitDurationTotal="7.343µs"
level=info msg="API call has been processed" name=foo processingDuration=10.439511ms subsys=rate totalDuration=10.45498ms uuid=523e0503-05af-4df4-8a33-03017f941722 waitDurationTotal="8.005µs"
level=info msg="API call has been processed" name=foo processingDuration=10.43895ms subsys=rate totalDuration=10.454259ms uuid=46a34bc6-b288-4ac0-9a54-c82c9d94aec8 waitDurationTotal="7.104µs"]]></system-out>
		</testcase>
		<testcase name="Test/ControllerSuite/TestNewAPILimiter" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestNewAPILimiterFromConfig" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestNewAPILimiterSet" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestParseRate" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestParseUserConfig" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestParseUserConfigKeyValue" classname="github.com/cilium/cilium/pkg/rate" time="0.000"></testcase>
		<testcase name="Test/ControllerSuite/TestReservationCancel" classname="github.com/cilium/cilium/pkg/rate" time="0.510">
			<system-out><![CDATA[level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=de0acb8a-9ad3-4664-be1b-7d01e4220081
level=info msg="API request released by rate limiter" burst=10 limit=50.00/s maxWaitDuration=500ms maxWaitDurationLimiter=499.907768ms name=foo parallelRequests=1 subsys=rate uuid=de0acb8a-9ad3-4664-be1b-7d01e4220081 waitDurationLimiter=0s waitDurationTotal="101.099µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=45a6544c-d6b1-46f6-8f88-1f20ee98702c
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=5fcdf7f8-3343-41a4-9ff1-1b6c239b9aab
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=32fda6f8-1143-4457-901b-8d06bf657229
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=4405e8ec-3fd1-483d-af9d-a905e7aa03c6
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=cefb79ff-cfdf-4e8e-8107-95861cc22870
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=13911d16-53e1-4389-b641-c39326312919
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=f7fe9c73-af20-4ac9-ad11-518991a80af5
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=8e3863ff-6ff5-450f-8fc9-5f3d4d4320ba
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=d6171b50-b758-45ce-9217-e4cc35497fd5
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=f2709577-075c-40f3-9bec-b29ff3d089cd
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=dba49514-e933-4a89-a919-a97880fa9848
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=9ace6717-6cd4-4936-8e94-725d753b9712
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=1cf01d08-4865-4e58-82a3-2e3a22c3adfe
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=78893422-eb27-4d4c-bb6e-a52a1e5abf08
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=0b13b63b-7ba4-4881-a6f6-3f6d5219f14a
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=8d31eee9-6d47-4bf6-940d-27a0af7ed64e
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=1f733b18-7421-4c17-a1d1-2f9fe4b14a20
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=db8b9404-e4fa-4ac3-9c7e-236295eed37f
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=0b60a957-e210-4f9c-a2b4-049e9cdca4c7
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=00fc8dc0-b659-451f-9d9a-8491bb6f4697
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=00fc8dc0-b659-451f-9d9a-8491bb6f4697
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.13963ms uuid=00fc8dc0-b659-451f-9d9a-8491bb6f4697 waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=45a6544c-d6b1-46f6-8f88-1f20ee98702c
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.551838ms uuid=45a6544c-d6b1-46f6-8f88-1f20ee98702c waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=5fcdf7f8-3343-41a4-9ff1-1b6c239b9aab
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.556518ms uuid=5fcdf7f8-3343-41a4-9ff1-1b6c239b9aab waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=32fda6f8-1143-4457-901b-8d06bf657229
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.569312ms uuid=32fda6f8-1143-4457-901b-8d06bf657229 waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=4405e8ec-3fd1-483d-af9d-a905e7aa03c6
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.585391ms uuid=4405e8ec-3fd1-483d-af9d-a905e7aa03c6 waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=cefb79ff-cfdf-4e8e-8107-95861cc22870
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.596983ms uuid=cefb79ff-cfdf-4e8e-8107-95861cc22870 waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=13911d16-53e1-4389-b641-c39326312919
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.607513ms uuid=13911d16-53e1-4389-b641-c39326312919 waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=f7fe9c73-af20-4ac9-ad11-518991a80af5
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.618722ms uuid=f7fe9c73-af20-4ac9-ad11-518991a80af5 waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=8e3863ff-6ff5-450f-8fc9-5f3d4d4320ba
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.628901ms uuid=8e3863ff-6ff5-450f-8fc9-5f3d4d4320ba waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=d6171b50-b758-45ce-9217-e4cc35497fd5
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.640603ms uuid=d6171b50-b758-45ce-9217-e4cc35497fd5 waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=f2709577-075c-40f3-9bec-b29ff3d089cd
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.652305ms uuid=f2709577-075c-40f3-9bec-b29ff3d089cd waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=dba49514-e933-4a89-a919-a97880fa9848
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.664028ms uuid=dba49514-e933-4a89-a919-a97880fa9848 waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=9ace6717-6cd4-4936-8e94-725d753b9712
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.709261ms uuid=9ace6717-6cd4-4936-8e94-725d753b9712 waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=1cf01d08-4865-4e58-82a3-2e3a22c3adfe
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.721695ms uuid=1cf01d08-4865-4e58-82a3-2e3a22c3adfe waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=78893422-eb27-4d4c-bb6e-a52a1e5abf08
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.732074ms uuid=78893422-eb27-4d4c-bb6e-a52a1e5abf08 waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=0b13b63b-7ba4-4881-a6f6-3f6d5219f14a
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.74592ms uuid=0b13b63b-7ba4-4881-a6f6-3f6d5219f14a waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=8d31eee9-6d47-4bf6-940d-27a0af7ed64e
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.757853ms uuid=8d31eee9-6d47-4bf6-940d-27a0af7ed64e waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=1f733b18-7421-4c17-a1d1-2f9fe4b14a20
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.765527ms uuid=1f733b18-7421-4c17-a1d1-2f9fe4b14a20 waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=db8b9404-e4fa-4ac3-9c7e-236295eed37f
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.777669ms uuid=db8b9404-e4fa-4ac3-9c7e-236295eed37f waitDurationTotal=0s
level=warning msg="Not processing API request. Wait duration for maximum parallel requests exceeds maximum" error="context deadline exceeded" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=0b60a957-e210-4f9c-a2b4-049e9cdca4c7
level=info msg="API call has been processed" error="timed out while waiting to be served with 1 parallel requests: context deadline exceeded" name=foo processingDuration=0s subsys=rate totalDuration=500.789091ms uuid=0b60a957-e210-4f9c-a2b4-049e9cdca4c7 waitDurationTotal=0s
level=info msg="API call has been processed" name=foo processingDuration=510.355606ms subsys=rate totalDuration=510.475408ms uuid=de0acb8a-9ad3-4664-be1b-7d01e4220081 waitDurationTotal="101.099µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=6f53c70d-61f8-4af0-809d-f624b0e1dfdc
level=info msg="API request released by rate limiter" burst=10 limit=50.00/s maxWaitDuration=500ms maxWaitDurationLimiter=499.973411ms name=foo parallelRequests=1 subsys=rate uuid=6f53c70d-61f8-4af0-809d-f624b0e1dfdc waitDurationLimiter=0s waitDurationTotal="38.522µs"
level=info msg="API call has been processed" name=foo processingDuration="2.615µs" subsys=rate totalDuration="62.376µs" uuid=6f53c70d-61f8-4af0-809d-f624b0e1dfdc waitDurationTotal="38.522µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=ff75a561-645c-4f23-b94b-b13d2502ff7d
level=info msg="API request released by rate limiter" burst=10 limit=50.00/s maxWaitDuration=500ms maxWaitDurationLimiter=499.987977ms name=foo parallelRequests=1 subsys=rate uuid=ff75a561-645c-4f23-b94b-b13d2502ff7d waitDurationLimiter=0s waitDurationTotal="15.899µs"
level=info msg="API call has been processed" name=foo processingDuration=862ns subsys=rate totalDuration="29.555µs" uuid=ff75a561-645c-4f23-b94b-b13d2502ff7d waitDurationTotal="15.899µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=a86cf8f2-3667-405a-abd2-e70fd11eae6f
level=info msg="API request released by rate limiter" burst=10 limit=50.00/s maxWaitDuration=500ms maxWaitDurationLimiter=499.989631ms name=foo parallelRequests=1 subsys=rate uuid=a86cf8f2-3667-405a-abd2-e70fd11eae6f waitDurationLimiter=0s waitDurationTotal="13.195µs"
level=info msg="API call has been processed" name=foo processingDuration=621ns subsys=rate totalDuration="29.585µs" uuid=a86cf8f2-3667-405a-abd2-e70fd11eae6f waitDurationTotal="13.195µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=6d09b275-eb2c-4afd-80da-b38de2129761
level=info msg="API request released by rate limiter" burst=10 limit=50.00/s maxWaitDuration=500ms maxWaitDurationLimiter=499.989169ms name=foo parallelRequests=1 subsys=rate uuid=6d09b275-eb2c-4afd-80da-b38de2129761 waitDurationLimiter=0s waitDurationTotal="13.506µs"
level=info msg="API call has been processed" name=foo processingDuration=741ns subsys=rate totalDuration="37.29µs" uuid=6d09b275-eb2c-4afd-80da-b38de2129761 waitDurationTotal="13.506µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=b7196e89-63bc-4923-a156-c75c41ff947c
level=info msg="API request released by rate limiter" burst=10 limit=50.00/s maxWaitDuration=500ms maxWaitDurationLimiter=499.989841ms name=foo parallelRequests=1 subsys=rate uuid=b7196e89-63bc-4923-a156-c75c41ff947c waitDurationLimiter=0s waitDurationTotal="12.784µs"
level=info msg="API call has been processed" name=foo processingDuration=622ns subsys=rate totalDuration="66.143µs" uuid=b7196e89-63bc-4923-a156-c75c41ff947c waitDurationTotal="12.784µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=b3cd4cff-2b19-4132-b23d-d1a0c4fe9374
level=info msg="API request released by rate limiter" burst=10 limit=50.00/s maxWaitDuration=500ms maxWaitDurationLimiter=499.988538ms name=foo parallelRequests=1 subsys=rate uuid=b3cd4cff-2b19-4132-b23d-d1a0c4fe9374 waitDurationLimiter=0s waitDurationTotal="15.129µs"
level=info msg="API call has been processed" name=foo processingDuration=721ns subsys=rate totalDuration="27.873µs" uuid=b3cd4cff-2b19-4132-b23d-d1a0c4fe9374 waitDurationTotal="15.129µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=1ddb4c13-56a0-4d4a-930a-35ca3dfba226
level=info msg="API request released by rate limiter" burst=10 limit=50.00/s maxWaitDuration=500ms maxWaitDurationLimiter=499.985122ms name=foo parallelRequests=1 subsys=rate uuid=1ddb4c13-56a0-4d4a-930a-35ca3dfba226 waitDurationLimiter=0s waitDurationTotal="18.094µs"
level=info msg="API call has been processed" name=foo processingDuration=681ns subsys=rate totalDuration="32.211µs" uuid=1ddb4c13-56a0-4d4a-930a-35ca3dfba226 waitDurationTotal="18.094µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=919b9345-5a4f-4c12-acc7-7e89c12629cb
level=info msg="API request released by rate limiter" burst=10 limit=50.00/s maxWaitDuration=500ms maxWaitDurationLimiter=499.986735ms name=foo parallelRequests=1 subsys=rate uuid=919b9345-5a4f-4c12-acc7-7e89c12629cb waitDurationLimiter=0s waitDurationTotal="17.032µs"
level=info msg="API call has been processed" name=foo processingDuration=831ns subsys=rate totalDuration="31.97µs" uuid=919b9345-5a4f-4c12-acc7-7e89c12629cb waitDurationTotal="17.032µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=7155e1f4-ccda-4a3b-8dfe-04b136457c6f
level=info msg="API request released by rate limiter" burst=10 limit=50.00/s maxWaitDuration=500ms maxWaitDurationLimiter=499.988689ms name=foo parallelRequests=1 subsys=rate uuid=7155e1f4-ccda-4a3b-8dfe-04b136457c6f waitDurationLimiter=0s waitDurationTotal="14.317µs"
level=info msg="API call has been processed" name=foo processingDuration=762ns subsys=rate totalDuration="28.273µs" uuid=7155e1f4-ccda-4a3b-8dfe-04b136457c6f waitDurationTotal="14.317µs"
level=info msg="Processing API request with rate limiter" maxWaitDuration=500ms name=foo parallelRequests=1 subsys=rate uuid=11df639f-b2c0-487f-958f-12711765fd09
level=info msg="API request released by rate limiter" burst=10 limit=50.00/s maxWaitDuration=500ms maxWaitDurationLimiter=499.98894ms name=foo parallelRequests=1 subsys=rate uuid=11df639f-b2c0-487f-958f-12711765fd09 waitDurationLimiter=0s waitDurationTotal="14.206µs"
level=info msg="API call has been processed" name=foo processingDuration=622ns subsys=rate totalDuration="28.413µs" uuid=11df639f-b2c0-487f-958f-12711765fd09 waitDurationTotal="14.206µs"]]></system-out>
		</testcase>
		<testcase name="Test/ControllerSuite/TestSkipInitial" classname="github.com/cilium/cilium/pkg/rate" time="0.020"></testcase>
		<testcase name="Test/ControllerSuite/TestStressRateLimiter" classname="github.com/cilium/cilium/pkg/rate" time="1.230">
			<system-out><![CDATA[level=info msg="&{name:foo params:{EstimatedProcessingDuration:5000000 AutoAdjust:true MeanOver:10 ParallelRequests:50 MaxParallelRequests:0 MinParallelRequests:1 RateLimit:1000 RateBurst:1 MinWaitDuration:0 MaxWaitDuration:10000000 Log:false DelayedAdjustmentFactor:0.5 SkipInitial:0 MaxAdjustmentFactor:100} metrics:<nil> mutex:{internalRWMutex:{RWMutex:{w:{state:0 sema:0} writerSem:0 readerSem:0 readerCount:{_:{} v:0} readerWait:{_:{} v:0}}}} meanProcessingDuration:0.0056114623 processingDurations:[5687595 5622987 5537891 5473111 5524133 5462609 5391619 5433925 5742921 6237832] meanWaitDuration:0.0095030329 waitDurations:[9055112 10166766 10319406 10323522 9025560 10132121 9055954 7911802 8984650 10055436] parallelRequests:47 adjustmentFactor:0.8910333408102912 limiter:0xc000263bd0 currentRequestsInFlight:0 requestsProcessed:1000 requestsScheduled:9303 parallelWaitSemaphore:0xc000263b80}" subsys=rate
level=info msg="Total retries: 8303" subsys=rate]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/rate	coverage: 99.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/redirectpolicy" tests="8" failures="0" errors="0" id="488" hostname="kind-bpf-next" time="0.034" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/redirectpolicy" time="0.000"></testcase>
		<testcase name="Test/ManagerSuite" classname="github.com/cilium/cilium/pkg/redirectpolicy" time="0.000"></testcase>
		<testcase name="Test/ManagerSuite/TestManager_AddRedirectPolicy_AddrMatcherDuplicateConfig" classname="github.com/cilium/cilium/pkg/redirectpolicy" time="0.000"></testcase>
		<testcase name="Test/ManagerSuite/TestManager_AddRedirectPolicy_SvcMatcherDuplicateConfig" classname="github.com/cilium/cilium/pkg/redirectpolicy" time="0.000"></testcase>
		<testcase name="Test/ManagerSuite/TestManager_AddrMatcherConfigDualStack" classname="github.com/cilium/cilium/pkg/redirectpolicy" time="0.000"></testcase>
		<testcase name="Test/ManagerSuite/TestManager_AddrMatcherConfigMultiplePorts" classname="github.com/cilium/cilium/pkg/redirectpolicy" time="0.000"></testcase>
		<testcase name="Test/ManagerSuite/TestManager_AddrMatcherConfigSinglePort" classname="github.com/cilium/cilium/pkg/redirectpolicy" time="0.000"></testcase>
		<testcase name="Test/ManagerSuite/TestManager_OnAddandUpdatePod" classname="github.com/cilium/cilium/pkg/redirectpolicy" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/redirectpolicy	coverage: 46.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/revert" tests="5" failures="0" errors="0" id="489" hostname="kind-bpf-next" time="0.005" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/revert" time="0.000"></testcase>
		<testcase name="Test/RevertTestSuite" classname="github.com/cilium/cilium/pkg/revert" time="0.000"></testcase>
		<testcase name="Test/RevertTestSuite/TestFinalizeList" classname="github.com/cilium/cilium/pkg/revert" time="0.000"></testcase>
		<testcase name="Test/RevertTestSuite/TestRevertStack" classname="github.com/cilium/cilium/pkg/revert" time="0.000"></testcase>
		<testcase name="Test/RevertTestSuite/TestRevertStackError" classname="github.com/cilium/cilium/pkg/revert" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/revert	coverage: 83.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/safeio" tests="6" failures="0" errors="0" id="490" hostname="kind-bpf-next" time="0.003" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestReadLimitExceeds" classname="github.com/cilium/cilium/pkg/safeio" time="0.000"></testcase>
		<testcase name="TestReadLimitIsLess" classname="github.com/cilium/cilium/pkg/safeio" time="0.000"></testcase>
		<testcase name="TestReadLimitIsEqual" classname="github.com/cilium/cilium/pkg/safeio" time="0.000"></testcase>
		<testcase name="TestReadLimitExceedsLargeBuffer" classname="github.com/cilium/cilium/pkg/safeio" time="0.000"></testcase>
		<testcase name="TestReadLimitIsLessLargeBuffer" classname="github.com/cilium/cilium/pkg/safeio" time="0.000"></testcase>
		<testcase name="TestReadLimitIsEqualLargeBuffer" classname="github.com/cilium/cilium/pkg/safeio" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/safeio	coverage: 35.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/safetime" tests="4" failures="0" errors="0" id="491" hostname="kind-bpf-next" time="0.007" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/safetime" time="0.000"></testcase>
		<testcase name="Test/SafetimeSuite" classname="github.com/cilium/cilium/pkg/safetime" time="0.000"></testcase>
		<testcase name="Test/SafetimeSuite/TestNegativeDuration" classname="github.com/cilium/cilium/pkg/safetime" time="0.000">
			<system-out><![CDATA[time="2023-05-31T11:44:27Z" level=warning msg="BUG: negative duration" duration=-999.999569ms endTime="2023-05-31 11:44:27.622877967 +0000 UTC m=+0.000995872" file-path=/host/pkg/safetime/safetime_test.go line=38 startTime="2023-05-31 11:44:28.622877526 +0000 UTC m=+1.000995441"
]]></system-out>
		</testcase>
		<testcase name="Test/SafetimeSuite/TestNonNegativeDuration" classname="github.com/cilium/cilium/pkg/safetime" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/safetime	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/serializer" tests="1" failures="0" errors="0" id="492" hostname="kind-bpf-next" time="0.003" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestFuncSerializer" classname="github.com/cilium/cilium/pkg/serializer" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/serializer	coverage: 90.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/service" tests="45" failures="0" errors="0" id="493" hostname="kind-bpf-next" time="0.043" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/service" time="0.010"></testcase>
		<testcase name="Test/IDAllocTestSuite" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/IDAllocTestSuite/TestBackendID" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/IDAllocTestSuite/TestGetMaxServiceID" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/IDAllocTestSuite/TestServices" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite" classname="github.com/cilium/cilium/pkg/service" time="0.010"></testcase>
		<testcase name="Test/ManagerTestSuite/TestDeleteServiceWithTerminatingBackends" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestGetServiceNameByAddr" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestHealthCheckNodePort" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestHealthCheckNodePortDisabled" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestL7LoadBalancerServiceOverride" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestLocalRedirectLocalBackendSelection" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestLocalRedirectServiceOverride" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestRestoreServiceWithBackendStates" classname="github.com/cilium/cilium/pkg/service" time="0.000">
			<system-out><![CDATA[level=info msg="Persisting service with backend state update" backendID=1 backendPreferred=false backendState=2 l3n4Addr="10.0.0.1:8080" serviceID=1 subsys=service
level=info msg="Persisting service with backend state update" backendID=2 backendPreferred=false backendState=3 l3n4Addr="10.0.0.2:8080" serviceID=1 subsys=service
level=info msg="Persisting updated backend state for backend" backendID=1 backendPreferred=false backendState=2 l3n4Addr="10.0.0.1:8080" subsys=service
level=info msg="Persisting updated backend state for backend" backendID=2 backendPreferred=false backendState=3 l3n4Addr="10.0.0.2:8080" subsys=service
level=info msg="Restored services from maps" failedServices=0 restoredServices=1 subsys=service
level=info msg="Restored backends from maps" failedBackends=0 restoredBackends=3 skippedBackends=0 subsys=service]]></system-out>
		</testcase>
		<testcase name="Test/ManagerTestSuite/TestRestoreServiceWithTerminatingBackends" classname="github.com/cilium/cilium/pkg/service" time="0.000">
			<system-out><![CDATA[level=warning msg="Unable to add entry to affinity match map" backendID=1 error="Backend 1 already exists in 1 affinity map" serviceID=1 subsys=service
level=warning msg="Unable to add entry to affinity match map" backendID=2 error="Backend 2 already exists in 1 affinity map" serviceID=1 subsys=service
level=warning msg="Unable to add entry to affinity match map" backendID=3 error="Backend 3 already exists in 1 affinity map" serviceID=1 subsys=service
level=info msg="Restored services from maps" failedServices=0 restoredServices=1 subsys=service
level=info msg="Restored backends from maps" failedBackends=0 restoredBackends=3 skippedBackends=0 subsys=service]]></system-out>
		</testcase>
		<testcase name="Test/ManagerTestSuite/TestRestoreServices" classname="github.com/cilium/cilium/pkg/service" time="0.000">
			<system-out><![CDATA[level=info msg="Restored services from maps" failedServices=0 restoredServices=2 subsys=service
level=info msg="Restored backends from maps" failedBackends=0 restoredBackends=3 skippedBackends=0 subsys=service]]></system-out>
		</testcase>
		<testcase name="Test/ManagerTestSuite/TestRestoreServicesWithLeakedBackends" classname="github.com/cilium/cilium/pkg/service" time="0.000">
			<system-out><![CDATA[level=info msg="Restored services from maps" failedServices=0 restoredServices=1 subsys=service
level=info msg="Restored backends from maps" failedBackends=0 restoredBackends=2 skippedBackends=4 subsys=service]]></system-out>
		</testcase>
		<testcase name="Test/ManagerTestSuite/TestSyncServices" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestSyncWithK8sFinished" classname="github.com/cilium/cilium/pkg/service" time="0.000">
			<system-out><![CDATA[level=info msg="Restored services from maps" failedServices=0 restoredServices=2 subsys=service
level=info msg="Restored backends from maps" failedBackends=0 restoredBackends=3 skippedBackends=0 subsys=service
level=warning msg="Deleting no longer present service" l3n4Addr="{AddrCluster:1.1.1.1 L4Addr:{Protocol:NONE Port:80} Scope:0}" serviceID=1 subsys=service
level=info msg="Deleted orphan backends" orphanBackends=1 subsys=service]]></system-out>
		</testcase>
		<testcase name="Test/ManagerTestSuite/TestTrafficPolicy" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestUpdateBackendsState" classname="github.com/cilium/cilium/pkg/service" time="0.000">
			<system-out><![CDATA[level=info msg="Persisting service with backend state update" backendID=1 backendPreferred=false backendState=2 l3n4Addr="10.0.0.1:8080" serviceID=1 subsys=service
level=info msg="Persisting service with backend state update" backendID=1 backendPreferred=false backendState=2 l3n4Addr="10.0.0.1:8080" serviceID=2 subsys=service
level=info msg="Persisting updated backend state for backend" backendID=1 backendPreferred=false backendState=2 l3n4Addr="10.0.0.1:8080" subsys=service
level=info msg="Persisting service with backend state update" backendID=1 backendPreferred=false backendState=0 l3n4Addr="10.0.0.1:8080" serviceID=1 subsys=service
level=info msg="Persisting service with backend state update" backendID=1 backendPreferred=false backendState=0 l3n4Addr="10.0.0.1:8080" serviceID=2 subsys=service
level=info msg="Persisting updated backend state for backend" backendID=1 backendPreferred=false backendState=0 l3n4Addr="10.0.0.1:8080" subsys=service]]></system-out>
		</testcase>
		<testcase name="Test/ManagerTestSuite/TestUpsertAndDeleteService" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestUpsertAndDeleteServiceNat46" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestUpsertAndDeleteServiceNat64" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestUpsertAndDeleteServiceWithoutIPv6" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestUpsertServiceWithExternalClusterIP" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestUpsertServiceWithOnlyTerminatingBackends" classname="github.com/cilium/cilium/pkg/service" time="0.000">
			<system-out><![CDATA[level=warning msg="Unable to add entry to affinity match map" backendID=1 error="Backend 1 already exists in 1 affinity map" serviceID=1 subsys=service
level=warning msg="Unable to add entry to affinity match map" backendID=2 error="Backend 2 already exists in 1 affinity map" serviceID=1 subsys=service
level=warning msg="Unable to add entry to affinity match map" backendID=1 error="Backend 1 already exists in 1 affinity map" serviceID=1 subsys=service
level=warning msg="Unable to add entry to affinity match map" backendID=1 error="Backend 1 already exists in 1 affinity map" serviceID=1 subsys=service]]></system-out>
		</testcase>
		<testcase name="Test/ManagerTestSuite/TestUpsertServiceWithOutExternalClusterIP" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test/ManagerTestSuite/TestUpsertServiceWithTerminatingBackends" classname="github.com/cilium/cilium/pkg/service" time="0.000">
			<system-out><![CDATA[level=warning msg="Unable to add entry to affinity match map" backendID=1 error="Backend 1 already exists in 1 affinity map" serviceID=1 subsys=service
level=warning msg="Unable to add entry to affinity match map" backendID=2 error="Backend 2 already exists in 1 affinity map" serviceID=1 subsys=service
level=warning msg="Unable to add entry to affinity match map" backendID=3 error="Backend 3 already exists in 1 affinity map" serviceID=1 subsys=service]]></system-out>
		</testcase>
		<testcase name="Test/ManagerTestSuite/TestUpsertServiceWithZeroWeightBackends" classname="github.com/cilium/cilium/pkg/service" time="0.000">
			<system-out><![CDATA[level=warning msg="Unable to add entry to affinity match map" backendID=1 error="Backend 1 already exists in 1 affinity map" serviceID=1 subsys=service
level=warning msg="Unable to add entry to affinity match map" backendID=2 error="Backend 2 already exists in 1 affinity map" serviceID=1 subsys=service
level=warning msg="Unable to add entry to affinity match map" backendID=3 error="Backend 3 already exists in 1 affinity map" serviceID=1 subsys=service
level=warning msg="Unable to add entry to affinity match map" backendID=1 error="Backend 1 already exists in 1 affinity map" serviceID=1 subsys=service]]></system-out>
		</testcase>
		<testcase name="TestLocalRedirectServiceExistsError" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_by_port_number" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_by_port_number/all_ports_are_allowed" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_by_port_number/only_http_port" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_by_port_number/no_match" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_by_port_named" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_by_port_named/all_ports_are_allowed" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_by_port_named/only_http_named_port" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_by_port_named/multiple_named_ports" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_with_preferred_backend" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_with_preferred_backend/all_ports_are_allowed" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_with_preferred_backend/only_named_ports" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_with_preferred_backend/multiple_named_ports" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<testcase name="Test_filterServiceBackends/filter_with_preferred_backend/only_port_number" classname="github.com/cilium/cilium/pkg/service" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/service	coverage: 76.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/service/healthserver" tests="4" failures="0" errors="0" id="494" hostname="kind-bpf-next" time="0.014" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/service/healthserver" time="0.000"></testcase>
		<testcase name="Test/ServiceHealthServerSuite" classname="github.com/cilium/cilium/pkg/service/healthserver" time="0.000"></testcase>
		<testcase name="Test/ServiceHealthServerSuite/TestServiceHealthServer_UpsertService" classname="github.com/cilium/cilium/pkg/service/healthserver" time="0.000"></testcase>
		<testcase name="Test/ServiceHealthServerSuite/Test_httpHealthServer_ServeHTTP" classname="github.com/cilium/cilium/pkg/service/healthserver" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/service/healthserver	coverage: 70.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/service/store" tests="4" failures="0" errors="0" id="495" hostname="kind-bpf-next" time="0.024" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/service/store" time="0.000"></testcase>
		<testcase name="Test/ServiceGenericSuite" classname="github.com/cilium/cilium/pkg/service/store" time="0.000"></testcase>
		<testcase name="Test/ServiceGenericSuite/TestClusterService" classname="github.com/cilium/cilium/pkg/service/store" time="0.000"></testcase>
		<testcase name="Test/ServiceGenericSuite/TestPortConfigurationDeepEqual" classname="github.com/cilium/cilium/pkg/service/store" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/service/store	coverage: 18.7% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/signal" tests="4" failures="0" errors="0" id="496" hostname="kind-bpf-next" time="1.021" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/signal" time="1.000"></testcase>
		<testcase name="Test/signalSuite" classname="github.com/cilium/cilium/pkg/signal" time="1.000"></testcase>
		<testcase name="Test/signalSuite/TestLifeCycle" classname="github.com/cilium/cilium/pkg/signal" time="1.000">
			<system-out><![CDATA[level=info msg="Datapath signal listener running" subsys=signal
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal]]></system-out>
		</testcase>
		<testcase name="Test/signalSuite/TestSignalSet" classname="github.com/cilium/cilium/pkg/signal" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/signal	coverage: 73.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/slices" tests="46" failures="0" errors="0" id="497" hostname="kind-bpf-next" time="0.004" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestUnique" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestUnique/nil_slice" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestUnique/empty_slice" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestUnique/single_element" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestUnique/all_uniques" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestUnique/all_duplicates" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestUnique/uniques_and_duplicates" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUnique" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUnique/nil_slice" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUnique/empty_slice" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUnique/single_element" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUnique/all_uniques" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUnique/all_duplicates" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUnique/uniques_and_duplicates" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUniqueFunc" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUniqueFunc/nil_slice" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUniqueFunc/empty_slice" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUniqueFunc/single_element" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUniqueFunc/all_uniques" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUniqueFunc/all_duplicates" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSortedUniqueFunc/uniques_and_duplicates" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestUniqueKeepOrdering" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestDiff" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestDiff/empty_second_slice" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestDiff/empty_first_slice" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestDiff/both_empty" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestDiff/both_nil" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestDiff/subset" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestDiff/equal" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestDiff/same_size_not_equal" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestDiff/smaller_size" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestDiff/larger_size" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestDiff/subset_with_duplicates" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestDiff/subset_with_more_duplicates" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSubsetOf" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSubsetOf/empty_second_slice" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSubsetOf/empty_first_slice" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSubsetOf/both_empty" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSubsetOf/both_nil" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSubsetOf/subset" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSubsetOf/equal" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSubsetOf/same_size_not_equal" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSubsetOf/smaller_size" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSubsetOf/larger_size" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSubsetOf/subset_with_duplicates" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<testcase name="TestSubsetOf/subset_with_more_duplicates" classname="github.com/cilium/cilium/pkg/slices" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/slices	coverage: 82.6% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/socketlb" tests="5" failures="0" errors="0" id="498" hostname="kind-bpf-next" time="0.040" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestAttachCgroup" classname="github.com/cilium/cilium/pkg/socketlb" time="0.010">
			<system-out><![CDATA[level=info msg="No existing link found at /sys/fs/bpf/cilium-test3804367852/test for program test" subsys=socketlb]]></system-out>
		</testcase>
		<testcase name="TestAttachCgroupWithPreviousAttach" classname="github.com/cilium/cilium/pkg/socketlb" time="0.000">
			<system-out><![CDATA[level=info msg="No existing link found at /sys/fs/bpf/cilium-test1586459453/test for program test" subsys=socketlb]]></system-out>
		</testcase>
		<testcase name="TestAttachCgroupWithExistingLink" classname="github.com/cilium/cilium/pkg/socketlb" time="0.000">
			<system-out><![CDATA[level=info msg="Updated link /sys/fs/bpf/cilium-test3457521167/test for program test" subsys=socketlb]]></system-out>
		</testcase>
		<testcase name="TestDetachCGroupWithPreviousAttach" classname="github.com/cilium/cilium/pkg/socketlb" time="0.000"></testcase>
		<testcase name="TestDetachCGroupWithExistingLink" classname="github.com/cilium/cilium/pkg/socketlb" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/socketlb	coverage: 39.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/source" tests="3" failures="0" errors="0" id="499" hostname="kind-bpf-next" time="0.003" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/source" time="0.000"></testcase>
		<testcase name="Test/SourceTestSuite" classname="github.com/cilium/cilium/pkg/source" time="0.000"></testcase>
		<testcase name="Test/SourceTestSuite/TestAllowOverwrite" classname="github.com/cilium/cilium/pkg/source" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/source	coverage: 90.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/spanstat" tests="14" failures="0" errors="0" id="500" hostname="kind-bpf-next" time="0.107" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/spanstat" time="0.100"></testcase>
		<testcase name="Test/SpanStatTestSuite" classname="github.com/cilium/cilium/pkg/spanstat" time="0.100"></testcase>
		<testcase name="Test/SpanStatTestSuite/TestSpanStat" classname="github.com/cilium/cilium/pkg/spanstat" time="0.100"></testcase>
		<testcase name="Test/SpanStatTestSuite/TestSpanStatSeconds" classname="github.com/cilium/cilium/pkg/spanstat" time="0.000"></testcase>
		<testcase name="Test/SpanStatTestSuite/TestSpanStatSecondsRaceCondition" classname="github.com/cilium/cilium/pkg/spanstat" time="0.000"></testcase>
		<testcase name="Test/SpanStatTestSuite/TestSpanStatStart" classname="github.com/cilium/cilium/pkg/spanstat" time="0.000"></testcase>
		<testcase name="TestSpanStatRaceCondition" classname="github.com/cilium/cilium/pkg/spanstat" time="0.000"></testcase>
		<testcase name="TestSpanStatRaceCondition/End_function" classname="github.com/cilium/cilium/pkg/spanstat" time="0.000"></testcase>
		<testcase name="TestSpanStatRaceCondition/EndError_function" classname="github.com/cilium/cilium/pkg/spanstat" time="0.000"></testcase>
		<testcase name="TestSpanStatRaceCondition/Seconds_function" classname="github.com/cilium/cilium/pkg/spanstat" time="0.000"></testcase>
		<testcase name="TestSpanStatRaceCondition/Total_function" classname="github.com/cilium/cilium/pkg/spanstat" time="0.000"></testcase>
		<testcase name="TestSpanStatRaceCondition/FailureTotal_function" classname="github.com/cilium/cilium/pkg/spanstat" time="0.000"></testcase>
		<testcase name="TestSpanStatRaceCondition/SuccessTotal_function" classname="github.com/cilium/cilium/pkg/spanstat" time="0.000"></testcase>
		<testcase name="TestSpanStatRaceCondition/Reset_function" classname="github.com/cilium/cilium/pkg/spanstat" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/spanstat	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/statedb" tests="1" failures="0" errors="0" id="501" hostname="kind-bpf-next" time="0.007" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestDB" classname="github.com/cilium/cilium/pkg/statedb" time="0.000">
			<system-out><![CDATA[level=info msg=Invoked duration="374.639µs" function="statedb.runTest (db_test.go:61)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg=Stopping subsys=hive]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/statedb	coverage: 62.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/status" tests="6" failures="0" errors="0" id="502" hostname="kind-bpf-next" time="0.317" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/status" time="0.300"></testcase>
		<testcase name="Test/StatusTestSuite" classname="github.com/cilium/cilium/pkg/status" time="0.300"></testcase>
		<testcase name="Test/StatusTestSuite/TestCollectorFailureTimeout" classname="github.com/cilium/cilium/pkg/status" time="0.080">
			<system-out><![CDATA[level=warning msg="No response from probe within 0.02 seconds" probe= subsys=status
level=warning msg="Timeout while waiting probe" probe= startTime="2023-05-31 11:44:33.765401981 +0000 UTC m=+0.011406482" subsys=status]]></system-out>
		</testcase>
		<testcase name="Test/StatusTestSuite/TestCollectorSuccess" classname="github.com/cilium/cilium/pkg/status" time="0.030"></testcase>
		<testcase name="Test/StatusTestSuite/TestCollectorSuccessAfterTimeout" classname="github.com/cilium/cilium/pkg/status" time="0.180">
			<system-out><![CDATA[level=warning msg="No response from probe within 0.02 seconds" probe= subsys=status
level=warning msg="Timeout while waiting probe" probe= startTime="2023-05-31 11:44:33.878244594 +0000 UTC m=+0.124249084" subsys=status]]></system-out>
		</testcase>
		<testcase name="Test/StatusTestSuite/TestVariableProbeInterval" classname="github.com/cilium/cilium/pkg/status" time="0.010"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/status	coverage: 75.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/stream" tests="14" failures="0" errors="0" id="503" hostname="kind-bpf-next" time="0.156" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestMap" classname="github.com/cilium/cilium/pkg/stream" time="0.000"></testcase>
		<testcase name="TestFilter" classname="github.com/cilium/cilium/pkg/stream" time="0.000"></testcase>
		<testcase name="TestReduce" classname="github.com/cilium/cilium/pkg/stream" time="0.000"></testcase>
		<testcase name="TestThrottle" classname="github.com/cilium/cilium/pkg/stream" time="0.000"></testcase>
		<testcase name="TestRetry" classname="github.com/cilium/cilium/pkg/stream" time="0.000"></testcase>
		<testcase name="TestRetryFuncs" classname="github.com/cilium/cilium/pkg/stream" time="0.040"></testcase>
		<testcase name="TestDistict" classname="github.com/cilium/cilium/pkg/stream" time="0.000"></testcase>
		<testcase name="TestDebounce" classname="github.com/cilium/cilium/pkg/stream" time="0.100"></testcase>
		<testcase name="TestFirst" classname="github.com/cilium/cilium/pkg/stream" time="0.000"></testcase>
		<testcase name="TestLast" classname="github.com/cilium/cilium/pkg/stream" time="0.000"></testcase>
		<testcase name="TestToSlice" classname="github.com/cilium/cilium/pkg/stream" time="0.000"></testcase>
		<testcase name="TestToChannel" classname="github.com/cilium/cilium/pkg/stream" time="0.000"></testcase>
		<testcase name="TestMulticast" classname="github.com/cilium/cilium/pkg/stream" time="0.000"></testcase>
		<testcase name="TestMulticastCancel" classname="github.com/cilium/cilium/pkg/stream" time="0.010"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/stream	coverage: 89.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/sysctl" tests="7" failures="0" errors="0" id="504" hostname="kind-bpf-next" time="0.033" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/sysctl" time="0.000"></testcase>
		<testcase name="Test/SysctlLinuxPrivilegedTestSuite" classname="github.com/cilium/cilium/pkg/sysctl" time="0.000"></testcase>
		<testcase name="Test/SysctlLinuxPrivilegedTestSuite/TestApplySettings" classname="github.com/cilium/cilium/pkg/sysctl" time="0.000">
			<system-out><![CDATA[level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.ip_forward sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.all.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv6.conf.all.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.ip_forward sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=foo.bar sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.ip_forward sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=foo.bar sysParamValue=1
level=warning msg="Failed to sysctl -w" error="could not open the sysctl file /proc/sys/foo/bar: open /proc/sys/foo/bar: no such file or directory" subsys=sysctl sysParamName=foo.bar sysParamValue=1]]></system-out>
		</testcase>
		<testcase name="Test/SysctlLinuxPrivilegedTestSuite/TestDisableEnable" classname="github.com/cilium/cilium/pkg/sysctl" time="0.000"></testcase>
		<testcase name="Test/SysctlLinuxPrivilegedTestSuite/TestWriteSysctl" classname="github.com/cilium/cilium/pkg/sysctl" time="0.000"></testcase>
		<testcase name="Test/SysctlLinuxTestSuite" classname="github.com/cilium/cilium/pkg/sysctl" time="0.000"></testcase>
		<testcase name="Test/SysctlLinuxTestSuite/TestFullPath" classname="github.com/cilium/cilium/pkg/sysctl" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/sysctl	coverage: 64.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/testutils" tests="3" failures="0" errors="0" id="505" hostname="kind-bpf-next" time="0.142" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/testutils" time="0.120"></testcase>
		<testcase name="Test/TestUtilsSuite" classname="github.com/cilium/cilium/pkg/testutils" time="0.120"></testcase>
		<testcase name="Test/TestUtilsSuite/TestCondition" classname="github.com/cilium/cilium/pkg/testutils" time="0.120"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/testutils	coverage: 8.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/trigger" tests="6" failures="0" errors="0" id="506" hostname="kind-bpf-next" time="0.411" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/trigger" time="0.410"></testcase>
		<testcase name="Test/TriggerTestSuite" classname="github.com/cilium/cilium/pkg/trigger" time="0.410"></testcase>
		<testcase name="Test/TriggerTestSuite/TestLongTrigger" classname="github.com/cilium/cilium/pkg/trigger" time="0.100"></testcase>
		<testcase name="Test/TriggerTestSuite/TestMinInterval" classname="github.com/cilium/cilium/pkg/trigger" time="0.100"></testcase>
		<testcase name="Test/TriggerTestSuite/TestNeedsDelay" classname="github.com/cilium/cilium/pkg/trigger" time="0.200"></testcase>
		<testcase name="Test/TriggerTestSuite/TestShutdownFunc" classname="github.com/cilium/cilium/pkg/trigger" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/trigger	coverage: 93.4% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/types" tests="19" failures="0" errors="0" id="507" hostname="kind-bpf-next" time="0.009" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/IPv4Suite" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/IPv4Suite/TestAddr" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/IPv4Suite/TestIP" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/IPv4Suite/TestString" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/IPv6Suite" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/IPv6Suite/TestAddr" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/IPv6Suite/TestIP" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/IPv6Suite/TestString" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/MACAddrSuite" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/MACAddrSuite/TestHardwareAddr" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/MACAddrSuite/TestString" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/PortsTestSuite" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/PortsTestSuite/TestPolicyNamedPortMap" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/PortsTestSuite/TestPolicyNamedPortMultiMap" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/PortsTestSuite/TestPolicyNamedPortMultiMapUpdate" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/PortsTestSuite/TestPolicyNewPortProto" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/PortsTestSuite/TestPolicyPortProtoSet" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<testcase name="Test/PortsTestSuite/TestPolicyValidateName" classname="github.com/cilium/cilium/pkg/types" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/types	coverage: 83.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/version" tests="5" failures="0" errors="0" id="508" hostname="kind-bpf-next" time="0.004" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/version" time="0.000"></testcase>
		<testcase name="Test/VersionSuite" classname="github.com/cilium/cilium/pkg/version" time="0.000"></testcase>
		<testcase name="Test/VersionSuite/TestParseKernelVersion" classname="github.com/cilium/cilium/pkg/version" time="0.000"></testcase>
		<testcase name="Test/VersionSuite/TestStructIsSet" classname="github.com/cilium/cilium/pkg/version" time="0.000"></testcase>
		<testcase name="Test/VersionSuite/TestVersionArchMatchesGOARCH" classname="github.com/cilium/cilium/pkg/version" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/version	coverage: 58.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/versioncheck" tests="3" failures="0" errors="0" id="509" hostname="kind-bpf-next" time="0.004" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/versioncheck" time="0.000"></testcase>
		<testcase name="Test/VersionCheckTestSuite" classname="github.com/cilium/cilium/pkg/versioncheck" time="0.000"></testcase>
		<testcase name="Test/VersionCheckTestSuite/TestMustCompile" classname="github.com/cilium/cilium/pkg/versioncheck" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/versioncheck	coverage: 60.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/pkg/wireguard/agent" tests="4" failures="0" errors="0" id="510" hostname="kind-bpf-next" time="0.036" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/pkg/wireguard/agent" time="0.000"></testcase>
		<testcase name="Test/AgentSuite" classname="github.com/cilium/cilium/pkg/wireguard/agent" time="0.000"></testcase>
		<testcase name="Test/AgentSuite/TestAgent_PeerConfig" classname="github.com/cilium/cilium/pkg/wireguard/agent" time="0.000"></testcase>
		<testcase name="Test/AgentSuite/TestAgent_PeerConfig_WithEncryptNode" classname="github.com/cilium/cilium/pkg/wireguard/agent" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/pkg/wireguard/agent	coverage: 33.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/plugins/cilium-cni/chaining/api" tests="4" failures="0" errors="0" id="511" hostname="kind-bpf-next" time="0.015" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/plugins/cilium-cni/chaining/api" time="0.000"></testcase>
		<testcase name="Test/APISuite" classname="github.com/cilium/cilium/plugins/cilium-cni/chaining/api" time="0.000"></testcase>
		<testcase name="Test/APISuite/TestNonChaining" classname="github.com/cilium/cilium/plugins/cilium-cni/chaining/api" time="0.000"></testcase>
		<testcase name="Test/APISuite/TestRegistration" classname="github.com/cilium/cilium/plugins/cilium-cni/chaining/api" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/plugins/cilium-cni/chaining/api	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/plugins/cilium-cni/types" tests="10" failures="0" errors="0" id="512" hostname="kind-bpf-next" time="0.024" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/plugins/cilium-cni/types" time="0.020"></testcase>
		<testcase name="Test/CNITypesSuite" classname="github.com/cilium/cilium/plugins/cilium-cni/types" time="0.020"></testcase>
		<testcase name="Test/CNITypesSuite/TestReadCNIConf" classname="github.com/cilium/cilium/plugins/cilium-cni/types" time="0.010"></testcase>
		<testcase name="Test/CNITypesSuite/TestReadCNIConfAzurev2WithPlugins" classname="github.com/cilium/cilium/plugins/cilium-cni/types" time="0.000"></testcase>
		<testcase name="Test/CNITypesSuite/TestReadCNIConfClusterPoolV2" classname="github.com/cilium/cilium/plugins/cilium-cni/types" time="0.000"></testcase>
		<testcase name="Test/CNITypesSuite/TestReadCNIConfENI" classname="github.com/cilium/cilium/plugins/cilium-cni/types" time="0.000"></testcase>
		<testcase name="Test/CNITypesSuite/TestReadCNIConfENIWithPlugins" classname="github.com/cilium/cilium/plugins/cilium-cni/types" time="0.000"></testcase>
		<testcase name="Test/CNITypesSuite/TestReadCNIConfENIv2WithPlugins" classname="github.com/cilium/cilium/plugins/cilium-cni/types" time="0.000"></testcase>
		<testcase name="Test/CNITypesSuite/TestReadCNIConfError" classname="github.com/cilium/cilium/plugins/cilium-cni/types" time="0.000"></testcase>
		<testcase name="Test/CNITypesSuite/TestReadCNIConfIPAMType" classname="github.com/cilium/cilium/plugins/cilium-cni/types" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/plugins/cilium-cni/types	coverage: 58.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/cassandra" tests="18" failures="0" errors="0" id="513" hostname="kind-bpf-next" time="0.299" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.280"></testcase>
		<testcase name="Test/CassandraSuite" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.280"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraAdditionalQueries" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.030"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraBatchRequestPolicy" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.020"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraBatchRequestPolicyDenied" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.020"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraBatchRequestPreparedStatement" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.020"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraBatchRequestPreparedStatementDenied" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.020"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraExecutePreparedStatement" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.020"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraExecutePreparedStatementUnknownID" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.020">
			<system-out><![CDATA[time="2023-05-31T11:44:41Z" level=warning msg="No cached entry for prepared-id = 'aaaa'"]]></system-out>
		</testcase>
		<testcase name="Test/CassandraSuite/TestCassandraOnDataMultiReq" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.010"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraOnDataNoHeader" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.010"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraOnDataOptionsReq" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.010"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraOnDataPartialReq" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.010"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraOnDataQueryReq" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.010"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraOnDataSplitQueryReq" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.010"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraPreparedResultReply" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.020"></testcase>
		<testcase name="Test/CassandraSuite/TestCassandraUseQuery" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.020"></testcase>
		<testcase name="Test/CassandraSuite/TestSimpleCassandraPolicy" classname="github.com/cilium/cilium/proxylib/cassandra" time="0.020"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/proxylib/cassandra	coverage: 79.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/kafka" tests="16" failures="0" errors="0" id="514" hostname="kind-bpf-next" time="0.283" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/proxylib/kafka" time="0.280"></testcase>
		<testcase name="Test/KafkaSuite" classname="github.com/cilium/cilium/proxylib/kafka" time="0.280"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataInvalidMessage" classname="github.com/cilium/cilium/proxylib/kafka" time="0.020"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataNoHeader" classname="github.com/cilium/cilium/proxylib/kafka" time="0.010"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataResponse" classname="github.com/cilium/cilium/proxylib/kafka" time="0.020"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataSimpleHeaderMinimalPolicy" classname="github.com/cilium/cilium/proxylib/kafka" time="0.010"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataSimpleHeaderSimplePolicy" classname="github.com/cilium/cilium/proxylib/kafka" time="0.020"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataSimpleHeaderWithApiKeys" classname="github.com/cilium/cilium/proxylib/kafka" time="0.020"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataSimpleHeaderWithApiKeysMismatch" classname="github.com/cilium/cilium/proxylib/kafka" time="0.020"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataSimpleHeaderWithApiVersion" classname="github.com/cilium/cilium/proxylib/kafka" time="0.020"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataSimpleHeaderWithApiVersionMismatch" classname="github.com/cilium/cilium/proxylib/kafka" time="0.020"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataSimpleHeaderWithClientID" classname="github.com/cilium/cilium/proxylib/kafka" time="0.020"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataSimpleHeaderWithClientIDAllow" classname="github.com/cilium/cilium/proxylib/kafka" time="0.020"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataSimpleHeaderWithClientIDDeny" classname="github.com/cilium/cilium/proxylib/kafka" time="0.020"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataSimpleHeaderWithPolicyAllow" classname="github.com/cilium/cilium/proxylib/kafka" time="0.020"></testcase>
		<testcase name="Test/KafkaSuite/TestKafkaOnDataSimpleHeaderWithPolicyDrop" classname="github.com/cilium/cilium/proxylib/kafka" time="0.020"></testcase>
		<system-out><![CDATA[time="2023-05-31T11:44:41Z" level=info msg="init(): Registering kafkaParserFactory"
	github.com/cilium/cilium/proxylib/kafka	coverage: 91.5% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/libcilium" tests="46" failures="0" errors="0" id="515" hostname="kind-bpf-next" time="0.478" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestMemcache" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.010"></testcase>
		<testcase name="TestMemcache/text_set_pass" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_set_drop" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_get_pass" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_get_more" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_get_drop" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_gat_pass" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_gat_more" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_gat_drop" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_delete_pass" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_delete_drop" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_incr_pass" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_incr_drop" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_touch_pass" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_touch_drop" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_slabs_pass" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_slabs_drop" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_lru_crawler_response_req_more_and_pass" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_stats_response_req_more_and_pass" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_flush_all_pass" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_flush_all_denied" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_watch_passed" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_partial_linefeed" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/text_set_pass_on_empty_rule" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/bin_get_pass_exact_key" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/bin_get_pass_prefix_key" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/bin_get_pass_regex_key" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/bin_get_drop" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000">
			<system-out><![CDATA[    proxylib_memcached_test.go:640: Inject buffer too small, data truncated]]></system-out>
		</testcase>
		<testcase name="TestMemcache/bin_get_more" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/bin_get_split" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/bin_get_remaining_key" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestMemcache/bin_set_drop_and_allow" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000">
			<system-out><![CDATA[    proxylib_memcached_test.go:709: Inject buffer too small, data truncated
    proxylib_memcached_test.go:713: Inject buffer too small, data truncated]]></system-out>
		</testcase>
		<testcase name="TestOpenModule" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestOnNewConnection" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestOnDataNoPolicy" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.050">
			<system-out><![CDATA[    proxylib_test.go:160: Inject buffer too small, data truncated]]></system-out>
		</testcase>
		<testcase name="TestOnDataPanic" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.050"></testcase>
		<testcase name="TestUnsupportedL7Drops" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.050"></testcase>
		<testcase name="TestUnsupportedL7DropsGeneric" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.050"></testcase>
		<testcase name="TestEnvoyL7DropsGeneric" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.050"></testcase>
		<testcase name="TestTwoRulesOnSamePortFirstNoL7" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestTwoRulesOnSamePortFirstNoL7Generic" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestTwoRulesOnSamePortMismatchingL7" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.000"></testcase>
		<testcase name="TestSimplePolicy" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.050"></testcase>
		<testcase name="TestAllowAllPolicy" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.050"></testcase>
		<testcase name="TestAllowEmptyPolicy" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.050"></testcase>
		<testcase name="TestAllowAllPolicyL3Egress" classname="github.com/cilium/cilium/proxylib/libcilium" time="0.050"></testcase>
		<system-out><![CDATA[time="2023-05-31T11:44:42Z" level=info msg="init(): Registering kafkaParserFactory"
	github.com/cilium/cilium/proxylib/libcilium	coverage: 78.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/memcached/binary" tests="3" failures="0" errors="0" id="516" hostname="kind-bpf-next" time="0.009" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/proxylib/memcached/binary" time="0.000"></testcase>
		<testcase name="Test/BinaryMemcachedTestSuite" classname="github.com/cilium/cilium/proxylib/memcached/binary" time="0.000"></testcase>
		<testcase name="Test/BinaryMemcachedTestSuite/TestMemcacheGetKey" classname="github.com/cilium/cilium/proxylib/memcached/binary" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/proxylib/memcached/binary	coverage: 4.8% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/npds" tests="3" failures="0" errors="0" id="517" hostname="kind-bpf-next" time="11.043" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/proxylib/npds" time="11.010"></testcase>
		<testcase name="Test/ClientSuite" classname="github.com/cilium/cilium/proxylib/npds" time="11.010"></testcase>
		<testcase name="Test/ClientSuite/TestRequestAllResources" classname="github.com/cilium/cilium/proxylib/npds" time="11.010">
			<system-out><![CDATA[level=info msg="Envoy: Starting xDS gRPC server listening on /tmp/cilium_envoy_go_test3348654696/xds.sock" subsys=envoy-manager
level=info msg="starting xDS stream processing" subsys=xds xdsStreamID=1]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/proxylib/npds	coverage: 63.3% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/proxylib" tests="3" failures="0" errors="0" id="518" hostname="kind-bpf-next" time="0.010" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/proxylib/proxylib" time="0.000"></testcase>
		<testcase name="Test/LibSuite" classname="github.com/cilium/cilium/proxylib/proxylib" time="0.000"></testcase>
		<testcase name="Test/LibSuite/TestAdvanceInput" classname="github.com/cilium/cilium/proxylib/proxylib" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/proxylib/proxylib	coverage: 2.1% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/proxylib/r2d2" tests="7" failures="0" errors="0" id="519" hostname="kind-bpf-next" time="0.064" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/proxylib/r2d2" time="0.060"></testcase>
		<testcase name="Test/R2d2Suite" classname="github.com/cilium/cilium/proxylib/r2d2" time="0.060"></testcase>
		<testcase name="Test/R2d2Suite/TestR2d2OnDataAllowDenyCmd" classname="github.com/cilium/cilium/proxylib/r2d2" time="0.010"></testcase>
		<testcase name="Test/R2d2Suite/TestR2d2OnDataAllowDenyRegex" classname="github.com/cilium/cilium/proxylib/r2d2" time="0.010"></testcase>
		<testcase name="Test/R2d2Suite/TestR2d2OnDataBasicPass" classname="github.com/cilium/cilium/proxylib/r2d2" time="0.010"></testcase>
		<testcase name="Test/R2d2Suite/TestR2d2OnDataIncomplete" classname="github.com/cilium/cilium/proxylib/r2d2" time="0.010"></testcase>
		<testcase name="Test/R2d2Suite/TestR2d2OnDataMultipleReq" classname="github.com/cilium/cilium/proxylib/r2d2" time="0.010"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/proxylib/r2d2	coverage: 88.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/test/bpf_tests" tests="2" failures="0" errors="0" id="520" hostname="kind-bpf-next" skipped="2" time="0.040" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/test/bpf_tests" time="0.000">
			<skipped message="Skipped"><![CDATA[No Ginkgo test scope defined, skipping test suite of package test/
    test_suite_test.go:86: Run this package through Ginkgo with the --focus or -cilium.testScope options]]></skipped>
		</testcase>
		<testcase name="TestBPF" classname="github.com/cilium/cilium/test/bpf_tests" time="0.000">
			<skipped message="Skipped"><![CDATA[    bpf_test.go:54: Set -bpf-test-path to run BPF tests]]></skipped>
		</testcase>
		<system-out><![CDATA[level=info msg="environment variable \"K8S_VERSION\" was not set; setting to default value \"1.26\""
coverage: [no statements]
ok  	github.com/cilium/cilium/test	0.030s	coverage: [no statements]
	github.com/cilium/cilium/test/bpf_tests	coverage: 6.2% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/test/controlplane" tests="22" failures="0" errors="0" id="521" hostname="kind-bpf-next" time="9.747" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestControlPlane" classname="github.com/cilium/cilium/test/controlplane" time="9.640"></testcase>
		<testcase name="TestControlPlane/CNPStatusNodesGC" classname="github.com/cilium/cilium/test/controlplane" time="1.390">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=92.932308ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="259.292µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="6.162µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="6.072µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.516835ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.026µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="3.737µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="1.102µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.452µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=982ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="12.693µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.093µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.032µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="24.335µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="11.712µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="29.425µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=cnp-status-update-control-plane subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.3 ipAddr.ipv6="<nil>" k8sNodeIP=172.18.0.3 labels="map[kubernetes.io/hostname:cnp-status-update-control-plane]" nodeName=cnp-status-update-control-plane subsys=k8s v4Prefix=10.244.0.0/24 v6Prefix="<nil>"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Policy Add Request" ciliumNetworkPolicy="[&{EndpointSelector:{\"matchLabels\":{\"any:foo\":\"bar\",\"k8s:io.kubernetes.pod.namespace\":\"cnp-status-update-namespace\"}} NodeSelector:{} Ingress:[] IngressDeny:[] Egress:[] EgressDeny:[] Labels:[k8s:io.cilium.k8s.policy.derived-from=CiliumNetworkPolicy k8s:io.cilium.k8s.policy.name=cnp-status-update-policy k8s:io.cilium.k8s.policy.namespace=cnp-status-update-namespace k8s:io.cilium.k8s.policy.uid=39668ba4-eb30-4e35-baa0-4db34aa639ad] Description:}]" policyAddRequest=f494603b-9d0f-4669-9479-247b060fd77f subsys=daemon
level=info msg="Policy imported via API, recalculating..." policyAddRequest=f494603b-9d0f-4669-9479-247b060fd77f policyRevision=2 subsys=daemon
level=info msg="More than one stale router IP was found on the cilium_host device after restoration, cleaning up old router IPs." subsys=daemon
level=info msg="Imported CiliumNetworkPolicy" ciliumNetworkPolicyName=cnp-status-update-policy k8sApiVersion=cilium.io/v2 k8sNamespace=cnp-status-update-namespace subsys=k8s-watcher
level=info msg="Policy Add Request" ciliumNetworkPolicy="[&{EndpointSelector:{\"matchLabels\":{\"any:foo\":\"bar\"}} NodeSelector:{} Ingress:[] IngressDeny:[] Egress:[] EgressDeny:[] Labels:[k8s:io.cilium.k8s.policy.derived-from=CiliumClusterwideNetworkPolicy k8s:io.cilium.k8s.policy.name=cnp-status-update-policy k8s:io.cilium.k8s.policy.uid=39668ba4-eb30-4e35-baa0-4db34aa639ad] Description:}]" policyAddRequest=bcf53d5b-a589-4491-b9c1-43d94a3dc54f subsys=daemon
level=info msg="Policy imported via API, recalculating..." policyAddRequest=bcf53d5b-a589-4491-b9c1-43d94a3dc54f policyRevision=3 subsys=daemon
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Imported CiliumNetworkPolicy" ciliumNetworkPolicyName=cnp-status-update-policy k8sApiVersion=cilium.io/v2 k8sNamespace= subsys=k8s-watcher
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: cnp-status-update-control-plane" subsys=daemon
level=info msg="  Node-IPv6: <nil>" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.3" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.0.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Adding local node to cluster" node="{cnp-status-update-control-plane default [{InternalIP 172.18.0.3} {CiliumInternalIP 1.1.1.2}] 10.244.0.0/24 [] <nil> [] 1.1.1.148 <nil> <nil> <nil> 0 local 0 map[kubernetes.io/hostname:cnp-status-update-control-plane] map[] 1 }" subsys=nodediscovery
level=info msg="Creating or updating CiliumNode resource" node=cnp-status-update-control-plane subsys=nodediscovery
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=warning msg="Unable to get node resource" error="ciliumnodes.cilium.io \"cnp-status-update-control-plane\" not found" subsys=nodediscovery
level=info msg="Successfully created CiliumNode resource" subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Start hook executed" duration=205.323868ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="4.378µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="1.934µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.413µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg="Starting IP identity watcher" subsys=ipcache
level=info msg=Invoked duration="34.334µs" function="cmd.registerOperatorHooks (root.go:156)" subsys=hive
level=info msg=Invoked duration=12.898734ms function="api.glob..func1 (cell.go:32)" subsys=hive
level=info msg=Invoked duration="62.908µs" function="apis.createCRDs (cell.go:63)" subsys=hive
level=info msg=Invoked duration="66.674µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Invoked duration="92.543µs" function="auth.registerIdentityWatcher (watcher.go:43)" subsys=hive
level=info msg=Invoked duration="73.318µs" function="cmd.registerLegacyOnLeader (root.go:362)" subsys=hive
level=info msg=Invoked duration="87.213µs" function="identitygc.registerGC (gc.go:79)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="2.063µs" function="cmd.registerOperatorHooks.func1 (root.go:159)" subsys=hive
level=info msg="Support for coordination.k8s.io/v1 not present, fallback to non HA mode" subsys=controlplane.test
level=info msg="Skipping creation of CRDs" subsys=create-crds
level=info msg="Start hook executed" duration="7.945µs" function="apis.createCRDs.func1 (cell.go:65)" subsys=hive
level=info msg="Start hook executed" duration="1.924µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=862ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="4.709µs" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration=831ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration=962ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration=822ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration=892ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.131µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration=721ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration=932ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Initializing IPAM" mode=kubernetes subsys=controlplane.test
level=info msg="Managing Cilium Node Taints or Setting Cilium Is Up Condition for Kubernetes Nodes" k8sNamespace=default label-selector="k8s-app=cilium" remove-cilium-node-taints=true set-cilium-is-up-condition=true set-cilium-node-taints=false subsys=controlplane.test
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="Start hook executed" duration=15.072585ms function="*api.server.Start" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="35.506µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="29.616µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="51.395µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="82.568µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="10.649µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="34.299µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="20.829µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="19.256µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.196µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.617µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="25.157µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.567µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="27.511µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="17.393µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="25.067µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="47.698µs" function="*api.server.Stop" subsys=hive
level=info msg="Starting to synchronize CiliumNode custom resources" subsys=controlplane.test
level=info msg="Skipping clean up of CNP and CCNP node status updates" subsys=controlplane.test
level=info msg="Starting to garbage collect stale CiliumNode custom resources" subsys=watchers
level=info msg="Starting to garbage collect stale CiliumEndpoint custom resources" subsys=controlplane.test
level=info msg="CiliumNodes caches synced with Kubernetes" subsys=controlplane.test
level=info msg="Starting CNP derivative handler" subsys=controlplane.test
level=info msg="Starting CCNP derivative handler" subsys=controlplane.test
level=info msg="Initialization complete" subsys=controlplane.test
level=info msg="Start hook executed" duration=201.850869ms function="cmd.(*legacyOnLeader).onStart" subsys=hive
level=info msg="Starting CRD identity garbage collector" interval=15m0s subsys=k8s-identities-gc
level=info msg="Start hook executed" duration="164.055µs" function="identitygc.registerGC.func1 (gc.go:107)" subsys=hive
level=error msg="unable to get Cilium identities from local store" error="context canceled" subsys=k8s-identities-gc
level=info msg="Stop hook executed" duration="39.844µs" function="identitygc.registerGC.func2 (gc.go:119)" subsys=hive
level=info msg="Stop hook executed" duration="115.305µs" function="cmd.(*legacyOnLeader).onStop" subsys=hive
level=info msg="Stop hook executed" duration="15.609µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="7.194µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.046µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.737µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.497µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.837µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="18.124µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg="Stop hook executed" duration="88.285µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="21.972µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="19.587µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration=136.441358ms function="cmd.registerOperatorHooks.func2 (root.go:167)" subsys=hive
level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=78.309108ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="270.214µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="5.94µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="3.596µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.667062ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.466µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.423µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="2.084µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.172µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.332µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.393µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.703µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.162µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="23.594µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="11.251µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="58.368µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=cnp-status-update-control-plane subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.3 ipAddr.ipv6="<nil>" k8sNodeIP=172.18.0.3 labels="map[kubernetes.io/hostname:cnp-status-update-control-plane]" nodeName=cnp-status-update-control-plane subsys=k8s v4Prefix=10.244.0.0/24 v6Prefix="<nil>"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: cnp-status-update-control-plane" subsys=daemon
level=info msg="  Node-IPv6: <nil>" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.3" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.0.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Creating or updating CiliumNode resource" node=cnp-status-update-control-plane subsys=nodediscovery
level=info msg="Policy Add Request" ciliumNetworkPolicy="[&{EndpointSelector:{\"matchLabels\":{\"any:foo\":\"bar\",\"k8s:io.kubernetes.pod.namespace\":\"cnp-status-update-namespace\"}} NodeSelector:{} Ingress:[] IngressDeny:[] Egress:[] EgressDeny:[] Labels:[k8s:io.cilium.k8s.policy.derived-from=CiliumNetworkPolicy k8s:io.cilium.k8s.policy.name=cnp-status-update-policy k8s:io.cilium.k8s.policy.namespace=cnp-status-update-namespace k8s:io.cilium.k8s.policy.uid=39668ba4-eb30-4e35-baa0-4db34aa639ad] Description:}]" policyAddRequest=8a9ecbed-1571-4abb-b53f-ae3cf271072f subsys=daemon
level=info msg="Policy imported via API, recalculating..." policyAddRequest=8a9ecbed-1571-4abb-b53f-ae3cf271072f policyRevision=2 subsys=daemon
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Adding local node to cluster" node="{cnp-status-update-control-plane default [{InternalIP 172.18.0.3} {CiliumInternalIP 1.1.1.2}] 10.244.0.0/24 [] <nil> [] 1.1.1.37 <nil> <nil> <nil> 0 local 0 map[kubernetes.io/hostname:cnp-status-update-control-plane] map[] 1 }" subsys=nodediscovery
level=info msg="Start hook executed" duration=305.066086ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="5.41µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="2.084µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.273µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg="Imported CiliumNetworkPolicy" ciliumNetworkPolicyName=cnp-status-update-policy k8sApiVersion=cilium.io/v2 k8sNamespace=cnp-status-update-namespace subsys=k8s-watcher
level=info msg="Policy Add Request" ciliumNetworkPolicy="[&{EndpointSelector:{\"matchLabels\":{\"any:foo\":\"bar\"}} NodeSelector:{} Ingress:[] IngressDeny:[] Egress:[] EgressDeny:[] Labels:[k8s:io.cilium.k8s.policy.derived-from=CiliumClusterwideNetworkPolicy k8s:io.cilium.k8s.policy.name=cnp-status-update-policy k8s:io.cilium.k8s.policy.uid=39668ba4-eb30-4e35-baa0-4db34aa639ad] Description:}]" policyAddRequest=fee70a6f-bf0c-4011-b764-82d544adf10a subsys=daemon
level=info msg="Policy imported via API, recalculating..." policyAddRequest=fee70a6f-bf0c-4011-b764-82d544adf10a policyRevision=3 subsys=daemon
level=info msg="Imported CiliumNetworkPolicy" ciliumNetworkPolicyName=cnp-status-update-policy k8sApiVersion=cilium.io/v2 k8sNamespace= subsys=k8s-watcher
level=info msg=Invoked duration="38.461µs" function="cmd.registerOperatorHooks (root.go:156)" subsys=hive
level=info msg=Invoked duration=14.692466ms function="api.glob..func1 (cell.go:32)" subsys=hive
level=info msg=Invoked duration="128.169µs" function="apis.createCRDs (cell.go:63)" subsys=hive
level=info msg=Invoked duration="136.634µs" function="lbipam.glob..func1 (cell.go:25)" subsys=hive
level=info msg=Invoked duration="180.817µs" function="auth.registerIdentityWatcher (watcher.go:43)" subsys=hive
level=info msg=Invoked duration="167.201µs" function="cmd.registerLegacyOnLeader (root.go:362)" subsys=hive
level=info msg=Invoked duration="156.783µs" function="identitygc.registerGC (gc.go:79)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="4.328µs" function="cmd.registerOperatorHooks.func1 (root.go:159)" subsys=hive
level=info msg="Support for coordination.k8s.io/v1 not present, fallback to non HA mode" subsys=controlplane.test
level=info msg="Skipping creation of CRDs" subsys=create-crds
level=info msg="Start hook executed" duration="20.237µs" function="apis.createCRDs.func1 (cell.go:65)" subsys=hive
level=info msg="Start hook executed" duration="8.095µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="2.435µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="17.822µs" function="*job.group.Start" subsys=hive
level=info msg="Start hook executed" duration="6.452µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.273µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.633µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="2.074µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="LB-IPAM initializing" subsys=lbipam
level=info msg="Start hook executed" duration="2.074µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="2.154µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="20.308µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Initializing IPAM" mode=kubernetes subsys=controlplane.test
level=info msg="Managing Cilium Node Taints or Setting Cilium Is Up Condition for Kubernetes Nodes" k8sNamespace=default label-selector="k8s-app=cilium" remove-cilium-node-taints=true set-cilium-is-up-condition=true set-cilium-node-taints=false subsys=controlplane.test
level=info msg="Starting to synchronize CiliumNode custom resources" subsys=controlplane.test
level=info msg="Garbage collected status/nodes in Cilium Network Policies found=1, gc=1" subsys=controlplane.test
level=info msg="Starting to garbage collect stale CiliumNode custom resources" subsys=watchers
level=info msg="Starting to garbage collect stale CiliumEndpoint custom resources" subsys=controlplane.test
level=info msg="Garbage collected status/nodes in Cilium Clusterwide Network Policies found=1, gc=1" subsys=controlplane.test
level=info msg="Start hook executed" duration=14.460914ms function="*api.server.Start" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="36.909µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="41.037µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="28.994µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="154.448µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="18.404µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="29.084µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="18.314µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="16.721µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.466µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.326µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="22.593µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.076µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="25.097µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="13.956µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="20.117µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Stop hook executed" duration="28.875µs" function="*api.server.Stop" subsys=hive
level=info msg="CiliumNodes caches synced with Kubernetes" subsys=controlplane.test
level=info msg="Starting CNP derivative handler" subsys=controlplane.test
level=info msg="Starting CCNP derivative handler" subsys=controlplane.test
level=info msg="Initialization complete" subsys=controlplane.test
level=info msg="Start hook executed" duration=101.807932ms function="cmd.(*legacyOnLeader).onStart" subsys=hive
level=info msg="Starting CRD identity garbage collector" interval=15m0s subsys=k8s-identities-gc
level=info msg="Start hook executed" duration="198.89µs" function="identitygc.registerGC.func1 (gc.go:107)" subsys=hive
level=error msg="unable to get Cilium identities from local store" error="context canceled" subsys=k8s-identities-gc
level=info msg="Stop hook executed" duration="47.599µs" function="identitygc.registerGC.func2 (gc.go:119)" subsys=hive
level=info msg="Stop hook executed" duration="97.016µs" function="cmd.(*legacyOnLeader).onStop" subsys=hive
level=info msg="Stop hook executed" duration="4.519µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.419µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="12.173µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="6.082µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.536µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.015µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="12.172µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="LB-IPAM done initializing" subsys=lbipam
level=info msg="Stop hook executed" duration="63.679µs" function="*job.group.Stop" subsys=hive
level=info msg="Stop hook executed" duration="10.285µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="17.764µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration=39.769799ms function="cmd.registerOperatorHooks.func2 (root.go:167)" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/NodeHandler" classname="github.com/cilium/cilium/test/controlplane" time="0.470">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=88.160997ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="263.04µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="5.46µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="8.014µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=101.150675ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="2.375µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.283µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="2.134µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration=972ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.493µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.463µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.633µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.473µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="16.06µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="12.413µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="44.525µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=minimal subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=10.0.0.1 ipAddr.ipv6="<nil>" k8sNodeIP=10.0.0.1 labels="map[]" nodeName=minimal subsys=k8s v4Prefix=10.0.1.0/24 v6Prefix="<nil>"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: minimal" subsys=daemon
level=info msg="  Node-IPv6: <nil>" subsys=daemon
level=info msg="  External-Node IPv4: 10.0.0.1" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.0.1.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Creating or updating CiliumNode resource" node=minimal subsys=nodediscovery
level=warning msg="Unable to get node resource" error="ciliumnodes.cilium.io \"minimal\" not found" subsys=nodediscovery
level=info msg="Successfully created CiliumNode resource" subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Adding local node to cluster" node="{minimal default [{InternalIP 10.0.0.1} {CiliumInternalIP 1.1.1.2}] 10.0.1.0/24 [] <nil> [] 1.1.1.28 <nil> <nil> <nil> 0 local 0 map[] map[] 1 }" subsys=nodediscovery
level=info msg="Start hook executed" duration=206.532971ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="17.774µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="2.555µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.293µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="29.736µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="25.408µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="30.668µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="55.568µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="12.654µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="91.361µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="23.894µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="20.889µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.767µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="4.048µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="32.02µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.517µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="31.829µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.138µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="22.061µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/CiliumNodes" classname="github.com/cilium/cilium/test/controlplane" time="2.230"></testcase>
		<testcase name="TestControlPlane/CiliumNodes/v1.24" classname="github.com/cilium/cilium/test/controlplane" time="0.810">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=126.682504ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="428.938µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="6.342µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="19.326µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=200.797828ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="4.238µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.483µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="3.145µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.212µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="2.655µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="16.411µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.012µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="6.693µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="18.845µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="13.625µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="26.009µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=cilium-nodes-worker subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.3 ipAddr.ipv6="<nil>" k8sNodeIP=172.18.0.3 labels="map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:cilium-nodes-worker kubernetes.io/os:linux]" nodeName=cilium-nodes-worker subsys=k8s v4Prefix=10.244.1.0/24 v6Prefix="<nil>"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: cilium-nodes-worker" subsys=daemon
level=info msg="  Node-IPv6: <nil>" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.3" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.1.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Adding local node to cluster" node="{cilium-nodes-worker default [{InternalIP 172.18.0.3} {CiliumInternalIP 1.1.1.2}] 10.244.1.0/24 [] <nil> [] 1.1.1.220 <nil> <nil> <nil> 0 local 0 map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:cilium-nodes-worker kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Start hook executed" duration=205.655982ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="5.521µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="2.384µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.863µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=error msg="Host endpoint not found" subsys=endpoint-manager
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=error msg="Host endpoint not found" subsys=endpoint-manager
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=error msg="Host endpoint not found" subsys=endpoint-manager
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=error msg="Host endpoint not found" subsys=endpoint-manager
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="33.152µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="25.778µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="26.389µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="87.341µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="11.501µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="25.278µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.559µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="11.631µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.487µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.625µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="57.678µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.417µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="25.768µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.258µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="24.896µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/CiliumNodes/v1.25" classname="github.com/cilium/cilium/test/controlplane" time="0.660">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=92.110326ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="239.045µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="5.681µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="3.496µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.578754ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.576µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.102µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration=862ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.172µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.473µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.062µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration=982ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="6.312µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="28.102µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="10.489µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="47.937µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=cilium-nodes-worker subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.3 ipAddr.ipv6="<nil>" k8sNodeIP=172.18.0.3 labels="map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:cilium-nodes-worker kubernetes.io/os:linux]" nodeName=cilium-nodes-worker subsys=k8s v4Prefix=10.244.1.0/24 v6Prefix="<nil>"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: cilium-nodes-worker" subsys=daemon
level=info msg="  Node-IPv6: <nil>" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.3" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.1.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=info msg="Adding local node to cluster" node="{cilium-nodes-worker default [{InternalIP 172.18.0.3} {CiliumInternalIP 1.1.1.2}] 10.244.1.0/24 [] <nil> [] 1.1.1.86 <nil> <nil> <nil> 0 local 0 map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:cilium-nodes-worker kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Start hook executed" duration=204.263635ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="6.933µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="2.665µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="2.053µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=error msg="Host endpoint not found" subsys=endpoint-manager
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=error msg="Host endpoint not found" subsys=endpoint-manager
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=error msg="Host endpoint not found" subsys=endpoint-manager
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=error msg="Host endpoint not found" subsys=endpoint-manager
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="50.785µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="33.402µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="29.725µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="65.993µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="9.518µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="19.436µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="13.896µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="16.62µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="6.303µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="11.091µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="40.889µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.416µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="27.281µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="13.055µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.709µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/CiliumNodes/v1.26" classname="github.com/cilium/cilium/test/controlplane" time="0.750">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=103.152167ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="325.346µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="3.435µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="3.486µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.801679ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="4.197µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.432µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="1.443µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.182µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=872ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.042µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="17.442µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration=942ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="14.828µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="11.201µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="25.475µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=cilium-nodes-worker subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.3 ipAddr.ipv6="<nil>" k8sNodeIP=172.18.0.3 labels="map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:cilium-nodes-worker kubernetes.io/os:linux]" nodeName=cilium-nodes-worker subsys=k8s v4Prefix=10.244.1.0/24 v6Prefix="<nil>"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: cilium-nodes-worker" subsys=daemon
level=info msg="  Node-IPv6: <nil>" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.3" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.1.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Adding local node to cluster" node="{cilium-nodes-worker default [{InternalIP 172.18.0.3} {CiliumInternalIP 1.1.1.2}] 10.244.1.0/24 [] <nil> [] 1.1.1.218 <nil> <nil> <nil> 0 local 0 map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:cilium-nodes-worker kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Start hook executed" duration=304.217575ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="3.075µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="1.633µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.374µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=error msg="Host endpoint not found" subsys=endpoint-manager
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=error msg="Host endpoint not found" subsys=endpoint-manager
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=error msg="Host endpoint not found" subsys=endpoint-manager
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=error msg="Host endpoint not found" subsys=endpoint-manager
level=info msg="Creating or updating CiliumNode resource" node=cilium-nodes-worker subsys=nodediscovery
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="24.195µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="26.048µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="61.755µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="57.166µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="9.578µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="15.239µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="14.557µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="14.648µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="6.302µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="9.277µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="28.769µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="4.739µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="34.629µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="13.756µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="20.769µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/DualStack" classname="github.com/cilium/cilium/test/controlplane" time="1.350"></testcase>
		<testcase name="TestControlPlane/Services/DualStack/v1.24" classname="github.com/cilium/cilium/test/controlplane" time="0.370">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=77.304481ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="253.102µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="3.848µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="5.049µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.393172ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="2.405µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.673µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="2.274µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration=962ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.022µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.091µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.493µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration=862ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="16.852µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="12.885µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="16.657µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=dual-stack-worker subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.4 ipAddr.ipv6="fc00:f853:ccd:e793::4" k8sNodeIP=172.18.0.4 labels="map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:dual-stack-worker kubernetes.io/os:linux]" nodeName=dual-stack-worker subsys=k8s v4Prefix=10.244.1.0/24 v6Prefix="fd00:10:244:1::/64"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="More than one stale router IP was found on the cilium_host device after restoration, cleaning up old router IPs." subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: dual-stack-worker" subsys=daemon
level=info msg="  Node-IPv6: fc00:f853:ccd:e793::4" subsys=daemon
level=info msg="  IPv6 allocation prefix: fd00:10:244:1::/64" subsys=daemon
level=info msg="  IPv6 router address: cafe::2" subsys=daemon
level=info msg="  Local IPv6 addresses:" subsys=daemon
level=info msg="  - f00d::1" subsys=daemon
level=info msg="  - f00d::2" subsys=daemon
level=info msg="  - f00d::3" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.4" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.1.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Adding local node to cluster" node="{dual-stack-worker default [{InternalIP 172.18.0.4} {InternalIP fc00:f853:ccd:e793::4} {CiliumInternalIP 1.1.1.2} {CiliumInternalIP cafe::2}] 10.244.1.0/24 [] fd00:10:244:1::/64 [] 1.1.1.202 cafe::71c1 <nil> <nil> 0 local 0 map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:dual-stack-worker kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=info msg="Creating or updating CiliumNode resource" node=dual-stack-worker subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Start hook executed" duration=110.73823ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="8.244µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="1.994µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.333µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="27.822µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="40.735µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="25.086µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="72.305µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="12.914µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="28.503µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="20.328µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="24.075µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.926µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.625µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="42.292µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="11.802µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="20.178µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="30.316µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="18.094µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/DualStack/v1.25" classname="github.com/cilium/cilium/test/controlplane" time="0.430">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=120.760028ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="248.844µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="6.381µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="6.321µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.338552ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="5.35µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration=972ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="1.544µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.393µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.413µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.293µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.202µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.423µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="20.057µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="12.814µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="115.576µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=dual-stack-worker subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.4 ipAddr.ipv6="fc00:f853:ccd:e793::4" k8sNodeIP=172.18.0.4 labels="map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:dual-stack-worker kubernetes.io/os:linux]" nodeName=dual-stack-worker subsys=k8s v4Prefix=10.244.1.0/24 v6Prefix="fd00:10:244:1::/64"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: dual-stack-worker" subsys=daemon
level=info msg="  Node-IPv6: fc00:f853:ccd:e793::4" subsys=daemon
level=info msg="  IPv6 allocation prefix: fd00:10:244:1::/64" subsys=daemon
level=info msg="  IPv6 router address: cafe::2" subsys=daemon
level=info msg="  Local IPv6 addresses:" subsys=daemon
level=info msg="  - f00d::1" subsys=daemon
level=info msg="  - f00d::2" subsys=daemon
level=info msg="  - f00d::3" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.4" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.1.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Creating or updating CiliumNode resource" node=dual-stack-worker subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Start hook executed" duration=110.220622ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="23.735µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="2.485µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.523µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Adding local node to cluster" node="{dual-stack-worker default [{InternalIP 172.18.0.4} {InternalIP fc00:f853:ccd:e793::4} {CiliumInternalIP 1.1.1.2} {CiliumInternalIP cafe::2}] 10.244.1.0/24 [] fd00:10:244:1::/64 [] 1.1.1.252 cafe::6c55 <nil> <nil> 0 local 0 map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:dual-stack-worker kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=info msg="Datapath signal listener running" subsys=signal
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="24.966µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="24.536µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="26.079µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="119.333µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="9.357µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="28.744µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="16.532µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="16.851µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.485µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.906µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="31.73µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.865µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="22.823µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.919µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="19.306µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/DualStack/v1.26" classname="github.com/cilium/cilium/test/controlplane" time="0.550">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=135.848291ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="362.936µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="5.831µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="4.288µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.397429ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="3.296µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="2.625µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="1.062µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration=912ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=802ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration=992ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.583µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.032µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="16.852µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="12.023µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="51.651µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=dual-stack-worker subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.4 ipAddr.ipv6="fc00:f853:ccd:e793::4" k8sNodeIP=172.18.0.4 labels="map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:dual-stack-worker kubernetes.io/os:linux]" nodeName=dual-stack-worker subsys=k8s v4Prefix=10.244.1.0/24 v6Prefix="fd00:10:244:1::/64"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: dual-stack-worker" subsys=daemon
level=info msg="  Node-IPv6: fc00:f853:ccd:e793::4" subsys=daemon
level=info msg="  IPv6 allocation prefix: fd00:10:244:1::/64" subsys=daemon
level=info msg="  IPv6 router address: cafe::2" subsys=daemon
level=info msg="  Local IPv6 addresses:" subsys=daemon
level=info msg="  - f00d::1" subsys=daemon
level=info msg="  - f00d::2" subsys=daemon
level=info msg="  - f00d::3" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.4" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.1.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Creating or updating CiliumNode resource" node=dual-stack-worker subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Start hook executed" duration=204.299609ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="3.316µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="1.733µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.542µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Adding local node to cluster" node="{dual-stack-worker default [{InternalIP 172.18.0.4} {InternalIP fc00:f853:ccd:e793::4} {CiliumInternalIP 1.1.1.2} {CiliumInternalIP cafe::2}] 10.244.1.0/24 [] fd00:10:244:1::/64 [] 1.1.1.12 cafe::9a3e <nil> <nil> 0 local 0 map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:dual-stack-worker kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=info msg="Datapath signal listener running" subsys=signal
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="30.788µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="96.099µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="31.469µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="99.841µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="11.832µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="20.157µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="17.763µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="16.431µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.217µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.216µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="22.702µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="4.157µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="22.972µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="10.901µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="61.279µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/GracefulTermination" classname="github.com/cilium/cilium/test/controlplane" time="0.510">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=104.175156ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="279.11µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="5.45µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="5.761µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.883308ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="5.681µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.313µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="1.412µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.272µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="2.004µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.824µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.442µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.463µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="18.085µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="15.87µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="50.161µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=graceful-term-control-plane subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.2 ipAddr.ipv6="<nil>" k8sNodeIP=172.18.0.2 labels="map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:graceful-term-control-plane kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:]" nodeName=graceful-term-control-plane subsys=k8s v4Prefix=10.244.0.0/24 v6Prefix="<nil>"
level=info msg="Opting out from node-to-node encryption on this node as per 'node-encryption-opt-out-labels' label selector" Selector=node-role.kubernetes.io/control-plane subsys=k8s
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: graceful-term-control-plane" subsys=daemon
level=info msg="  Node-IPv6: <nil>" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.2" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.0.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Adding local node to cluster" node="{graceful-term-control-plane default [{InternalIP 172.18.0.2} {CiliumInternalIP 1.1.1.2}] 10.244.0.0/24 [] <nil> [] 1.1.1.108 <nil> <nil> <nil> 0 local 0 map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:graceful-term-control-plane kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:] map[] 1 }" subsys=nodediscovery
level=info msg="Creating or updating CiliumNode resource" node=graceful-term-control-plane subsys=nodediscovery
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg="Start hook executed" duration=105.419697ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="3.647µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="2.183µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.693µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="27.521µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="27.3µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="33.462µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="117.018µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="10.059µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="25.006µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="16.41µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="12.243µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.587µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.437µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="27.862µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.055µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="49.773µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.158µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="21.5µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/NodePort" classname="github.com/cilium/cilium/test/controlplane" time="3.700"></testcase>
		<testcase name="TestControlPlane/Services/NodePort/v1.24/nodeport-control-plane" classname="github.com/cilium/cilium/test/controlplane" time="0.630">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=107.319188ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="214.44µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="5.781µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="3.957µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=200.812958ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="4.019µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.522µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="1.122µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.061µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.563µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.172µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.222µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.864µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="17.082µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="11.602µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="25.518µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Retrieved node information from kubernetes node" nodeName=nodeport-control-plane subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.4 ipAddr.ipv6="fc00:f853:ccd:e793::4" k8sNodeIP=172.18.0.4 labels="map[kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-control-plane kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:]" nodeName=nodeport-control-plane subsys=k8s v4Prefix=10.244.0.0/24 v6Prefix="fd00:10:244::/64"
level=info msg="Opting out from node-to-node encryption on this node as per 'node-encryption-opt-out-labels' label selector" Selector=node-role.kubernetes.io/control-plane subsys=k8s
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: nodeport-control-plane" subsys=daemon
level=info msg="  Node-IPv6: fc00:f853:ccd:e793::4" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.4" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.0.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Creating or updating CiliumNode resource" node=nodeport-control-plane subsys=nodediscovery
level=info msg="Adding local node to cluster" node="{nodeport-control-plane default [{InternalIP 172.18.0.4} {InternalIP fc00:f853:ccd:e793::4} {CiliumInternalIP 1.1.1.2}] 10.244.0.0/24 [] <nil> [] 1.1.1.216 <nil> <nil> <nil> 0 local 0 map[kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-control-plane kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:] map[] 1 }" subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg="Start hook executed" duration=203.357218ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="5.189µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg="Start hook executed" duration="2.194µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.402µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="23.623µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="35.436µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="22.171µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="32.768µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="12.223µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="19.096µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.118µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="14.517µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.895µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.715µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="21.3µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="28.753µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="20.738µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="11.893µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="16.482µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/NodePort/v1.24/nodeport-worker" classname="github.com/cilium/cilium/test/controlplane" time="0.450">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=62.546188ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="231.252µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="4.559µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="4.939µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Start hook executed" duration=200.507296ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="14.467µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.413µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="1.373µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.152µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=822ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.112µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.614µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.282µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="12.454µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="8.766µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="24.245µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=nodeport-worker subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.3 ipAddr.ipv6="fc00:f853:ccd:e793::3" k8sNodeIP=172.18.0.3 labels="map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-worker kubernetes.io/os:linux]" nodeName=nodeport-worker subsys=k8s v4Prefix=10.244.2.0/24 v6Prefix="fd00:10:244:2::/64"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: nodeport-worker" subsys=daemon
level=info msg="  Node-IPv6: fc00:f853:ccd:e793::3" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.3" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.2.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Adding local node to cluster" node="{nodeport-worker default [{InternalIP 172.18.0.3} {InternalIP fc00:f853:ccd:e793::3} {CiliumInternalIP 1.1.1.2}] 10.244.2.0/24 [] <nil> [] 1.1.1.234 <nil> <nil> <nil> 0 local 0 map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-worker kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=info msg="Creating or updating CiliumNode resource" node=nodeport-worker subsys=nodediscovery
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Start hook executed" duration=103.65247ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="10.32µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="1.874µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.132µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="29.356µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="24.877µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="28.172µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="129.281µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="14.757µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="29.936µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="21.24µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="33.041µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="14.467µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.396µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="73.409µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="6.532µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="41.497µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="21.289µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="13.606µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/NodePort/v1.24/nodeport-worker2" classname="github.com/cilium/cilium/test/controlplane" time="0.340">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=61.90716ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="143.287µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="2.785µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="3.235µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=100.361813ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="1.853µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="2.064µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="1.402µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.614µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.193µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="2.014µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.794µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.252µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="15.599µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="8.326µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="24.867µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=nodeport-worker2 subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.5 ipAddr.ipv6="fc00:f853:ccd:e793::5" k8sNodeIP=172.18.0.5 labels="map[cilium.io/ci-node:k8s2 kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-worker2 kubernetes.io/os:linux]" nodeName=nodeport-worker2 subsys=k8s v4Prefix=10.244.1.0/24 v6Prefix="fd00:10:244:1::/64"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: nodeport-worker2" subsys=daemon
level=info msg="  Node-IPv6: fc00:f853:ccd:e793::5" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.5" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.1.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Creating or updating CiliumNode resource" node=nodeport-worker2 subsys=nodediscovery
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Start hook executed" duration=103.623318ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="27.22µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="8.045µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="10.59µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg="Adding local node to cluster" node="{nodeport-worker2 default [{InternalIP 172.18.0.5} {InternalIP fc00:f853:ccd:e793::5} {CiliumInternalIP 1.1.1.2}] 10.244.1.0/24 [] <nil> [] 1.1.1.61 <nil> <nil> <nil> 0 local 0 map[cilium.io/ci-node:k8s2 kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-worker2 kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="20.979µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="20.027µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="45.645µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="33.893µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="10.119µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="17.914µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="42.413µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="29.815µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.075µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.486µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="34.449µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="4.117µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="17.337µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="16.621µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="23.032µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/NodePort/v1.25/nodeport-control-plane" classname="github.com/cilium/cilium/test/controlplane" time="0.350">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=60.913236ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="241.83µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="5.05µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="5.189µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Start hook executed" duration=100.541421ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="1.333µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="2.435µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="1.231µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration=962ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.052µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.353µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.312µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.422µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="14.687µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="10.6µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="18.056µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=nodeport-control-plane subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.4 ipAddr.ipv6="fc00:f853:ccd:e793::4" k8sNodeIP=172.18.0.4 labels="map[kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-control-plane kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:]" nodeName=nodeport-control-plane subsys=k8s v4Prefix=10.244.0.0/24 v6Prefix="fd00:10:244::/64"
level=info msg="Opting out from node-to-node encryption on this node as per 'node-encryption-opt-out-labels' label selector" Selector=node-role.kubernetes.io/control-plane subsys=k8s
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: nodeport-control-plane" subsys=daemon
level=info msg="  Node-IPv6: fc00:f853:ccd:e793::4" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.4" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.0.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Adding local node to cluster" node="{nodeport-control-plane default [{InternalIP 172.18.0.4} {InternalIP fc00:f853:ccd:e793::4} {CiliumInternalIP 1.1.1.2}] 10.244.0.0/24 [] <nil> [] 1.1.1.159 <nil> <nil> <nil> 0 local 0 map[kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-control-plane kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:] map[] 1 }" subsys=nodediscovery
level=info msg="Creating or updating CiliumNode resource" node=nodeport-control-plane subsys=nodediscovery
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Start hook executed" duration=103.605252ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="5.22µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="109.684µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="2.364µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="23.314µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="14.056µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="22.352µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="29.114µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="7.874µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="16.03µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="24.312µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="12.693µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.347µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.064µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="22.666µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.015µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="20.874µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="8.987µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="17.592µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/NodePort/v1.25/nodeport-worker" classname="github.com/cilium/cilium/test/controlplane" time="0.340">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=61.674874ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="646.285µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="5.65µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="2.836µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=101.097657ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="1.273µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.092µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="1.162µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration=982ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.172µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.262µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.393µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.092µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="13.796µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="8.154µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="25.678µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Retrieved node information from kubernetes node" nodeName=nodeport-worker subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.3 ipAddr.ipv6="fc00:f853:ccd:e793::3" k8sNodeIP=172.18.0.3 labels="map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-worker kubernetes.io/os:linux]" nodeName=nodeport-worker subsys=k8s v4Prefix=10.244.1.0/24 v6Prefix="fd00:10:244:1::/64"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: nodeport-worker" subsys=daemon
level=info msg="  Node-IPv6: fc00:f853:ccd:e793::3" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.3" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg="  IPv4 allocation prefix: 10.244.1.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Creating or updating CiliumNode resource" node=nodeport-worker subsys=nodediscovery
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Adding local node to cluster" node="{nodeport-worker default [{InternalIP 172.18.0.3} {InternalIP fc00:f853:ccd:e793::3} {CiliumInternalIP 1.1.1.2}] 10.244.1.0/24 [] <nil> [] 1.1.1.57 <nil> <nil> <nil> 0 local 0 map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-worker kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Start hook executed" duration=104.011806ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="13.977µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="1.974µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.413µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="29.655µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="28.764µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="22.411µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="32.851µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="9.077µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="14.957µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="11.446µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="54.431µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.345µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.234µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="40.745µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.204µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="14.006µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="7.123µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="9.658µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/NodePort/v1.25/nodeport-worker2" classname="github.com/cilium/cilium/test/controlplane" time="0.440">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=62.722428ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="224.759µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="3.216µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="2.996µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Start hook executed" duration=200.226227ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="1.763µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration=842ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration=972ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration=972ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.062µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.453µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.042µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration=631ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="14.958µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="10.65µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="36.218µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=nodeport-worker2 subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.5 ipAddr.ipv6="fc00:f853:ccd:e793::5" k8sNodeIP=172.18.0.5 labels="map[cilium.io/ci-node:k8s2 kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-worker2 kubernetes.io/os:linux]" nodeName=nodeport-worker2 subsys=k8s v4Prefix=10.244.2.0/24 v6Prefix="fd00:10:244:2::/64"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: nodeport-worker2" subsys=daemon
level=info msg="  Node-IPv6: fc00:f853:ccd:e793::5" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.5" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.2.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg="Adding local node to cluster" node="{nodeport-worker2 default [{InternalIP 172.18.0.5} {InternalIP fc00:f853:ccd:e793::5} {CiliumInternalIP 1.1.1.2}] 10.244.2.0/24 [] <nil> [] 1.1.1.145 <nil> <nil> <nil> 0 local 0 map[cilium.io/ci-node:k8s2 kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-worker2 kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=info msg="Creating or updating CiliumNode resource" node=nodeport-worker2 subsys=nodediscovery
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Start hook executed" duration=103.303409ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="4.89µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="1.964µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.583µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="22.23µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="39.684µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="19.527µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="43.721µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="8.526µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="35.564µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="13.706µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="30.001µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.717µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.775µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="32.456µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.715µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="23.444µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="11.751µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="24.969µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/NodePort/v1.26/nodeport-control-plane" classname="github.com/cilium/cilium/test/controlplane" time="0.350">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=64.092362ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="175.077µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="3.867µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="4.058µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Start hook executed" duration=100.562567ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="2.745µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.061µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration=871ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration=551ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=942ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="4.179µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="2.394µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration=952ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="12.143µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="10.359µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="52.998µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=nodeport-control-plane subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.4 ipAddr.ipv6="fc00:f853:ccd:e793::4" k8sNodeIP=172.18.0.4 labels="map[kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-control-plane kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:]" nodeName=nodeport-control-plane subsys=k8s v4Prefix=10.244.0.0/24 v6Prefix="fd00:10:244::/64"
level=info msg="Opting out from node-to-node encryption on this node as per 'node-encryption-opt-out-labels' label selector" Selector=node-role.kubernetes.io/control-plane subsys=k8s
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: nodeport-control-plane" subsys=daemon
level=info msg="  Node-IPv6: fc00:f853:ccd:e793::4" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.4" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.0.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Creating or updating CiliumNode resource" node=nodeport-control-plane subsys=nodediscovery
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg="Start hook executed" duration=103.570698ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="3.307µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="2.225µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.353µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg="Adding local node to cluster" node="{nodeport-control-plane default [{InternalIP 172.18.0.4} {InternalIP fc00:f853:ccd:e793::4} {CiliumInternalIP 1.1.1.2}] 10.244.0.0/24 [] <nil> [] 1.1.1.77 <nil> <nil> <nil> 0 local 0 map[kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-control-plane kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:] map[] 1 }" subsys=nodediscovery
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="20.509µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="36.177µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="50.364µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="68.07µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="11.411µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="23.812µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="16.06µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="15.971µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.436µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.314µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="22.712µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.086µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="21.95µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="9.728µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="13.274µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/NodePort/v1.26/nodeport-worker" classname="github.com/cilium/cilium/test/controlplane" time="0.440">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=62.845653ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="239.216µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="5.03µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="3.958µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Start hook executed" duration=100.289306ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="1.662µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.322µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="1.923µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.243µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="1.203µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.131µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.403µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration=992ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="15.298µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="13.725µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="15.845µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=nodeport-worker subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.3 ipAddr.ipv6="fc00:f853:ccd:e793::3" k8sNodeIP=172.18.0.3 labels="map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-worker kubernetes.io/os:linux]" nodeName=nodeport-worker subsys=k8s v4Prefix=10.244.1.0/24 v6Prefix="fd00:10:244:1::/64"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: nodeport-worker" subsys=daemon
level=info msg="  Node-IPv6: fc00:f853:ccd:e793::3" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.3" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.1.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Adding local node to cluster" node="{nodeport-worker default [{InternalIP 172.18.0.3} {InternalIP fc00:f853:ccd:e793::3} {CiliumInternalIP 1.1.1.2}] 10.244.1.0/24 [] <nil> [] 1.1.1.41 <nil> <nil> <nil> 0 local 0 map[cilium.io/ci-node:k8s1 kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-worker kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=info msg="Creating or updating CiliumNode resource" node=nodeport-worker subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Start hook executed" duration=203.237408ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="16.45µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="1.994µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Start hook executed" duration="1.302µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="21.25µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="29.876µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="21.961µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="35.729µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="15.349µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="20.238µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="31.166µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="32.262µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="12.814µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="10.629µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="30.197µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="2.975µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="21.408µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="20.979µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="48.891µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<testcase name="TestControlPlane/Services/NodePort/v1.26/nodeport-worker2" classname="github.com/cilium/cilium/test/controlplane" time="0.350">
			<system-out><![CDATA[level=warning msg="Running Cilium with \"kvstore\"=\"\" requires identity allocation via CRDs. Changing identity-allocation-mode to \"crd\"" subsys=config
level=info msg=Invoked duration=63.830311ms function="cmd.glob..func4 (daemon_main.go:1597)" subsys=hive
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration="153.486µs" function="auth.newManager (cell.go:76)" subsys=hive
level=info msg=Invoked duration="2.845µs" function="suite.startCiliumAgent.func8 (agent.go:83)" subsys=hive
level=info msg=Starting subsys=hive
level=info msg="Start hook executed" duration="3.277µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Start hook executed" duration=100.83877ms function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration="1.914µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="1.012µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration="1.062µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration="1.032µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration=781ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.112µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="1.042µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration=962ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="13.505µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=info msg="Start hook executed" duration="9.357µs" function="cmd.newPolicyTrifecta.func1 (policy.go:127)" subsys=hive
level=info msg="Start hook executed" duration="31.739µs" function="*manager.manager.Start" subsys=hive
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Detected MTU 1500" subsys=mtu
level=info msg="L7 proxies are disabled" subsys=daemon
level=warning msg="No valid cgroup base path found: socket load-balancing tracing with Hubble will not work.See the kubeproxy-free guide for more details." subsys=cgroup-manager
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=info msg="Retrieved node information from kubernetes node" nodeName=nodeport-worker2 subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.5 ipAddr.ipv6="fc00:f853:ccd:e793::5" k8sNodeIP=172.18.0.5 labels="map[cilium.io/ci-node:k8s2 kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-worker2 kubernetes.io/os:linux]" nodeName=nodeport-worker2 subsys=k8s v4Prefix=10.244.2.0/24 v6Prefix="fd00:10:244:2::/64"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Detected devices" devices="[enp0s2]" subsys=linux-datapath
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=1.1.1.0/24 v6Prefix="cafe::/96"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: nodeport-worker2" subsys=daemon
level=info msg="  Node-IPv6: fc00:f853:ccd:e793::5" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.5" subsys=daemon
level=info msg="  Internal-Node IPv4: 1.1.1.2" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.2.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 10.0.0.2" subsys=daemon
level=info msg="  - 10.0.0.3" subsys=daemon
level=info msg="  - 10.0.0.4" subsys=daemon
level=info msg="Creating or updating CiliumNode resource" node=nodeport-worker2 subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Start hook executed" duration=104.123011ms function="cmd.newDaemonPromise.func1 (daemon_main.go:1646)" subsys=hive
level=info msg="Start hook executed" duration="4.969µs" function="signal.provideSignalManager.func1 (cell.go:26)" subsys=hive
level=info msg="Start hook executed" duration="2.744µs" function="auth.newManager.func1 (cell.go:81)" subsys=hive
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=info msg="Adding local node to cluster" node="{nodeport-worker2 default [{InternalIP 172.18.0.5} {InternalIP fc00:f853:ccd:e793::5} {CiliumInternalIP 1.1.1.2}] 10.244.2.0/24 [] <nil> [] 1.1.1.245 <nil> <nil> <nil> 0 local 0 map[cilium.io/ci-node:k8s2 kubernetes.io/arch:amd64 kubernetes.io/hostname:nodeport-worker2 kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=info msg="Start hook executed" duration="1.734µs" function="auth.newManager.func2 (cell.go:96)" subsys=hive
level=info msg="Datapath signal listener running" subsys=signal
level=info msg=Stopping subsys=hive
level=info msg="Datapath signal listener exiting" subsys=signal
level=info msg="Datapath signal listener done" subsys=signal
level=info msg="Stop hook executed" duration="19.707µs" function="signal.provideSignalManager.func2 (cell.go:29)" subsys=hive
level=info msg="Waiting for all endpoints' goroutines to be stopped." subsys=daemon
level=info msg="All endpoints' goroutines stopped." subsys=daemon
level=info msg="Stop hook executed" duration="26.339µs" function="cmd.newDaemonPromise.func2 (daemon_main.go:1666)" subsys=hive
level=info msg="Stop hook executed" duration="23.423µs" function="*manager.manager.Stop" subsys=hive
level=info msg="Stop hook executed" duration="41.748µs" function="cmd.newPolicyTrifecta.func2 (policy.go:131)" subsys=hive
level=info msg="Stop hook executed" duration="16.321µs" function="endpointmanager.newDefaultEndpointManager.func2 (cell.go:189)" subsys=hive
level=info msg="Stop hook executed" duration="15.96µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Stop" subsys=hive
level=info msg="Stop hook executed" duration="45.046µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="19.353µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.656µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Stop" subsys=hive
level=info msg="Stop hook executed" duration="9.608µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Stop" subsys=hive
level=info msg="Stop hook executed" duration="31.532µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Stop" subsys=hive
level=info msg="Stop hook executed" duration="3.997µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Stop" subsys=hive
level=info msg="Stop hook executed" duration="24.106µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Node].Stop" subsys=hive
level=info msg="Stop hook executed" duration="8.786µs" function="*manager.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive
level=info msg="Stop hook executed" duration="26.805µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Stop" subsys=hive]]></system-out>
		</testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/test/controlplane	coverage: 100.0% of statements]]></system-out>
	</testsuite>
	<testsuite name="github.com/cilium/cilium/test/helpers/logutils" tests="3" failures="0" errors="0" id="522" hostname="kind-bpf-next" time="0.006" timestamp="2023-05-31T11:45:55Z">
		<testcase name="Test" classname="github.com/cilium/cilium/test/helpers/logutils" time="0.000"></testcase>
		<testcase name="Test/LogutilsTestSuite" classname="github.com/cilium/cilium/test/helpers/logutils" time="0.000"></testcase>
		<testcase name="Test/LogutilsTestSuite/TestLogErrorsSummary" classname="github.com/cilium/cilium/test/helpers/logutils" time="0.000"></testcase>
		<system-out><![CDATA[	github.com/cilium/cilium/test/helpers/logutils	coverage: 95.9% of statements]]></system-out>
	</testsuite>
	<testsuite name="" tests="1" failures="0" errors="0" id="523" hostname="kind-bpf-next" skipped="1" time="0.000" timestamp="2023-05-31T11:45:55Z">
		<testcase name="TestVerifier" classname="" time="0.000">
			<skipped message="Skipped"><![CDATA[    verifier_test.go:80: Please set -cilium-base-path to run verifier tests]]></skipped>
		</testcase>
		<system-out><![CDATA[coverage: [no statements]
ok  	github.com/cilium/cilium/test/verifier	0.020s	coverage: [no statements]]]></system-out>
	</testsuite>
</testsuites>
